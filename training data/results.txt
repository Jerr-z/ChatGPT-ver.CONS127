 All right. Hi, everyone. Welcome to your last Cons 127 class. There we go. Lovely. Okay. Hopefully this will be useful for you guys. I've kind of gone off. I got a decent amount of posts on the discussion board, which is great. So I've kind of gone off that for what to go over today. I'm going to start by going over again the exam format just to remind you guys in case you have any questions about that. And then we'll dive into the review. We'll go over over a couple topics that were posted on the discussion board. I do have Julia here today to, if you have questions about Simon 7. Otherwise, you know, she has lots of office hours this week and next week still. But if you do want to talk to her while she's here, that's fine too. So I'll get her to come in right as we finish class. Okay. So just a reminder, due dates. So blog post five, do this Thursday. And then assignment seven, blog post six. Do next Thursday, April 13th. Julia has office hours later this week. And then twice next week as well. Okay. Final exam. Here we go. One more time. Just to remind you guys. It's Thursday, April 27th at 7 p.m. Don't know why they gave us such a crappy time. They give me a crappy time every single semester. They just, they don't like me very much, I guess. So it's two hours long. It's from 7 p.m. to 9 p.m. There's a 10 minute buffer from when you open and close the exam. So that means that the exam will open at 6.50 p.m. And it'll close hard at 9.10 p.m. But no matter when you start it, you'll have two hours to complete it. So if you start it at, you know, 6.50 p.m. It'll close at 8.50 p.m. But no matter what, it'll close at 9.10 p.m. So if you start it at 8 p.m., it'll still close at 9.10 p.m. It's through Canvas. It's online. It says, I think, I just checked on, on SSC or whatever. It says, for location, it says C-prof and then in brackets, was SRC or something like that. They really just like to confuse you guys and they really annoy me because I said please remove the location. It's an online exam. I told you guys it's an online exam. Remove the location. And their solution to that was just to change it to C-prof, but was in SRC, which is probably even more confusing. So sorry about that. It's online. It's fully online. It's on Canvas. There's no in-person component anywhere. So don't show up in person. If you have any technical difficulties while you are writing the exam, you can email Evan. If you do have technical difficulties while writing the exam, email Evan right away. Don't wait until the exam closes to email him and say, hey, there was an issue I didn't get all the time I was supposed to. If there's an issue, as soon as it occurs, email him because we can tack on extra time while you're writing the exam, but it's much more difficult to re-open it after it's been auto-submitted once the time is expired. So if you have an issue, just email Evan right away. Okay. There's 40 multiple choice questions worth 1.5 marks each. So 60 points total. Some will have graphs or pictures that you have to interpret. Not many. Maybe about, I think, five or six. There's one matching question worth four points. So it's kind of like there's eight little parts to it. So they're each worth half a mark, essentially. So one marking question worth four points total. There's six short answer questions, each requiring one very brief paragraph, maybe four lines, five lines, maximum, each worth six points as well. So for a total of 36 points there. Just make sure I'm recording here. I am. Okay. The exam's open book. Questions will appear one by one just like the midterm. They'll appear in a random order within groups. So multiple choice first, matching second, short answer last. You won't be able to return to any questions after completing them. Question will pop up. You'll put in the answer. It'll go into the next one. It'll go on to the next question. You can't go back to any previous questions. Final has to be done individually. You can't work together. You can't share answers. You can't discuss answers. Extra time for A and D students is added to their canvas profiles automatically. So you should see that reflected in the closing time of your final if you're an A and D student. And then you'll also have an extended deadline to accommodate the extra time as well. So if you're an A and D student, your time should already be applied. You shouldn't have to email us or anything. If by chance you start the exam and you're an A and D student, and the timer doesn't look right, it doesn't look like you have as much time as you're supposed to. Again, just email Evan right away. We'll get it sorted right off the bat. Okay. Terms of pre and post midterm content. There's about a 40 60 split for the multiple choice and matching. So about 40% of it is pre midterm. About 60% of it is post midterm. For the short answer questions, one of them is pre midterm. Two of them are post midterm. And three of them are kind of whole course. They incorporate topics and concepts from multiple lectures, both pre and post midterm and kind of force you to link together different things that we've talked about. Okay. The finals worth 40% of your overall grade. There's practice questions posted on Canvas. We don't provide any answers for those practice questions. But if you're unsure about any, you can post in the final exam discussion board. And Evan or myself will answer, tell you whether you're on the right track. What needs to be improved. The final includes all lecture content, assignment material that's not covered in lecture will not be on the final. So I say that just to say I often get the question, are assignments covered on the final exam? In kind of a, you know, strict sense, no. But in a more general sense, if it's a topic that we've covered in lecture that occurred on the assignment, then sure, it might be in the final exam. But that's because we've talked about it in lecture. That makes sense? Okay. Please post in the final discussion board for questions about the final. If you have any questions, concerns about content, about format, feel free to discuss with each other, answer each other's questions. Myself and Evan will also monitor that. That's the best place to go to ask questions if you're studying and you can't figure out an answer to something or, yeah, anything like that. Just post on the final exam discussion board. Okay. You need reliable internet. Please make sure you have reliable internet during the final exam period. We can't be reliable for your internet connection. Again, if you have an emergency during the exam period, contact Evan right away. We won't answer any questions related to content. We'll help you with the technical difficulty right away. But if it's a question related to the actual content of the exam, then we'll just respond and say, hey, we can't answer this. Again, if your exam opens at eight, it'll still close at nine, ten PM. It'll close at nine, ten, no matter when you start it. And if you start it earlier than seven, ten PM, you'll still only have two hours to complete it. Okay. No plagiarism. Plagiarism will give you a zero on the final. You can't copy and paste sentences from the slides or the internet. And you must answer in your own words. Don't copy work of your neighbors or classmates. Copy and pasting and changing a few words. Still plagiarism. So please just make sure you answer in your own words. I always get questions or concerns about this slide. And I'll try to clarify with a couple more details. The concern about plagiarism on the exam is really just with the short answer questions. Obviously we can't really in a strict sense monitor plagiarism on the multiple choice questions. We try to reduce it by the format of the exam by only having one question pop up per time and by it being random questions popping up. We try to limit your ability to kind of plagiarize and work together. With the final, with the short answer questions, it's a little bit more difficult to monitor that. And I am aware that for some of the questions, maybe you only need to answer a short phrase or something or a list. And kind of inherently that's going to be, you know, copy and pasted from the slides. That is fine. If it's, you know, a list, if it's kind of direct fact relating to something that we've discussed in class, that's fine. This really relates to the questions that are a bit more application based, that are really requiring you to link topics together that are requiring you to maybe give your own opinions and thoughts. Those are where we really want you to answer in your own words. And so please be aware of that. And that's where we want you to focus with that. Does that make sense? You can still use notes, internet, whatever to answer the questions. You can read and comprehend whatever you want, but you still have to type out your answer in your own words. If you, you know, take something over, you change a couple aunts and aurs or whatever, change the order of the sentence that still constitutes this plagiarism. So just try and avoid that. Does that make sense? Any questions about that? Okay. Sweet. Okay. You're going to, if you do the practice questions, you go over class notes. You take note of what I've highlighted during lecture, as well as the end of the lecture with the practice questions and the final exam practice questions we've posted, you're going to do great. That covers the vast majority of anything that you're possibly going to see on the final exam. I don't try to trick you with the final exam questions. It's all stuff that we've covered. It's all stuff that I've highlighted is important. So you're going to do great. Don't worry about it. All good? Sound good? Okay. Okay. So I'm going to go over for the review. A couple topics that we're posted on the discussion board. I'm going to talk about map projections, orbits briefly, spectral signatures and NDVI. Just briefly talk about active versus passive remote sensing and then some data types, applications and advantages of earth observation. So I'll use the cryosphere as an example, but it could kind of be applied to any of those topics we talked about, whether it's biosphere, oceans and freshwater, wildlife, but I'll kind of focus on the cryosphere as a specific example. It's hard for me to go over every single one of those in detail for all of those because that's essentially me kind of just giving each of those lectures start to finish again. But I'll kind of go over synthesizing data types, applications and advantages of earth observation data for just as an example, the cryosphere. And then I'll end kind of linking that, potentially giving you some ideas for the other topics we talked about, like the biosphere, like oceans and freshwater, etc. And then lastly, I mentioned three of the short answer questions, kind of required to link topics together to bridge concepts between different lectures to bring together information from pre-mid term and post-mid term. And I'll kind of try to talk about in a bit more deal, I can't give away what the questions will be, but I'll try and kind of explain what I mean by that in a bit more detail and then look at an example question of that that was post it on the discussion board that was just taken from the end of one of my slides as an example question, but it kind of is close to an example of this idea of having a question that forces you to link together multiple lectures and topics from both pre- and post-mid term. Okay, so let's talk about projections first. First off, let me just say, projections are confusing and weird and funky. So if you have had trouble wrapping your head around projections and how they work and what they are, that's totally understandable, that's totally fair, even very advanced Earth observation data users are still perplexed and confused by projections. So don't be concerned if it's a confusing concept for you. There's really two kind of key categories of information you need to know about projections. And that's essentially just how we classify the different kinds of projections that are out there. One of the ways that we can classify the different kinds of projections is based off this type of surface that the map or that the globe is projected onto. So you can project the globe onto a cylindrical surface, onto a conic surface or cone or onto a planar surface. So cylindrical projections or where the Earth is projected onto a cylinder which creates whole world maps that are rectangular and distortion is really heavy towards the poles. So that's like the Mercator projection, for example. Conic projections is where the Earth is projected onto a cone. It's good for representing parts of the Earth but not necessarily all of the Earth. And planar projections is where the Earth is projected onto a plane and there's a particularly large amount of distortion in planar projections towards the edge of the projections. All you really need to know about the different classes, the different types of projections in this case is that one of the ways we classify them is based off the type of surface the globe is projected onto, whether that cylindrical, conic or planar. The other way we talk about projections is based off what is being distorted and what is being preserved in that projection. So there's four categories that we generally talk about. First is conformal projections. Conformal projections are any projection where angles between positions are preserved. Makes it really convenient for sea navigation. That's why these projections were developed so that if you got a heading on the map, it was a true heading in real life. But it has a high amount of distortion of countries and continents areas or size. An example of that again is the Mercator. Now you'll notice that I mentioned the Mercator in the last slide as well. That's because the Mercator happens to be a cylindrical conformal projection. It is a cylinder that is used to project the globe onto in the case of the Mercator projection. And in the case of the Mercator projection, it's conformal. Angles are preserved. The reason that we talk about the two different ways we can classify projections, aka the type of surfaces you can project onto like this or the type of distortions that you can have like this such as conformal, is because a cylindrical projection is not always necessarily a conformal projection. And a conformal projection is not always necessarily a cylindrical projection. The Mercator projection is a unique example where it is a cylindrical conformal projection. But these can be mutually exclusive. So the type of projection it is based off its preservation and distortion of different map elements such as angles, such as size, such as distance is a different way that we can classify projections from just the type of surface the globe is projected onto like is the case with this. So conformal projections, angles are preserved, area and distance is distorted, particularly increasing towards the poles. And in equivalent equal area projection, the area of countries and continents is preserved. The size of them is preserved, but the angles are distorted and thus the shapes of the continents and countries are distorted. Example of that is the mall weed or the gal Peters projection. Then we have equidistant projections. All distances from a single point are correct. In this case and most commonly equidistant projections, that common point is the very center of the map. So in this example, any distance measurement from the very center of this map to anywhere else is proportionally correct. But if you say try to distance measurement from here to here, it would not be correct. So any distance measurement from the center of this map to anywhere else is correct. Otherwise, it's not correct. Example of an equidistant projection is the azimuthal equidistant projection. Lastly, we have the compromise projections that has a balance of the different distortions of both shape and area and distance. Produces quite visually appealing maps, but none of the metric properties of this map are proportionally correct. Area, distance and shape cannot be measured accurately with a compromise map. It just gives us something that is aesthetic. So again, two ways that we can classify our map projections. The different types of surfaces that we project the globe onto. These three here, cylindrical, conic, planar. Or based off what element of the map, what metric property is preserved. Conformal angles are preserved. Equivalent, equal area, area or size is preserved. Equidistant distances preserved, typically just from one point. And then compromise, nothing is preserved. Everything is distorted. Make sense? Okay. So we talked about four different key types of orbits for the midterm. Talked about low earth orbits, close elliptical orbits, far elliptical orbits and geostationary orbits. Low earth orbits are the closest to earth. We talked about the international space station, as well as potentially some other earth observation satellites being in low earth orbit. We talked about the close elliptical orbit, which mostly has earth observation satellites in it. We talked about the far elliptical orbit, which mostly has GNSS or global navigation satellite systems in it. And then we talked about geostationary orbits, mostly used for weather satellites. Now I wanted to clarify, because I know it's always a little bit confusing. The difference, because we then talked about polar orbits and sun synchronous orbits. As soon as we're talking about polar orbits or sun synchronous orbits, we are specifically talking about distinct kinds of close elliptical orbits. So as soon as you see polar orbit or sun synchronous orbit, you don't need to be thinking about low earth orbit, far elliptical orbit, geostationary orbit. Don't think about those anymore. Polar orbit or sun synchronous orbit are specific types of close elliptical orbits. So the close elliptical orbit is defined by its altitude at an altitude of about 700 to 2000 kilometers. The polar orbit is a close elliptical orbit that has a particular orientation. The polar orbit is oriented such that the satellites pass over or very close to the north and south pole. So a polar orbit is just a close elliptical orbit with a specific orientation, such that the satellites pass over close to or right above the north and south pole. The sun synchronous orbit is a specific type of polar orbit. So the sun synchronous orbit is a specific type of polar orbit, which is a specific type of close elliptical orbit. So the sun synchronous orbit is defined as an orbit that passes over the equator at the same time each day. Landsat for example passes over the equator at 10 to 10 30 a.m. local time every single day. As such, it also passes over any exact same point on the earth at the same local time each time it passes over it. And we do that so that we can kind of control for potential variation in the angle of the sun when we're collecting earth observation remote sensing imagery. So the purpose of the sun synchronous orbit is so that if we're taking an image of a point on the earth, the same exact point at the same local time every time we take it, that eliminates some of the potential variation associated with taking an image at the same point across different times of the day. So a sun synchronous orbit is a specific type of polar orbit and a polar orbit is a specific type of close elliptical orbit. So by nature is a sun synchronous orbit, a close elliptical orbit? That's where you all go, yes, Chris. Yes. No? Okay. I got it. Last class. It's all good. Okay. Yes. Is the answer. Sun-sungous orbit is a type of close elliptical orbit. Okay. Let's go on now. A briefly kind of summarize some of what we talked about in the electromagnetic spectrum lecture. So kind of the first thing we talked about were radiation fundamentals. We talked about how the photon is kind of the unit or discrete package, the smallest kind of package of light that we can describe or radiation. And we can often describe the characteristics of photons of light, of radiation, of energy using this equation, c equals lambda times v, where c is a constant, c never changes. It's the speed of light. c is always the exact same. 3.0 times 10 to the 8 meters per second is always constant. Lambda and frequency v will potentially change depending on the type of radiation you have. Lambda is wavelength size. v is frequency. Lambda and frequency, wavelength size and frequency are inversely proportional. As wavelength size decreases, frequency increases. As frequency increases, energy associated with those photons with that radiation increases. So, larger wavelengths, lower frequency, less energy associated with those wavelengths, with that radiation, with that light. We then talked about different portions of the spectrum. Talked about radio waves, microwaves, ultraviolet waves, infrared waves, visible portions of the spectrum. We discussed kind of their approximate wavelength sizes, the range of each of those wavelength sizes for different portions of the spectrum. And then their level of use in Earth observation remote sensing. Do we use x-rays very often in Earth observation remote sensing? No, we do not. Do we use near infrared very often in Earth observation remote sensing? Yes, we do. So that kind of stuff. And the last thing we talked about was surface interactions. So just that when light, when energy, when radiation hits a surface on the Earth, it is either reflected, transmitted, or absorbed. It has to go into one of those three domains of interactions. Any radiation that hits a surface of the Earth is either reflected, transmitted, or absorbed in some proportion. Maybe it's 100% absorbed and 0% transmitted and reflected. Maybe it's 50% reflected and then 25% transmitted, 25% absorbed. But 100% of the incident radiation of the incoming radiation is either reflected, transmitted, or absorbed. Okay. Going to talk about spectral signatures and NDVI in a lot of, in kind of more detail than I will with a lot of the other topics. Partly because I know it's a difficult topic for students to wrap their heads around, partly because there were some posts about it on the discussion board. And maybe, just maybe, there's another reason why I'm going over this in detail. Maybe. Okay. Take a hint. So a spectral signature is the pattern of spectral response of a material. It's typically visualized with a graph showing the percentage of radiation of different wavelengths reflected from an object. So here we have various spectral signatures plotted for different materials. We got the spectral signature of water, the spectral signature of green vegetation, and the spectral signature of soil. A spectral signature inherently can describe the spectral response of a material across really any range or any kind of portion of the electromagnetic spectrum. Often, or oftentimes, we only show the visible near infrared and mid infrared because that's what we commonly use as bands for detecting and measuring in earth observation remote sensing. But by definition, a spectral signature can cover any portion of the electromagnetic spectrum. We often just show visible near infrared and mid infrared. Now, by plotting spectral signatures of different materials together, like we've done here with water, vegetation, and soil, the portions of the spectrum where their signatures differ can be easily identified. We can look at this spectral signature of soil, green vegetation, and water, and say, okay, right on the edge here, a visible red light and near infrared light, we can see that vegetation has a high level of reflectance, soil has a medium level of reflectance, and water has very, very little reflectance. Using that knowledge, we can measure the radiation reflectance of different surfaces of the earth with remote sensing instruments with earth observation satellites, and we can use that information of their spectral signatures to say, well, according to these measurements, this area is probably soil, or this area is probably green vegetation, or this area is probably water. And we can say that because we measure near infrared reflectance over an area, and we know that near infrared reflectance is very high for green vegetation. So if we measure a high amount of visible green reflectance, then we, or sorry, of near infrared reflectance, then we can say, okay, that area is probably vegetation. If we measure near infrared reflectance in a particular area, and we see that the reflectance is very, very, very, very low, then we can say, okay, that's probably water. This process forms the basis of earth observation remote sensing, because it's how scientists are able to differentiate different surfaces of the earth using multi-spectral satellite data. By measuring the reflectance of different bands across the surface of the earth, we can differentiate different materials on the surface of the earth by relating those measurements to the known spectral signatures we have about those different materials. Now, NDVI is a vegetation metric or index derived from spectral remote sensing data, and in terms of vegetation, it tells us how healthy or unhealthy vegetation is. It measures how healthy or unhealthy vegetation is, and it takes advantage of two different spectral signatures. Takes advantage of the spectral signature of healthy vegetation and the spectral signature of unhealthy vegetation. The spectral signature of healthy vegetation, looks like something red visible reflectance high near infrared reflectance. The spectral signature of unhealthy vegetation looks like this. Much lower near infrared reflectance relative to healthy vegetation here and proportionally a little bit more visible red reflectance than what you see here. So this kind of boom goes up to here and this boom drops down to here boom from there to there if we're looking at unhealthy vegetation. This is the equation for NDVI near infrared reflectance minus visible red reflectance divided by near infrared reflectance plus visible red reflectance and it's a commonly used spectral or passive remote sensing satellite system or sorry it's commonly used with spectral or passive remote sensing satellite systems that we talked about in class like Landsat like MODIS. These instruments have bands measure reflectance in portions of the spectrum such as near infrared and visible red which allows us to use those satellites to derive the NDVI metric which essentially again tells us how healthy or unhealthy vegetation is. So if we look at a spectral signature here of healthy vegetation and then we look at the bands of say Landsat 7 we know that Landsat 7 has a band in the visible red and in the near infrared. We again know that with NDVI we use reflectance in the near infrared and the visible red in order to derive that metric in order to measure vegetation health. Okay so just to compare again the spectral signature of healthy vegetation on the left here the spectral signature of unhealthy vegetation here on the right. Again red when it's healthy vegetation very low reflectance but very high near infrared reflectance when vegetation is healthy. When vegetation is unhealthy much higher visible red reflectance much lower near infrared reflectance. This is the spectral signature that NDVI takes advantage of because we see low red reflectance and high near infrared reflectance for healthy vegetation we get a high NDVI value because we see higher red reflectance and lower near infrared reflectance for unhealthy vegetation we see a lower NDVI value. So NDVI takes advantage of the difference in visible red and near infrared reflectance for healthy versus unhealthy vegetation. Any questions about that? Okay I'm drilling it for a reason again. Okay I'm gonna end there. Okay next thing I want to talk about is active versus passive remote sensing. So want to clarify a couple terms in case you are using them on the final exam. I've kind of tossed around terms that maybe you guys haven't differentiated in your minds. So passive remote sensing, spectral remote sensing or spectral information and optical information. So passive by definition a passive remote sensing instrument involves radiation naturally being emitted from the sun bouncing off of the surface of the earth and being measured by the instrument. Spectral information relates to our ability with say a passive remote sensing system to measure different bands across the spectrum. To measure reflectance of different bands across the spectrum. So a higher amount of spectral information means essentially that you have more bands a larger portion of the spectrum being measured. And then lastly optical, optical essentially means visible. So if you're using the term optical then it essentially refers to anything measuring visible light. Optics, it's how things are actually viewed. So optical data essentially means data in the visible portion of the spectrum. Spectral data means information across the electromagnetic spectrum. And passive data, passive remote sensing means any data collected based off of radiation naturally being emitted from the sun hitting the surface of the earth and then being measured by the sensor. Oftentimes those things kind of overlap because passive sensors often give you a large breadth of spectral information including optical data. But they are technically different. So I just wanted to clarify those in case you're using those terms on the exam. Okay, just to drill home difference between passive and active remote sensing. Passive remote sensing is when energy is naturally emitted from the sun. Energy is reflected off the surface of the earth and then that reflection is measured by the sensor. Active remote sensing produces their own energy from the sensor from the instrument. That energy, that radiation travels towards the target, bounces off that target and then the sensor detects and measures that reflected radiation. Okay, some advantages and disadvantages of active remote sensing. Advantages, active remote sensing are often weather independent. Not all though. Radar is weather independent. Lidar is generally not. So lidar is typically not able to see through clouds. Radar is. It is however, in both the case of radar and lidar, sunlight independent. You don't need to have radiation being emitted from the sun and bouncing off the earth in order to collect active remote sensing data. So you can survey at any time of the day and you can control what energy you're emitting. You can use near infrared or visible red or visible green, whatever, you know, microwave radiation if you want, whatever wavelengths you desire. It potentially can penetrate vegetation, soil, ice and snow and give you information on surface layers and structure. Note there, again, key difference there between radar and lidar. Radar can penetrate through surfaces. Lidar cannot. Lidar can give you 3D structural information, but lidar cannot actually penetrate through materials. Radar can. Some disadvantages, it gives you limited spectral information because you're essentially only emitting and measuring one single wavelength. It's a much more complicated analysis than traditional passive remote sensing systems and they are often more expensive. Okay, important note, some advantages or disadvantages, mostly advantages, only applied to specific sensors or specific types of active remote sensing. So for example, active remote sensing systems that are weather that are weather independent that can see through clouds is only true for radar. It's not true for lidar. In an exam setting, if I ask you what are some of the advantages and disadvantages of active versus passive remote sensing, if you do not specify this, if you do not clarify this, then you won't get full marks. So if I ask you what are the advantages of active remote sensing and you say weather independent, but you don't specify that that's only true for radar, then you won't get full marks. Okay, had a request to quickly summarize some of the topics from the biosphere lecture. So the biosphere we started off by talking about what the fundamental driver of the biosphere is, which is essentially photosynthesis. We talked about measuring photosynthesis with gross primary productivity and net primary productivity. And then we talked about some of the factors that could affect GPP and MPP. In detail, we talked most about temperature and how temperature can affect GPP and MPP and how GPP and MPP change as temperature increases. We then talked about how we measure GPP and MPP with Earth observation data. We mostly focused on MODIS. Then we talked about carbon sinks and sources. We talked about what carbon sinks are, what carbon sources are. So sinks being things that take carbon out of the atmosphere more than they release it. Sources being things that produce or give carbon to the atmosphere more than they sequester it. Then we talked about dynamic sinks and sources. Forests, for example, the forest is, you know, burning, then it's kind of more of a source, but if it's a forest that's growing, that's healthy, it's more of a sink. And then we talked about how we can measure these changes in carbon sinks and sources using Earth observation data. We talked about measuring biomass with LiDAR using the structural 3D information you can get from LiDAR. We talked about using Landsat to detect and map disturbances like wildfires, like logging and forest harvest to be able to quantify some of these carbon sinks and sources and be able to get a sense of our carbon budget and carbon cycles all across the world. Okay, in class we often talked about kind of four main data types, data sets, sensors, satellites. We talked about Landsat, MODIS, radar and LiDAR in really the most detail. Talk about some other ones too that you'll see kind of maybe a little bit on the exam, but you'll see more or a heavier weighting of questions towards Landsat, MODIS, radar and LiDAR. So I'm going to use the cryosphere as kind of an example. So in the case of the cryosphere, what kinds of data can we get from each of these sensors, Landsat, MODIS, radar and LiDAR, and what are some of their applications? Then I'll end talking about what are some of the advantages of using Earth observation to monitor the cryosphere. Okay, so Landsat and MODIS inherently give us spectral data. They give us multi spectral data. They give us measurements of reflectance across various bands in the electromagnetic spectrum. So when you think of that, when you think you know spectral data, when you think about Landsat and MODIS, think spectral signatures. Think about differentiating different materials based off knowledge of spectral signatures. So for example with Landsat and MODIS, we can differentiate different cryospheric materials. The types of you know applications of Landsat and MODIS are mostly specific to their spatial and temporal resolutions. So Landsat has that moderate 30 meter spatial resolution. It's best for differentiating different cryospheric materials in kind of regional or moderate sized areas because of that finer spatial detail. So it can be used for things like ice and snow type mapping in specific glaciers or in specific mountain ranges. It's not super practical to map snow coverage across the whole world using something like Landsat. Makes more sense to probably use MODIS for that. If you're looking at more of a regional scale, specific mountain range, a specific group of glaciers all kind of together and you want a higher level of detail of the different types of ice and snow in those particular areas, Landsat's probably the satellite you want to use. MODIS on the other hand has that fine temporal resolution of one to two days. So it's best for either tracking things at a global scale because as that coarser spatial resolution or for tracking things that might change on a daily basis. So we for example looked at ice sheets breaking off in Antarctica. Something that really in terms of a spectral data set in terms of passive remote sensing, something only MODIS could track because it was changing on a daily time scale. If something's changing daily we can't really track it with Landsat because we only get a Landsat image every 16 days. Okay radar can give us information on backscatter and ranging. Backscatter is the strength of the radiation that bounces off the target which can be used for things like estimating snowmass, estimating ice cover and that's because the different levels of snowmass or ice cover will result in a different backscatter. It will result in a different level of intensity of microwave energy being reflected back to the sensor. Ranging is how far the target is from the sensor and that can be used for things like quantifying soil stability for mapping ice flow. So the ranging kind of gives us that 3D level of information. So we can see how shapes of surfaces might be changing because we can get things like topography. We can get three-dimensional information using ranging measurements. So from radar we can get backscatter or ranging measurements. Sometimes both and we'll combine them. Backscatter has to do with the strength of the signal returned. Ranging has to do with how far away the target is and each of those might have different applications. Again one of the unique things about radar is its ability to penetrate below the surface. Pardon me are you on the phone at the back there? Are you on the phone? You gotta get out. Oh please. I gotta be honest. The only reason I did that is because when I was in first year I did the same thing and the prof kicked me out. So you know I learned my lesson. So probably learn hers. Okay so one of the super unique things about radar is that it is able to penetrate below the surface. So in the case of the cryosphere for example it can be used for understanding snow and ice layers below the surface which can be potentially used to gauge and measure things like snow ice and age different types of snow and ice layers that exist below the surface and maybe where water tables lie below the surface or where the bedrock is below the surface. So with radar we can penetrate below the surface of the ice and get measurements of the different surface layers below the very very top. We can see how deep is the water underneath this ice or how deep is the bedrock underneath this ice. Things like that. Again only radar is capable to actually penetrate the surface. Spectral or passive remote sensing data sets cannot. Lidar cannot. However Lidar does give us high resolution three dimensional information. Again it does not penetrate the surface but we can get high resolution three dimensional information and we talked kind of in general about two types of data that we can get with Lidar. The terrain or topography information generally in the form of digital elevation models or DEMs as well as the vegetation structural information we can get to kind of build up the models of what the structure of trees might look like. Now in the case of the cryosphere really only the terrain information is helpful for measuring and monitoring ice there's not much structural information that we can derive but we can get really high resolution 3D sea ice topography and terrain measurements which can be really helpful for understanding things like melt patterns. We can get a sense for sea ice elevation above sea level so oftentimes ice set for example as it's orbiting will take really high resolution 3D measurements of the surface of the ice and then relate that to what the sea level is to then get a measurement of how deep the ice is. Again Lidar not penetrating the surface even if it's measuring how deep the ice or snow is. It's measuring in this case in the case of Lidar if it's measuring ice depth it's just measuring where the top of the ice is and then is relating that to some other sea level measurement only radar can actually penetrate through the surface. Make sense? Okay so depending on the data set advantages in kind of the context of the cryosphere you get a standardized data collection so we talked about how historically cryospheric information was derived from field data from digging snow pits from using ablation stakes kind of a subjective process lots of human error involved people might take different measurements different ways so it's not a standardized way to collect data we also talked about historical either aerial or terrestrial imagery or photography again potentially people could collect that data in different ways so earth observation particularly in the case of Landsat and MODIS these satellites or even ice sat are very very standardized they always collect the exact type of data all the time so you always have just a standard data set to work with also allows for really efficient data collection going out historically and measuring or taking photos of ice on foot not very efficient you got to get a plane you got to hike out there takes a lot of time takes a lot of resources not an efficient way to collect data with satellites or you know with airborne radar or Lidar very very efficient data collection coverage is very very high we can collect data over a large area very very quickly and that kind of just brings us to that next point which is just coverage as a whole it's hard with field data or with historical terrestrial or aerial photography to get good coverage to get imagery or data covering a large area with remote sensing with earth observation data we can get a large amount of coverage very easily potentially globally every single day if we're talking about same MODIS for example and lastly you can get different kinds of information that might have different advantages associated with them depending on the resolutions of the instrument or data you're using for example with Landsat we have that moderate spatial resolution that gives us a moderate to fine level of detail about the cryosphere about ice about snow with you know MODIS maybe we don't have that same fine to moderate level of spatial information but we can get global daily estimates again something that's very advantageous something that you can't do with terrestrial photography or aerial photography or field data that requires digging pits or using ablation stakes so that's a couple examples of kind of advantages of earth observation specifically for the cryosphere there's also ones you know that would be specific to the biosphere specific to oceans and freshwater they wouldn't really be any different in the sense that it's still all about standardized data efficient data collection good coverage potentially different advantages associated with the different resolutions of your data but they maybe just be kind of fine-tuned specific to that topic so for you know the cryosphere that I just talked about I was relating all of that to kind of historically how we've collected data right ablation stakes snow pits terrestrial photography aerial photography so with say oceans and freshwater you would then kind of relate your answer to a question like this to that topic so for example with oceans you know historically measuring sea surface temperature we use boys right boys or people that were in boats that were taking measurements out there boys don't give us good coverage there only a single point measurement so they might have a really good temporal resolution they might collect sea surface measurements every couple of minutes every couple of hours but you only get a single point measured with earth observation data you get much much much larger coverage again potentially globally every single day depending on the data set so you can kind of fine-tune and answer to a question like this such as what's the advantage of using earth observation data to monitor the cryosphere to monitor the biosphere to monitor oceans and freshwater you can fine-tune your answer using these exact same points that I discussed but kind of based on or relating to historically how we monitored and measured that phenomenon so I kind of focused in my example answer on the cryosphere but there'd also be you know a way to relate that to the biosphere to oceans and freshwater does that make sense okay thing I know the person's here but someone kind of posted what or asked what the advantages of using earth observation data were for each of these like for the biosphere for the cryosphere for oceans and freshwater and I feel like I kind of answered them all there at the same time because it's really all the same things you just need to relate your answer back to kind of the context of either the cryosphere the biosphere or oceans and freshwater make sense okay okay so I can't clarify what I necessarily mean by by linking topics without giving away the exam question too much what I will say is that you know again it's it's about getting you guys to bring together information from pre midterm and post-mid term so just as an example I had this question pop up on the discussion board and I thought this was a good example of you know kind of linking topics together what attributes and resolutions would be appropriate for if we are designing a satellite for monitoring regional forest cover change okay well there's a couple things that we talked about throughout the course related to this right we talked about data requirements for monitoring change so there's that kind of perspective to try to answer this question we talked about resolutions of satellites and different data sets so there's that kind of perspective to answer this question and then we talked about spectral signatures and the kind of different spectral responses you might have for different materials so that can kind of also be integrated into this question so for what attributes and resolutions would be appropriate if we are designing a satellite for monitoring regional forest cover change okay so there's a couple keys here first off would be regional the coverage is a regional area that we're looking at here regional we're not looking at a global scale just regional so not as big and then we're looking at forest cover change so probably kind of like a land cover change so then you're saying okay regional forest cover change in your head you should already probably be going to kind of Landsat Landsat's good for regional areas for monitoring things like land cover and land use change but why well we know for monitoring forest cover change for example monitoring wildfires monitoring forest harvest we know and can can consider some of the data requirements for that right so we have the level of spatial detail required the level of the frequency of revisit required the temporal dimension required and the spectral region required so we know for example for monitoring forest cover change okay the level of spatial detail we require pretty moderate nothing crazy the level of revisit time we require the temporal resolution probably also pretty moderate maybe once a month twice a month with suffice we just want to see the forest cover kind of before and after some sort of change maybe wildfires forest harvest the spectral domain the region of the electromagnetic spectrum well again linking back to earlier in the course we know that vegetation that's healthy has very high near infrared reflectance we know unhealthy vegetation has very low near infrared reflectance so we probably want to be looking in the range of the near infrared maybe visible red portion of the spectrum and then for temporal dimension how long do we need data for well how long are we monitoring forest cover for if we're just monitoring forest cover for a couple years we just need data for a couple years for monitoring forest cover change due to climate change then maybe we need data for decades and decades and decades okay so that's kind of the you know the data requirement side then there's the actual resolution side where I say okay well if I had to pick a satellite to use here which you know is kind of a different question than what we see here this is just you know asking you to describe the attributes and resolutions of a data set or of a satellite that would be ideal for monitoring this so that's kind of different and that's what I just did I just kind of described the characteristics you would need okay but what if I actually asked you to pick a satellite what if I said monitor forest cover regional forest cover change pick a satellite why that satellite well in this case I probably say Landsat probably say Landsat because it has a 30 meter spatial resolution it has a 16 day revisit time it has it to portal dimension all the way back to the 70s or 80s if we want to look at change all the way back then and it has bands in the visible red and near infrared portion of the spectrum so it would make it really ideal for this kind of monitoring so again question like this it requires those links right to a couple of different topics it requires you to think about potentially the data requirements for monitoring this change potentially the resolutions for a particular satellite and then potentially the spectral signature of the phenomena or of the material that we're talking about here okay I think that's a good example to get you guys going you can definitely kind of you know expect questions in the realm of this it's not exactly like this but note you know the difference in terms of me asking you okay pick a satellite and justify to me why that satellite is ideal for monitoring or detecting this phenomenon versus what resolutions or attributes would be ideal for monitoring this particular phenomenon the way that this question is worded it says for designing a satellite right so doesn't have to be something that actually exists so maybe in my absolutely perfect scenario for forest cover change maybe I don't want the 30 meter spatial resolution that Landsat has in my perfect perfect situation if I were to answer this question directly maybe I'd say well I don't know maybe a 10 15 meter spatial resolution would be ideal maybe a weekly temporal resolution would be ideal you know the the spectral regions in the visible red and near infrared would probably still be ideal and a temporal dimension maybe spanning a couple decades three four decades would be pretty ideal so I just described the characteristics of a satellite that doesn't actually exist but I did just directly answer this question in terms of what resolutions and attributes would be most appropriate for monitoring this change again I might ask you something like that I might ask you just pick a satellite tell me its resolutions why would it be ideal for monitoring this change yeah so for this question here you know this question is kind of asking you what attributes and resolutions would be appropriate if you were designing a satellite so it's not asking you to call on the characteristics of a satellite that actually exist so therefore for this can be a bit broad so you could just say oh a moderate spatial resolution would be ideal you could give a range if you want you could say you know a moderate spatial resolution maybe around 20 meters 50 meters somewhere less than a hundred meters would be ideal but all I would require you for this kind of answer right here is moderate course fine kind of thing if it's pick a specific satellite or data set tell me its resolutions tell me why those resolutions are ideal then I suspect specifics like you know 30 meter spatial resolution for Landsat 16 day revisit time spectral bands in the visible red and near infrared data all the way back to the 70s that kind of thing yeah yes yes yeah so that that's just fundamentally two different kinds of questions me asking you to design a satellite or to describe the attributes of the perfect situation the perfect satellite is different than me saying pick a satellite what are its resolutions why would it be ideal for monitoring this particular thing that kind of makes sense okay in general with the short answers be specific fully explain your answers may ask you for examples to help explain your answers point form as fine as long as you're still providing enough information to fully answer the question you should still be using the correct terminology if you're using point form you should still be linking topics appropriately and explaining your answers in full even if you're using point form so just make sure that if you are using point form it doesn't kind of take away from you know the quality of your answer so the important just just kind of final tips and tricks reminders things that are good to go over the important topics questions the kind of final slide I have at the end of each lecture it's a good thing to go over the practice questions we've provided on canvas are both those two sets of questions end of each of my slides practice questions on campus both very very representative of final exam questions they may not be the exact same but the type of questions and the content covered is quite representative post on the final exam discussion board if you have questions leading up to the exam you'll do great if you go over these things if you use those resources go over those practice questions you are gonna do great okay after the final exam our final exams on one of the last exam days so I have a really really tight turnaround to get final grade submitted so April 27th is a Thursday so we will mark the finals probably the following Friday and hopefully have them done by the end of Friday but if for some reason we don't the absolute latest that will post the final exam grades will be Monday morning so may first just crazy that's like a month away but the absolute latest that we will post final exam grades is may first at 9 a.m. if you have any concerns I'm sorry it's such a short window I'm kind of buckled by the deadline that UBC gives me if you have any concerns about your final exam you have to email Evan by 5 p.m. on that Monday on May 1st I'm gonna do our absolute best to get the final to get the final exam grades posted before then but worst case scenario they'll be posted by 9 a.m. May 1st and you have to email Evan that day by 5 p.m. May 1st if you have any concerns you want any grades change you want us to look over anything it's a hard deadline I literally have to submit grades the next day so if you haven't emailed us then we'll just go ahead and submit the grades okay I'll take questions in one second we have any general questions that anyone else wants me to answer I always end the semester by just thanking you guys it's been a ton of fun to teach you guys I don't you know do this job because I have to I do this job because I really like it and I really enjoy it and whether it's your first semester your last semester at UBC I hope you've learned something in this class hope you have a good journey ahead of you the positive interactions I have with you guys really really make my day they mean the world to me I hope you learned something in the class be kind be safe and that's kind of it okay is there anything anyone wants me to go over for the whole class if not you can kind of just come down I'll be here for another 15 minutes Julia is literally outside the door here so if you have questions about assignment 7 you can come down and shout out there All right. Hi, everyone. We can get started. Can you tell? Can any of you tell is this? Is this lecture hall much bigger than the other one? Or do we just have way less people today? What do you guys think? Do you hear people? And that's probably just because this isn't on the midterm? Probably. Yeah. Fair. Fair. Cool. Okay. So, today we are talking about resolutions, resolutions in Earth observation. It's one of two lectures. The end of class today, if you are still, if you have any last minute questions, he still has some office hours later in the week. But if you want to talk to Tristan about the assignment that's due this week, he'll be here toward the end of class. Welcome to chat with him. Today we're talking about, like I said, resolutions. We kind of rushed the end of, or I kind of rushed the end of lecture yesterday a little bit. So I just wanted to review the last concept that we covered, which was about vegetation indices. So we talked about two vegetation indices that you can derive from Earth observation satellite data. One of them was called the simple ratio, or called often the near infrared to red simple ratio. The reason that we talked about that one and its significance is that it was the first spectral or remote sensing derived vegetation index from satellite imagery and other similar kinds of data sets. It's very simple. It's just near infrared divided by red. If you have a higher simple ratio value, then that represents healthier vegetation. That's all you need to know about the simple ratio. NDVI, normalized difference vegetation index. The reason that we talk about NDVI is that NDVI is the standard today for spectral remote sensing derived vegetation indices. So today you were to try and find some sort of vegetation index from satellite imagery. For the most part, it's going to be NDVI or some other derivative of NDVI. You won't ever really see the simple ratio used these days anymore. NDVI can range on a scale from negative one to one, which was kind of its big improvement over the simple ratio. The simple ratio just kind of had an arbitrary range of values. But the NDVI value is standardized from values of negative one to one, values closer to one being healthy vegetation. And in general, you're never going to see an area that's covered by vegetation, whether it's healthy or unhealthy, with an NDVI value lower than zero. So if it's lower than zero, it's probably not vegetation cover. It's probably not a forest or grass or some other form of vegetation. If it's zero to one, then it's probably some sort of vegetation. If it's closer to one, then it's healthier. If it's closer to zero, then it's unhealthier. Now both metrics take advantage of the exact same process or the exact same phenomenon, which we talked about in detail when we talked about spectral signatures. They take advantage of the fact that healthier vegetation will have higher near-infrared reflectance and lower visible red reflectance, while unhealthier vegetation will have lower near-infrared reflectance and higher visible red reflectance. So these are the equations for the two indices. They both take advantage of that same phenomenon. They just essentially scale the values differently because NDVI just standardizes them from negative one to one, whereas the simple ratio you might get a wide range of arbitrary values. Again, just to drill at home, they both take advantage of the exact same process, which is just that. If you're looking at the spectral signature of healthy vegetation here in green, you're going to have much lower visible red reflectance, much higher near-infrared reflectance. That's going to give you a higher NDVI value. If you have unhealthier vegetation, you're going to have much lower, sorry, much higher visible red reflectance, much lower near-infrared reflectance, you're going to result with a much lower NDVI or simple ratio value. Any questions about that? But NDVI, that will be on the midterm. All good? Okay. Sweet. Just make sure this is recording. It is. Okay. Today, we are going to talk about the different resolutions that are important to consider in Earth observation. We'll try to understand why they're important when imaging the Earth. Then we're going to talk about three different types of resolution. Spatial resolution, spectral resolution, and temporal resolution. We're going to define each of them, discuss their importance, and then hopefully you can hopefully you'll get a holistic view of how all of these different resolutions can result in different kinds of data, different kinds of satellite imagery, and thus different applications for that imagery. But you can definitely expect that a lot of the topics that we talk about today, especially resolutions, there will be a lot of questions about that on the final exam. It's a really, really important topic when it comes to Earth observation. Okay. To start, I just have this intro video that kind of talks about some of the stuff we talked about yesterday, and then also introduces some of the concepts we're going to talk about today. This slide might come from the sun, but it might come from other sources, like light bulbs. So, we also use cameras to take selfies and silly pictures of our furry friends. Scientists use high-powered cameras called imaging spectrometers to measure changes in things that impact our environment, like water quality, or vegetation cover and health. Imaging spectrometers mounted on airplanes and satellites help us create maps like this vegetation cover map for the entire United States. But how exactly do scientists measure changes to our environment, using a reflective light energy? To answer this question, let's have a look at the electromagnetic spectrum, which is composed of thousands of wavelengths of energy. Physical light, what we see with our eyes, is containing the blue, green, and red portions of the spectrum. The rest of the spectrum is not visible to human eyes, but can be detected and recorded by sophisticated camera-like sensors called the imaging spectrometers. Now, there are thousands of wavelengths to record the electromagnetic spectrum. To deal with all these wavelengths, imaging spectrometers to find a spectrum into groups of wavelengths called bands. For example, a band in the near-interred region of the spectrum could include energy from 800 to 850 nanometers. This band is useful to map healthy vegetation. The width of the number of bands will be called a spectral resolution of an image. Higher-sector resolution meets more bands that are spectrally more narrow. Lower-sector resolution meets fewer bands, each of which covers more of the spectrum. Now, imaging spectrometers measure reflective light energy. You see, different objects reflect absorb entrance mid-light differently, depending on their chemical and structural characteristics. For example, the bandies are green because they reflect more green light and the blue or red light. On the other hand, finding the dog reflects more light in the red portion of the spectrum because of the chemical and structural makeup of his fur. If one of the chemicals or structural makeup wasn't seen as a plant, then he was a breed. Now, when you point your camera towards your favorite canine doing something silly, the camera records the amount of light reflected from the dog and its surroundings. In the visible or red-greened blue bands of the electromagnetic spectrum, the camera creates what's called an RGB image, which is composed of millions of pixels. Each pixel in the image contains a value representing the amounts of red, green, and blue light reflected. We can break the image out into its red-greened blue bands, too. Here's the red band on its own. Brighted pixels mean that more light has reflected by objects in the image and recorded by the camera in the red part in the electromagnetic spectrum. The darker parts of areas were in the west light was recorded. When we combine these red-greened blue bands together, we get an image that looks similar to what we see through the camera lens. We can plot the amount of red-greened blue light recorded in each pixel to create what's called a spectral signature. In this signature, the amount of energy reflected in a particular wavelength is shown in the y-axis, and the full range of wavelengths that were measured by the camera. In this case, blue-greened red is on the x-axis. The spectral signature for Fido is quite different from the spectral signature for our plants. This makes it appear visually different to our eyes, too. Differences in spectral signatures can help scientists identify different types of surfaces in objects within images. Most cameras record light in invisible or red-green blue bands. However, plants, dogs, and other objects on the earth also reflect light that we can't see with our eyes. For example, plants reflect up to 60% more light in the near-end red portion of the electromagnetic spectrum than they do in the green portion of the spectrum. This is why differences in reflected light in the near-infrared portion of the spectrum are important for mapping vegetation on the ground. To measure these differences in the non-visible portion of the spectrum, we use imaging spectrometers, which record light in both the visible and non-visible parts of the spectrum. Imaging spectrometers produce what are called multi- and hyper-spector mode sensing data. Multi, meaning many bands, more than three, and hyper-meaning up to hundreds of bands, cause it at very high spectral resolution. We use these multi- and hyper-sector mode sensing data sets to measure light energy reflected from objects on the earth's surface and to estimate many physical and chemical properties of objects that we wouldn't see with our own eyes. We then use this measurement to classify what's on the ground. For example, pixels that have a spectral signature with a lot of non-ear infrared light energy are often vegetation. To review, different objects reflect absorb and transmit both visible light and light energy that we can't see differently. Imaging spectrometers record the amount of light that these objects reflect. The amount of light energy reflected by an object throughout the electromagnetic spectrum is called its spectral signature, which is driven by the physical structure and chemical makeup of the object. We can use these signatures to identify different objects in both a photograph and across the earth's surface. And that, my friends, is how we use reflective light energy to both map this on the ground and measure changes in our environments. Okay, so they kind of mentioned they really went over in detail spectral resolution a little bit, but they also reviewed the concept of spectral signatures, which was the focus of what we talked about yesterday. So we're going to talk about today three different kinds of resolution. Spatial resolution, spectral resolution, temporal resolution. Spatial resolution and temporal resolution are a little bit easier to wrap your head around. Spectral is usually the one that students struggle with the most. So if you have questions, feel free to stop me. If I'm going too fast, just tell me to slow down. Okay, so first off we have spatial resolution. Spatial resolution is the smallest possible feature or object that can be detected with a satellite image. With digital satellite imagery, which is pretty much the standard, that's the kind of imagery we have today. It's the minimum area that can be resolved by the sensor, generally denoted by the pixel size. So any image that we take, say this is a satellite image here, and we zoom in on it and zoom in and zoom in and zoom in. Eventually you'd see a pattern that would look something like this. You'd see this grid of pixels that would have each, a different shade of associated with them, a different brightness associated with them. The brightness or the shade that that pixel has is denoted by something called a brightness value or a digital number. So in this case we have here just what's called a panchromatic image. It's just a black and white image where we're showing overall the amount of light that's reflected all across, say, the visible to near infrared part of the spectrum for this image here. And we're saying, okay, let's zoom in to just one section of the image right in the middle here. Let's zoom in, zoom in, zoom in, zoom in. Eventually we'll see something that looks like this, this kind of grid of pixels here. Each one of these pixels has a value, a numerical value associated with it, that numerical value describes how much light was reflected off that area that's described by the pixel on the surface of the earth. Okay, so if we look at this kind of grid of squares on the right here, we see a value of zero, that's going to be shown very, very dark as a black pixel. There's very, very, very little to no reflectance of light, of radiation in that specific pixel. And so it gets a value of zero and it's displayed in our image as a completely black pixel. On the other hand, if we look at this value here, a very high value here, 255, that's a very, very bright pixel. That's an area on the ground where there is a high level of light or radiation reflected off of that part of the ground back to and measured by the sensor. And we can see here, that is denoted by a very, very bright pixel or a very, very bright square and white there. The size that these, each of these pixels, each one of these individual pixels, which is each measuring the reflectance for a particular area on the ground, the size of one of these pixels, meaning the area that they represent on the ground is what's called the spatial resolution. So if I have a pixel that's say 30 meters by 30 meters, or in other words, I have a sensor on board a satellite or a plane or whatever it might be, and it's able to resolve areas on the ground at 30 by 30 meters, which means it's measuring the reflectance for discrete parcels of surfaces of the earth at a 30 by 30 meter scale, then that's what my spatial resolution is. It's 30 by 30 meters. Generally speaking, resolutions on satellites are always fixed because satellites are orbiting the earth at a constant altitude, whereas if they were onboard an airplane or something, it could be variable depending on the altitude that that airplane was flying at. So it might depend. That's not too important for us. We'll mostly focus on satellite imagery. So if we took this image here, say this was a very fine resolution satellite image, then this case it's an image of Alberta, and it's a forest in Alberta, it's a lodgepole pine forest in Alberta, and we zoomed in on it right in this spot here, we said okay let's zoom, let's zoom, let's zoom, let's zoom. Now we can start to see some of that pixelated effect in this image, right? We can see those little squares. We know that in this case we're looking at a lodgepole pine forest in Alberta. We know lodgepole pine trees are about three meters or so wide, and we can see here we can kind of start to make out those individual trees here and here and here and here, and I can see there's you know maybe about five to a dozen individual pixels covering one of those trees. If I know that those trees are about three meters wide, then I can guess that those pixels are maybe 20 to 50 centimeters wide, which means that my spatial resolution is about 20 to 50 centimeters. If I zoom in even more and more and more again you can see that pixelated structure very very well. The area that these individual pixels represent on the ground in real life is what we describe as our spatial resolution. Does that make sense? Any questions about that? Okay. The importance of spatial resolution is that it directly relates to how much information or detail we can see in an image. If we go from a very fine resolution image here to very course here, where we have pixels that are 15 meters by 15 meters to pixels that are 30 by 30 meters, 60 by 60 meters, 120 by 120 meters, 240 by 240 meters, all the way to 480 by 480 meters, then that means as we get to this coarser and coarser and coarser spatial resolution, we have pixels that are representing larger and larger areas on the surface of the ground. That means that when we look at that image, it's going to give us way less detail with a larger pixel size or a coarser spatial resolution than if we look at an image that has a much finer spatial resolution with a much smaller pixel size. So coarser resolution has less detail and information and finer resolution has more detail, more information as you can see this. You can see this fine resolution image here is really detailed. We can make out a lot of the urban environment here. We can make out a lot of the different kind of mountains and vegetation on the side here, but when we get that coarser spatial resolution here, it's much harder to make out those kinds of details. All we can really see is kind of the boundary of the lake and maybe some rough boundaries of generally just dark areas versus brighter areas. Does that make sense? Any questions about spatial resolution? Just how we define it? So we just define it with the size of the pixel that we have in our image. And that just means that each pixel is measuring how much brightness there is, how much reflectance there is off the surface of the earth for a particular area. The size of that area, the size that this pixel represents in real life is our spatial resolution. So in this case, if one of these pixels was 20 centimeters by 20 centimeters wide, that would be a 20 centimeter resolution of spatial resolution. Make sense? Yeah? Yeah? So it's dependent on how far the sensor is from the ground. Generally, yes, we don't really go into that in depth very much because that's really only something you have to consider with aerial or drone imagery where you're controlling the altitude that you're flying at. With satellite imagery, it's always at a constant altitude, so its resolution is always fixed. It never changes. But with aerial and drone imagery, yes, it depends on the height that we're flying at. So cover more the pixel, cover more area. Correct. Yeah, correct, exactly. Yeah. Okay, so to summarize, our spatial resolution is typically defined by the pixel size. And that is generally just a function of the platform and the sensor specifications and geometry as well as maybe if we're doing aerial imagery, how high we're flying. But there's two other important considerations that we have to look at when we consider spatial resolution, because there's really two ways to think about spatial resolution. One of them is just the pixel size, the smallest area that we measure reflectance for on the surface of the earth. The other is the smallest object or feature that we can actually measure or look at on the surface of the earth with that image. We might have a constant pixel size, but we may be able to detect smaller or larger images or smaller or larger features or targets of interest on the surface of the earth, depending on things like the spatial arrangement of those targets and the data quality that we have on a given day from a given sensor from a given satellite. So the first one to consider is this example here. So let's say we take an image, we have a satellite image of an area where there are some tennis courts, right? Tennis court, we have black concrete with these white lines over that concrete. The spatial resolution of this image is 60 centimeters. So the pixels in this image are a size of 60 centimeters by 60 centimeters. That means that in this image, we've measured reflectance for a 60 centimeter by 60 centimeter area all across this image. Now, if we know that these tennis court lines are less than 20 centimeters, so much smaller than our pixel size, why would we still be able to see those lines in this image? So say we know that these lines, these white lines on the tennis court are about 15 to 20 centimeters in width, but our spatial resolution for this image, our pixel size, is only 60 centimeters. Somehow, we're still able to see those lines. Why do you guys think that might be? I'll let you brainstorm. Talk about with someone with your neighbor for a couple minutes and then we'll come back and discuss it. Brainstorm, brainstorm. Have at her. Give her. Guys are quiet today. What do you guys think? What do you guys think? Well, if you look at the image, the lines are actually white. They're a little gray. So with the 60 centimeter, each pixel has some of the line in it, but it also has some of the dark in it. So the white ends up being tainted and it ends up being white. So the whole pixel ends up being gray color. Yeah, exactly. Yeah. Does that make sense? Yeah. It's kind of like averaging that whole area that it's looking at. Because there's white and black, it just appears as like a really light gray kind of thing. Cool. That makes sense? It's like the digital number, the brightness value. So you'd have, generally speaking, you'd have a different digital number or a different brightness value for each band or channel of the electromagnetic spectrum that you're measuring. So you'd have a reflectance of blue light, reflectance of red light, reflectance of green light, reflectance of near infrared light, but you get to define what that band or channel looks like. So in that black and white image, we were just getting one single reflectance value for all the visible light. So we said just if there's a really large range of wavelength sizes that we can detect, what's the overall reflectance for all of those? So it wasn't, in that case, it wasn't an average. It was kind of just like a total for all the visible part of the spectrum. Okay, anyone have any ideas? I know you do because we just talked about it. Anyone not in the front row that I didn't just chat with want to have a swing? Yeah. Yeah, essentially because the lines are so reflective of visible light and the surface that they're on is not. So if we have black concrete with a white line going through it, the white line is 20 centimeters wide. Our pixel size is 60 centimeters wide. That means we kind of have, you know, this line here, but our pixel is kind of all around it. Our pixel is going to measure the reflectance, how many photons essentially are bouncing off that target, that 60 centimeter by 60 centimeter area. But it's just going to, it's just going to measure kind of the average or total amount of photons that are bouncing off that target. If we have a white line that's sending a ton of photons back to the sensor, a ton of light back to the sensor, but then we have a adjacent black concrete that's sending very, very little to no photons back to the sensor, we're only getting one measurement for that whole area, which means that overall we're still going to get quite a high level of reflectance because we have that even little sliver of a white line reflecting all the visible light very, very, very strongly. And because we're just kind of getting a an average or total measurement of that whole 60 centimeter by 60 centimeter pixel, it's still going to appear relatively bright in our image here. So then, would you say that the pixels in this image are 20 centimeters, is our spatial resolution or our pixel size 20 centimeters in this image? What do you guys think? No, yeah, someone shut it out. No, our spatial resolution hasn't changed, our pixel size hasn't changed. We're still dealing with a 60 centimeter pixel, but the smallest object that we can resolve or that we can detect on this image is actually smaller than our pixel size in this case, just because of the spatial arrangement of the targets, in this case, a very bright white line over top of a very dark black concrete surface, and the different spectral responses of those surfaces, one being very, very high in all visible light reflectance, one being very, very, very low to no visible light reflectance. Yeah. So why isn't this image just 60 centimeter chunks of generalized reflectance to that 60 centimeter chunk? Like if it's aggregating the amount of photons coming back, why isn't part of these 10 sports just average white chunks in 60 centimeters instead of this resolution that you can see? Like why are we able to kind of see these two adjacent lines next to each other and they're not just kind of merged together? Yeah, or just like the whole, but like the other, the photos that we can't see is like, yeah, right. So that's because we're still looking at a pixel by pixel basis, right? So if we have a 60 by 60 centimeter pixel with the 20 centimeter white line going through it, and that's for that pixel, we get generally a pretty high level of reflectance overall, then we have an adjacent pixel to that that's also 60 centimeters by 60 centimeters, but there's no white line going through it. We're measuring the reflectance for just that pixel, it's going to be really, really low because it's just a black concrete area that we're looking at. Yeah, yeah, I mean you can kind of, you know, you can kind of see them a little bit, the pixelated effect, but yeah, if you zoomed in on this image close enough, you would start to see the individual pixels. Yeah. It doesn't use your way to say that would just be there's at least a 60 centimeter gap between the two white lines. Yeah. You're going to have one gray, pink, white box, one black box, and then the next one is going to be white again. Exactly. Yeah, does that make sense? Yeah. Yeah. Yeah. Yeah. Okay. So that's one thing you have to consider when you're essentially looking at what's often termed the effective spatial resolution, which refers to the smallest object or feature that you can actually look at on an image versus the spatial resolution purely related to the pixel size of the image. The other thing that we often consider is the data quality. So poor atmospheric and light conditions when you're taking a satellite image over an area can often reduce the effective spatial resolution or the smallest object that you're able to look at on a given image. These two images are of the exact same area using the exact same sensor, but you can see here you can make out a lot more detail in this image because it was taken with better light and atmospheric conditions. The atmosphere, the components of our atmosphere are changing all the time. And as a satellite goes very, very quickly over kind of the top upper layer of our atmosphere, the conditions that it will pass over will always be varying. Sometimes there's really good atmospheric conditions for satellite imagery to be taken. Sometimes there's very bad atmosphere conditions for satellite imagery to be taken, and that can have a very large effect on what you're actually able to see in an image. So in this case, our pixel size is the exact same, but the poor conditions increase the size of the smallest detectable object or feature that you can actually look at in this image. Make sense? Any questions? A couple of nods here and there. All right. So generally speaking, we can classify spatial resolution into four broad categories, the first being low spatial resolution. So low spatial resolution, we generally classify where pixel size is greater than about 100 meters. The low resolution satellite that we talk about in this course a lot is the Terra and Aqua satellites, which have the modus sensor on board them. They have a spatial resolution of about 250 meters to 1000 meters or one kilometer. These kinds of data sets, these low spatial resolution data sets are really good for looking at very, very broad areas, for looking at things like, you know, land cover type or sea surface temperature or vegetation phenology for an entire continent or for the whole globe. Because each pixel takes up such a large area on the surface of the ground, it's very easy to make up maps or images covering the entire surface of the earth or covering an entire continent, an area that might be very, very large. Generally speaking, this kind of imagery in the case of modus, for example, is free. It's typically government owned and operated. So anyone can go on to a website called Earth Explorer and download modus imagery, as well as other kinds of low spatial resolution imagery for free, really anytime you want. But the key here in terms of the applications of low spatial resolution imagery is that you don't require smaller pixels in order to, you know, differentiate the different surface covers that you're looking at or the different objects or features that you're interested in when you're using low spatial resolution imagery. Then we talk about moderate spatial resolution imagery as about smaller than 100 meters in a pixel size. The platform that we often talk about and will talk about throughout this course, that's a really good example of moderate spatial resolution imagery is Landsat. Landsat has a bunch of different Landsat satellites ranging from one to nine, Landsat one to Landsat nine, as well as a bunch of different sensors. We'll talk about the thematic mapper and the operational land imager. It has a spatial resolution, Landsat that is, of about 30 meters. So 30 meters by 30 meters in pixel size. We're looking at here Landsat image of Vancouver. So you can see we can make out kind of the difference between, you know, at a much finer scale, we can see the river here going kind of through the Fraser right there. We can see kind of more agricultural rural areas down here. We can make out forests and mountains up here. We can make out differences in kind of the sediment plume that's coming out from the Fraser River here. We can make out, you know, urban areas here and even just small forested areas within those urban areas. Details that you wouldn't be able to see when you were looking at Vancouver from this scale here, from an image that had a much larger pixel size. So the applications of moderate spatial resolution allow for mapping things like forest cover, insect infestation, crop forecasting, or land cover at maybe a much finer resolution. One of the biggest things that moderate spatial resolution data is really good for that is much harder with course or low spatial resolution data is mapping disturbances, mapping things like how much logging is occurring, how much burned forest there's been from a forest fire, how much area has been or how much forest has gotten plowed down by an avalanche or by a landslide or something like that. Landsat and moderate spatial resolution data is often necessary to measure and track those things over something like MODIS, which is a low spatial resolution data set. Landsat is also operated by the government, by the US government, and it is free and open source. So anyone can go online again and download Landsat data whenever you want. Okay, then we have high spatial resolution imagery, which we generally classify as imagery with smaller than about a five meter pixel size. This image here is from the iconic satellite, which is no longer operating, but this example is a satellite image with a spatial resolution of about four meters. And you can see now we're able to make out individual buildings, individual roads, making this high spatial resolution data really valuable for urban mapping, for road mapping. The largest difference between high spatial resolution data and moderate spatial resolution data is that high spatial resolution imagery is never free. It's always privately owned and operated, which just means it gets quite expensive. You have to pay licenses or pay kind of buy a square kilometer in order to download and access some of this high spatial resolution imagery. So you generally have to have some funding from somewhere in order to get your hands on some of this high spatial resolution data. Okay, for the most part, we talk about high spatial resolution data being satellite derived. Very high spatial resolution data is generally collected from airplanes and drones. So the sensor can just be a digital camera or some kind of fancier spectral radiometer or other sensor that we might mount on a drone or plane in order to fly over and take some imagery of a certain area of interest. The spatial resolution can be quite variable depending on the sensor you're using, depending on the altitude that you're flying that drone or airplane over a given area. It can be all the way down to less than centimeters now. So we can get imagery that's 10 millimeters by 10 millimeters on some very high resolution sensors that are often flown on drones at very, very low altitudes. The biggest application for those in kind of a conservation environmental application sense is for monitoring individual trees. So when we fly drones and aerial imagery at relatively low altitudes, we can see individual trees in those images. And because we have many, many pixels representing the reflectance for an individual tree, we can very, very accurately monitor the health and progress and growth of individual trees within say a forest stand. So we can measure their health if they're being infested by insects. We can monitor how big trees are if they're growing, what their shape is like, a bunch of different things. And that's kind of the new age form of earth observation remote sensing is this push towards drones and their applications. Which we'll talk about in a lot more detail towards the end of the course. Drones and aerial imagery, you can also imagine for the most part are not free. You have to pay for a pilot or someone to go and fly your airplane or fly your drone. So it's often, you know, it often requires funding as well, generally not free to be able to go get aerial or drone imagery, which means that for very high spatial resolution imagery, you're gonna have to pay some money for it. Okay, any questions about different kinds of spatial resolution imagery, spatial resolution as a whole? No, yes, no? Okay. The next kind of resolution that we'll discuss is spectral resolution. Spectral resolution is usually the type of resolution that students struggle with the most. So if you have any questions, if I'm going too fast, let me know. We can define spectral resolution as the number and dimension of specific wavelength intervals in the electromagnetic spectrum to which a remote sensing instrument is sensitive to. That's kind of the long sentence form and kind of point form how we can describe spectral resolution is with three components. The number of spectral channels or bands being used, their location in the electromagnetic spectrum, and the bandwidth or range of each of those channels or bands. So if I were to ask you, say on a final exam, what are the three components of spectral resolution? You would say the number of spectral channels used, their location in the electromagnetic spectrum, and the bandwidth or range of each of those channels or bands. If I were to ask you on the final exam, hint and wink, wink, that's not true. So if we were to look at a spectral signature of vegetation, right, of healthy vegetation, we see that characteristic green hump in the visible part of the spectrum, that high level of near infrared reflectance, and then kind of a steady drop off as we get into the shortwave infrared part of the spectrum. We said, okay, for this particular sensor, we want six different bands or ranges across the electromagnetic spectrum that we want to measure reflectance for. We could say, okay, well, we're going to put band one here and kind of the blue part of the spectrum. We're going to put band two here in the green part of the spectrum, band three in the red, band four in the near infrared, and then band five and seven here over in the shortwave infrared part of the spectrum. Now this is the actual depiction of an actual spectral resolution from the Landsat seven satellite. And you'll notice that it goes from band five to band seven. That's just because Landsat has a sixth band called band six, but it's in the thermal infrared part of the spectrum. Not sure why they don't just call that one band, you know, eight or band seven instead, but they call it band six. It's in the thermal infrared part of the spectrum. So it's not shown on this graph here because the wavelength size of a thermal infrared band would be theoretically way off here in a much, much higher wavelength size or longer wavelength size. But essentially, Landsat seven from the visible to shortwave infrared part of the spectrum senses reflectance in each of these bands. So from wavelength sizes of about 430 to 500 nanometers, it's able to measure reflectance of just those wavelengths, just those photons with that size wavelength. Then it's also simultaneously able to take a measurement of wavelengths with a size of say 500 to 560 nanometers and measure the reflectance of just those wavelength sizes, separately from the wavelengths in the band one here. Then it does the same thing for band three and band four and band five and band seven. So for each pixel, Landsat seven as an example, we'll get a measure of reflectance, how many photons are bouncing off a target, going back to the sensor for each of these bands. So it'll say, okay, how many photons am I getting for band one, for wavelengths that have a size in this range, for band two, for wavelengths that have a size in this range, for band three wavelengths that have a size in this range, and et cetera, et cetera, et cetera. Does that make sense? Have I lost anyone? Guys, they're so quiet, it's scary. Okay, I'm gonna assume hopefully. Okay, so if we looked at this in kind of a table form as opposed to a graph form, we have here our band numbers and where they are located in the spectrum. So we see here, okay, there's seven bands for Landsat seven. They're located in the blue, green, red, near infrared, shortwave infrared, thermal, and again in the shortwave infrared part of the spectrum. And then each of them, band one goes from 450 to 520 nanometers, band two goes from 520 to 600 nanometers, band three goes from 630 to 600 nanometers, 690 nanometers, so on and so on and so on. I'm not gonna keep going through every single one because I will get tongue tied. But the point is, Landsat, the thematic mapper sensor on Landsat, if you were to describe its spectral resolution, you could also say, well, it's got seven channels or seven bands. They're located in the visible, near infrared, mid infrared, and thermal infrared part of the spectrum. And they have bandwidths ranging from 60 to 270 nanometers or additionally, the thermal one is about 2200 nanometers wide. So you could describe the bandwidths or the range of wavelength sizes that each band or channel is sensitive to by just saying what the start and end point is of each band or by just saying approximately where they are located in the spectrum and then what the actual width is in nanometers of each one. Make sense? Yeah? So is spectral resolution defined as like, what's up to get a higher spectral resolution? Does it just mean that it takes like, it can see light, detects light from more of a spectrum? Correct. Okay, so that has nothing to do with the actual size of each band. It does a little bit. Those things are inherently connected. So if you increase your spectral resolution, you're going to have more bands, but each of those bands is going to be thinner. Does that make sense? Yeah. So if you have what's called a panchromatic image, which just has one band, it would be one large broad band. It would measure photons with a size of 430 nanometers all the way to 700 nanometers. It'd be one broad band. That'd be the lowest spectral resolution. You then go to a multi spectral sensor, say a moderate spectral resolution. Maybe you're looking at something like Landsat 7, where you have about seven spectral bands. Now though, they're not as wide, right? The panchromatic one, the one that had the lowest spectral resolution, had this one broad band covering this whole part of the spectrum. For Landsat 7, each band covers a smaller part of the spectrum, a smaller region, but there's more of them. So as the part of the spectrum it covers gets bigger, you just add more and more bands. And they each get thinner and thinner and thinner and thinner. Oh, so this is the way to like calculate resolution, just be the upper band? Yes, but the technical definition and what I would expect from you on an exam, if I asked you to define spectral resolution or describe spectral resolution, I'd expect the number of bands, but not just the number of bands, also where they're located in the spectrum, say visible, near infrared, sorghvive infrared, and the width and wavelength size of each of those bands, or just where they start and end specifically in nanometers or micrometers on the electromagnetic spectrum. Does that make sense? Because technically speaking, and the reason for that is technically speaking, you could just add more bands, but you know, if we're just adding more bands and they all have the exact same bandwidth, then that's not really describing what our spectral resolution is per se. You need to be able to describe where the bands are located, how wide each of them are, and how many of them there are. Make sense? Cool. Okay. Any final questions on spectral resolution? Yeah? So we haven't talked about what the spectral resolution of modus is yet. We'll talk about that in our resolutions part two lecture, which will be the first lecture after the midterm, but modus has relative to lance that, what you would probably describe as a higher spectral resolution, it has more bands. Okay. So it's sensitive to a larger number of bands across the electromagnetic spectrum. So then, sorry, how is this spatial resolution connected to the spectral resolution? We haven't really talked about that at all. Oh, sorry, then I, that's okay. No, we were just talking about spectral resolution, just in the context of spectral resolution. It is all of these resolutions are inherently connected. But again, we'll talk about that in our next lecture, or the next resolutions lecture at least. Okay. Last resolution we're going to talk about today is temporal resolution. This one, I think, is probably the easiest to wrap your head around. So temporal resolution is just the amount of time it takes to revisit the same place on earth. We know satellites are orbiting the earth, they're always orbiting the earth, and they're taking images of the earth as they orbit it. The amount of time it takes for a satellite while it's orbiting to return to the same point on the earth to take another image of that point on the earth is what we describe as our temporal resolution, often just termed revisit time. Now, it's really important for a variety of reasons. The most being at the kind of courses scale that it controls the level of or the scale of time or change analysis that we can look at. We know that our environment is changing, but our environment changes sometimes on the scale of a couple minutes, a couple hours, a couple days, a couple years, a couple decades. The temporal resolution of a satellite essentially defines what temporal scale we can look at when we want to monitor change. Say, for example, vegetation phenology, whether vegetation is green or whether it's starting to sinesse and get yellow and then die off in the winter and then spring comes back and it gets green again. That's a seasonal cycle, right? We probably need a temporal resolution or an image at least once per season, once in the summer, once in the fall, once in the spring, once in the winter, I think that's the season I missed, in order to be able to quantify that seasonal change in vegetation. On the other hand, sea surface temperature, for example, changes on a daily scale. Certain days temperature is warm, other days temperature is cool. We need daily imagery or daily data to be able to monitor that change. Another example is deforestation, forest getting cut down. It takes sometimes weeks or months or years for an entire forest to be cut down if it's being deforested or if it's being logged, whatever that case might be. If you want to just measure for a certain area using satellite imagery, how much forest has been lost, really all you need is an image before the forest cut down and an image after all that forest got cut down. Say that's five years apart. Say it took someone or an organization, a group company, whatever. Say it took them five years to cut down all the forest in a particular area. Then the temporal resolution you would need to measure how much forest they cut down would only be about five years. You would need an image from before and from after. This is about really the difference when you're trying to monitor changes that happen on a daily scale or maybe happen on a seasonal scale or maybe happen on a yearly scale or maybe happen on a multi-annual scale taking decades or longer. Maybe we need an image of a certain area that's same area once every day. Maybe we only need an image of that area once every month, maybe only once every year. But that's what our temporal resolution is describing or defining. Now our temporal resolution generally depends on two key things. One being the orbit that the satellite is in. We know that Earth observation satellites in general are in one of two orbits, either a close elliptical orbit or a geostationary orbit. Generally speaking, most of these satellites, most Earth observation satellites that we look at, Landsat, MODIS, etc. They're in a close elliptical orbit. We know that a geostationary orbit is always looking at the same point on the center of the Earth, not the center of the Earth, but the same point on the Earth. If a satellite is in geostationary orbit, it's always looking at the same point on the Earth. What do you think its temporal resolution would be? If it was taking an image every 24 hours, yeah, you could say that. But yeah. Essentially it'd be constant. It's always looking at the same point on the Earth. It can take as many images of that same point on the Earth as it is capable to do. Essentially, it can take images every minute, every 30 seconds, every hour, if it's in a geostationary orbit, because it's always looking at the same point on the surface of the Earth. If it's in close elliptical orbit, which most Earth observation satellites are in that we'll discuss, then its temporal resolution is more related to its swath width. So close elliptical orbits or satellites in close elliptical orbits, generally speaking, have polar or near polar orbits, which means that they orbit from north to south over the globe. So their swath, their path over the Earth kind of looks like this. They kind of come down over the pole, orbit down to the south pole, and then return back on the other side and keep orbiting like that. The swath width is the geographic width or area that the satellite sensor is able to take images of as it crosses over that point or that area on the surface of the Earth. And we know that from one of our last lectures, actually, I don't know if I talked about that with you guys, that might be a different class, but in general, when we have a coarser spatial resolution, same modus, because the pixels are larger, they generally create a larger swath width, which means that as the satellite is traveling over the surface of the Earth, the width of the area or path that it's able to cover and take images of is going to be wider for something like modus with a lower spatial resolution because it has a larger pixel size. The same amount of pixels, the same amount of modus pixels versus the same amount of lansap pixels. If you stack them up side by side, say five modus pixels, right next to five lansap pixels, the five modus pixels are going to cover a wider area because they're a larger pixel. The lansat pixels all stacked next to each other are going to cover a smaller relative area and width because they are a smaller pixel. If we have a larger swath width, say a swath width like this, as opposed to a thinner swath width like this or like this, we're able to cover all of the surface of the Earth quicker with a larger swath width. Imagine that we take this kind of red swath here and we stack it up side by side by side by side over and over and over again until it covers the entire surface of the Earth. If we do that with a larger swath width, we're going to be able to cover all of the Earth with less orbits and thus less time. If we're trying to cover all of the surface of the Earth with this thin little swath like this, we're going to have to orbit the Earth way, way more times which is going to take much, much longer resulting in a lower temporal resolution. So if we have a larger swath width, we have a higher or finer temporal resolution. If we have a smaller swath width, we have a lower or coarser temporal resolution. Does that make sense? Okay. Now, we again talk about these two orbits that are used in Earth observation, either close elliptical orbits or geostationary orbits. Just to remind you of some of their characteristics, the close elliptical orbit, we define as having an altitude of somewhere between 700 to 2000 kilometers above the surface of the Earth. And the geostationary orbit, we define as the orbit speed being matched to the rotation of the Earth so that its location is static above a geographic spot on the Earth. And it has an altitude of about 36,000 kilometers. Now, we can further break down what we call a close elliptical orbit into a polar or near polar orbit and then into also a sun synchronous orbit. So a polar orbit is a type of close elliptical orbit whereby the satellite passes over the poles or near the poles. Imagine that point there is the satellite. You can see that as it's rotating or as it's orbiting around Earth, it passes over the North Pole and then it passes over the South Pole. Then it passes over the North Pole and then it passes over the South Pole. An orbit, a close elliptical orbit that is oriented such that it passes over either directly over the North Pole or close to it or directly over the South Pole or close to it is termed a polar or near polar orbit. So it looks like this. Now the key here and how satellites work is that they continue to orbit on this same plane essentially. But because Earth is rotating as it's orbiting, then if it comes around in orbit, see where it passes over right there. Now when it passes over that spot again, the equator is passing over a different area than where it was passing over before. So if we look at that in this diagram below here, if the satellite's orbiting a near polar orbit going over close to the Pole and then back close to the South Pole, if it goes over the equator at first here and then orbits around and the Earth continues to rotate underneath it, then when it passes over the equator again, it's going to be at a different longitude that it's passing over the equator. It's going to be at a different point on the surface of the Earth. And then as the Earth continuously rotates and rotates and our orbit, our orbiting satellite continues to orbit and orbit, eventually the satellite is able to get an image or take imagery of the entire surface of the Earth. And then once it's done that, it's able to return to the same spot on the surface of the Earth to start retaking imagery there again. Now in the case of say Landsat for example, Landsat always passes over the equator at a local time of about 942 a.m. So that means that Landsat, it's in its polar orbit or near polar orbit. It comes from the North Pole and then it travels southward over the South Pole, passes over the South Pole and then travels northward back towards the North Pole. And then each time it passes over the equator, it passes over the equator at the same local time, always at 942 a.m. Which is possible because the Earth is rotating underneath Landsat, underneath the satellite as it's orbiting, and so it's pushing it into a new time zone. Which also simultaneously allows it to take images of a different area. Because Landsat, because its orbit is also planned out such that it passes over the equator at the same local time each time it passes over it, it's also termed what's called a sun synchronous orbit. I'll talk a little bit more about what that means in the second. I'm just going to show you guys this video here so you get a depiction of what that Landsat orbit looks like and thus what a polar or near polar orbit looks like. As a Landsat satellite flies over the surface of the Earth, the instruments aboard the satellite are able to view the swath 185 kilometers wide and collect images along that swath as the satellite proceeds through its orbit. The spacecraft travels at approximately 4.7 miles per second. The satellite travels from north to south while it's over the sunlit portion of the Earth and travels south to north over the dark side of the Earth. One orbit takes about 90 million minutes so that's about approximately 15 orbits in the 24-hour period. The orbits maintain such that after 16 days the entire surface of the Earth has come within view of the Landsat instruments while sunlit. And then on day 17 the first ground will have this repeated so we get to view the entire surface once every 16 days. So like you said at the very end there Landsat is able to view the entire surface of the Earth every 16 days. That means on the 17th day it starts re-imaging the spot where it first started that 16-day cycle. That means that the temporal resolution of Landsat is 16 days. So this is just kind of a graphical illustration of what that looks like. Its orbit is going north to south like this orbit orbit Earth is rotating underneath it. Because of that its first orbit the swath is going to be here its second orbit the swath is going to be here its third orbit the swath is going to be here its fourth is going to be here fifth is going to be back over on the right side here and then six seven eight nine ten eleven twelve thirteen fourteen. That's done in about one day. So it'll do about fourteen or fifteen orbits in about one day in about twenty four hours and then it'll start again just adjacent to this first orbit here and it'll do another 15 orbits in a day. And then it'll start again just adjacent to that orbit or that path here and then it'll do another 15 orbits. It'll do that for 16 days until its finally built up an image or collected imagery for the entire surface of the Earth. At which point it'll then return to the starting point of its orbit and go over the same portion of the Earth that it's already taken an image of. So we've kind of thrown around a couple of different terms when we talk about these orbits when we talk about close elliptical orbits in particular. So what is the difference between a close elliptical orbit a polar orbit and a sun synchronous orbit. So a close elliptical orbit is defined by its altitude. We've described a close elliptical orbit as being about 700 to 2000 kilometers in altitude so above the surface of the Earth. A polar orbit is defined by the orientation of the orbit. A polar orbit is a specific type of close elliptical orbit that passes over the poles or close to them. That means that its orbital plane is near perpendicular to the equator. When it passes over the equator it's pretty much perpendicular in its path of travel relative to the equator. A sun synchronous orbit is a specific kind of polar or near polar orbit where it passes over the equator at the same time each day. Landsat passes over the equator between about 10 to 10.30 am. That's different from the other illustration I showed just because it's referring to a different Landsat satellite. But the point is any satellite that's in sun synchronous orbit will pass over the equator or just generally areas of the same latitude at the same local time every single time it passes over them. Which also just means that it passes over any same point on the Earth at the same local time. That's essentially inherently where the value in a sun synchronous orbit lies and why we use sun synchronous orbits. So why then why do you think a sun synchronous orbit would be useful? If I was taking imagery of a certain area, a certain point on the surface of the Earth why might it matter what time I pass over that point on the Earth. Consider these two images. This is a bit of a hint. Yeah. I hear you're saying that's not quite what I'm hoping to get out of it. Yeah. Yeah. That's the key. That's the key. So the problem is if you take an image of the same area at different times of the day, then you are going to get different values of reflectance for that same area just based on the time that you're taking that imagery at. That introduces a certain level of discrepancy or error or deviation that you can't really account for. But if you take an image of the same point at the same local time, every single time you take that image, then that means you can essentially say, well, if there's different values of reflectance that I'm measuring, then it has to be due to some sort of environmental change going on on the surface of the ground there. It's not due to the time that I'm taking the image at because I'm taking the image at the exact same time. You can imagine if you look at this example here and you take an image of say this urban area at noon versus an area of versus that same image of that same area. But at midnight, the reflectance values you're going to get, the levels of radiance you're going to get for each pixel are going to be very, very, very, very different. But that doesn't mean that anything is changing in these two areas. You've just taken the image at a different time of the day. So a sun synchronous orbit, the reason that Landsat and many Earth observation satellites are in a sun synchronous orbit is because it's able to control some of the deviation or some of the differences in reflectance values that you may be able to otherwise attribute to taking an image just at a different time of the day. If you take an image at the same point, always at the same time of day, then if you see changes in reflectance, there's got to be some sort of change going on on the surface of the Earth at that point of interest. Yeah. So does nighttime, does taking photos of places like say fours to the area of the candidate nighttime give us less overall information? Or will all of the sensors, I guess because it's not visible right? Well, so you can take nighttime imagery and we'll talk about some applications of nighttime imagery. But it's the same thing where if you are taking images of a certain area at night, you want to try to take images at the same time of night every time you're taking an image of that same point because if say you're measuring the difference in the amount of night lights, the amount of street lights that are present on the ground in order to quantify urbanization, then you want to make sure you're taking that image at the same point of the night every night because otherwise you're going to get different levels of emitance just based on taking the image at a different time of night. So we have multiple different satellites in different sense, like an assortment so that you can see Vancouver consistently at night or she consists of air. Correct. But the idea is with say Landsat and some of these other satellites is that the temporal resolution is defined by how long it takes that same satellite to return to the same point on the surface of the Earth. When it does return to that same point on the surface of the Earth, it'll be at the same local time that it last passed over that point on the surface of the Earth. It's a little bit harder to integrate different satellite data sets because they're inherently different spectral bands, different spatial resolutions, lots of different things about them. But when you're looking at just the same satellite, that's where the sun's synchronous orbit is really valuable because you have then the same data set, the same sensor always taking an image over the same point on the surface of the Earth given the same amount of angular radiation coming from the sun. It's always at the same angle, it's always taken at the same time of day. Okay. Now, just lastly to highlight, which I already mentioned, our geostationary orbit, generally speaking, most Earth observation satellites that we talk about that are in a geostationary orbit are weather satellites. An example of that is the GOES satellite. Sometimes there's also telephone and television relay satellites in geostationary orbit because in that way they have constant communication with ground stations. Satellites that are in geostationary orbit often have quite limited spatial coverage. They do have very high temporal resolution because they can always take an image of the exact same point, but they generally have quite limited spatial coverage. Each satellite can only cover about 25 to 30 percent of the Earth's surface and the coverage generally only extends to mid-latitude areas, say at about 55 degrees latitude north or south. So you don't get great global coverage with geostationary satellites, but you do get very, very high temporal resolution. So this is what a geostationary satellite would look like. You got a couple of the GOES satellites here, some weather satellites, some other weather satellites here as well. You'll see in a moment they're kind of spotlight the area that they're looking on on the surface of the Earth will be kind of illuminated. Given that they're always only looking at one point on the surface of the Earth, they're actually still able to see quite a large view of it just because they're at such a high altitude. They're so far away from the surface of the Earth, but this is the idea here. This GOES satellite, it's always looking at this portion of the Earth, so it's always taking imagery there, allows for a very high temporal resolution in that spot. Okay, a couple of practice questions here. I'll give you guys a couple minutes to look at them, brainstorm them with a neighbor. I have Tristan here in case you want to come down and ask him any questions about the assignment that you're working on this week. So I'll give you a couple minutes, brainstorm these, we'll go over the answers. If you want to head out now, you're welcome to, if you want to go over these answers in a moment, you're also welcome to stay. Thanks guys. Hey. So they're close elliptical polar and suns and crinus orbits, they're not exclusive, right? It could be a close elliptical polar. Yeah, so close elliptical is kind of like the broadest category. So a sun synchronous orbit is a type of polar orbit, a polar orbit is a type of close elliptical orbit. Okay. Yeah. Hey. I just have very quick questions about the special resolution. Yeah. And also the band. So can we actually use the like the band numbers to actually kind of explain why here in these questions that the white line is like reflective? Not really. Not really. No. So in this specific questions, the answer is just like, okay, because the ground actually is not reflective. So the white line is like more kind of significant than here. Yeah. So if you if you like thought of it as like, this is a pixel, right? This is one pixel. Yeah. It's 60 centimeters by 60 centimeters, right? There's a white line going through this pixel. Right? Like this. This, this is a white line, right? But this is black. This is all black. This is all black. This is all black. If we measure an overall amount of reflectance of photons for this whole 60 centimeter by 60 centimeter square, because there's this white line going through it and it's reflecting so many white photons, we're going to get an overall reflectance that's relatively high for this entire pixel. So in the image here, that's 60 by 60 centimeters, the whole thing is going to be white. Yeah. Okay. So this is what it looks like in reality. This is what it looks like on there, right, in our image, and that's just because the spatial arrangement is this one very bright line on top of very dark black materials. So if we measure the reflectance for this overall entire pixel, it's still going to be quite high. Just because this, just because this white line, even though it's smaller than the pixel size, is reflecting so many photons. Okay. Right. Yeah. No problem. Hey. So for this spatial resolution, yeah. So if the, if this image is 60 centimeters, is this saying that each square is 60 centimeters square or? Yep. Yep. Okay. You got it. And then, okay. So the tennis chord line would be like, yeah, or like, or like, like the way I like to think of it is like, it's like something like this, like, like if this is a 60 centimeter, yeah, yeah. So go by 60 centimeter pixel. And the white line kind of looks something like, like that. Yeah. Yeah. Yeah. Let me just go over the answers to these. And then we'll, and then we go over them. I'm just gonna go over the answers and then we can talk. Can you guys, can you guys just hold on one second, like, over the answers these, then you can, you can chat to him. Okay. Let's just go over the answer these real quick. What are the three resolutions that we discussed today that are important in Earth observation? Yeah. Yeah. Spectral resolution, spatial resolution, temporal resolution, exactly. What are the three aspects that we use to describe the spectral resolution of a satellite? Yeah. Yeah. Yeah. Yeah. Yeah. And the bands with the major. Exactly. The number of bands or channels, their location in the electromagnetic spectrum, and each of their bandwidths or ranges in wavelength size. Okay. This one, maybe you guys can just kind of shout out as we go through. So if we're talking about a sun synchronous orbit, is a sun synchronous orbit a close elliptical orbit? Yeah. Yeah. Yeah. It is a sun synchronous orbit is a type of close elliptical orbit. Is the sun synchronous orbit a polar orbit? Yes. Yeah. Exactly. Yeah. Yes. It's a polar orbit for sure. So a sun synchronous orbit is a specific type of polar orbit. A polar orbit is a specific type of close elliptical orbit. Ultimately, how is a sun synchronous orbit defined? How would you describe a sun synchronous orbit? You want to go good? Yeah. Yeah. I think it's a normal image that has like the same, that path of today's window. And because it's possible to take weight at the same time of the day, it has control radiation of reflection bodies. Yeah. Because it measures the difference. You got it. That's enough. That's good. Yeah. You're good. Yeah. Really good job. Yeah. So a sun synchronous orbit is just a polar orbit, a close elliptical orbit, whereby you pass by any point on the surface of the earth at the same local time each time you pass over it. Or you could say you pass over the equator at the same local time each time you pass over it. And then you already mentioned also why it's useful. It's useful because it helps control potential changes or deviations in reflectance that are that might occur if you're taking imagery and measuring reflectance at different times of the day. If you're always measuring reflectance for a given point at the earth on the earth at the same time of day, then that means that if there's different levels of reflectance that you're measuring, it's not due to just the time that you're looking at that portion of the earth. Must be due to something else. Okay. And then lastly, we already talked about this one. We kind of answered it directly. What is the temporal resolution of a satellite in geostationary orbit? It's very, very high. It can be very, very fine. We can essentially take repeated images all the time of a certain point on the earth if we're in a geostationary orbit. Okay. Next week on Monday, we're doing a review session. Please post in the discussion board if there's anything in particular you want me to go over. And otherwise, have a good week us. And I will see you next week. Welcome back. Hope you had a good Break. Yeah. How was your break? Good. Bad. Decent. Thumbs up. Couple of thumbs up. Cool. Okay. So today, announcements, Keep an eye out for class getting canceled tomorrow. It might. I don't know. I know that the office that we have In the department where i work, they have already said they're not coming into the office tomorrow. Sometimes they just do that and class will still go on no matter what. So i'll be here as long as it's not canceled, but just keep an eye on that. What else? We're trying to, we're going to try to Give the midterms back to you tomorrow. So either before class or just after class, we will post mid-term Grades. They were pretty well done overall. How did you guys find it? Decent. Thumbs up. Sweet. Okay. Cool. Sweet. So for now, what you should be working on, Assignment three is due this Thursday. Tristan is going to come to the end of class tomorrow, Assuming we have class. And you can ask him any questions if you'd like. There was a change to his office hours. Hopefully you guys saw that. So i think Evan sent out an announcement. Office hours are now Thursday morning. I think he had a late Thursday afternoon set and now they're Thursday morning instead because he has some medical appointment, i believe. So that got changed. There's also two sets of office hours on Wednesday. I'll actually pull those up in a second here. And then blog post four is due March 9th. So that is next week. And then assignment four due March 16th. So that's two weeks from this week. Like i said, office hours, two office hours On Wednesday for assignment three, one set on Thursday. Thursday morning now instead of Thursday afternoon or evening on zoom as usual. That's pretty much it. Any questions about assignment, mid-term, anything like that? Okay. Great. So quick review reminder where we are at in the Course. I really like where we're at in the Course from now on. I think it's a lot more fun to learn and talk about the kind of second half of the course. Content post mid-term. But just to remind you, the last thing that we talked about Prior to the mid-term was spatial resolutions, Spectral resolutions, and temporal resolutions. So i just want to quickly jog your memory About what each of those are and kind of what we discussed about each of them. And then we'll go on to kind of part two of this resolutions lecture. So quick reminder, spatial resolution is the smallest possible feature or object That can be detected on an image. It's typically defined by the pixel size. So the pixel size is just the smallest, resolvable area on that image. We have a pixel size of 10 meters. That means that the area on the surface of the Ground that we're measuring reflectance for it individually is a 10 meter by 10 Meter area. And this just impacts our level of detail Or information that we're able to get in an image. Courser spatial resolution data gives us less detail, less information. We talked about some other considerations as well, such as the spatial arrangement of targets. We use the example of the white tennis court lines on top of the black pavement As an example where you might be able to resolve or detect features that are smaller than the pixel size of the image. We also talked about data quality, how the atmosphere can heavily influence our ability to take high-quality satellite imagery. If we have poor atmospheric conditions, we might get a poorer effective spatial resolution. So we might actually not be able to pick out smaller features with poorer atmospheric conditions relative to better atmospheric conditions When that imagery is being taken by the satellite. We then talked about low, moderate, high, and very high spatial resolution imagery. We loosely defined each of them low being greater than what did we say? 100 meters, I think, moderate around moderate being less than 100 meters. High resolution imagery being less than about 5 meters, very high spatial resolution being less than about a meter. And then we talked about some of the applications of each of those. And kind of you'll notice from here on out throughout the course, a lot of the things that we talk about, a lot of the practice questions I'm going to give you are going to be trying to link different topics together. So there's topics from a lot of the different lectures that we'll discuss that will have a lot of overlap and have a lot of influence on other topics. And so we're going to try to, with the practice questions in particular towards the end of lecture, link a lot of those topics together. One example of that is going to be talking about some of the specific applications or examples where you might need low spatial resolution imagery versus moderate or high spatial resolution imagery. Yeah? I'm only here to speak about this. Pardon me? Yeah, you just like to talk to the camera. They're on lecture 10. So there's a part two. So if you go to the lecture 10, so the same page that we had like the last resolutions set of lecture slides on, there's a part one and then my recording from that and then a part two right under that. And this is that part two. So it should be there. If it's not, then let me know again. I think it's there. Okay. So the second thing that we are the next resolution that we talked about is the spectral resolution. So the spectral resolution is the number and dimension of wavelengths in the electromagnetic spectrum that the sensor measures. So it can be classified or categorized using three different things. That is the number of spectral bands, their location on the electromagnetic spectrum and the bandwidth of each of those bands. So again, if we look at this example here, we can see that there's one, two, three, four, five, six bands here. Their location on the spectrum are in the visible blue, green and red and then in the near infrared and short wave infrared part of the spectrum. And we could list off the bandwidth of each of these bands as well if we wanted to. We could say band one is from 450 to 500 nanometers. Band two is from about 500 nanometers to 570 nanometers. Band three is about 630 to 700 nanometers and so on and so forth. We then also lastly talked about temporal resolution, which we just defined as the amount of time it takes to revisit the same place on Earth. It impacts the level of temporal analysis possible. So do we want to look at changes occurring on a landscape at a daily scale? Or at a seasonal scale or at an annual scale? Maybe we want to know the daily sea surface temperature of an area. Then we need very high temporal resolution data. Maybe on the other hand, we're just interested in tracking deforestation, which occurs at a much slower rate. So maybe in that case, we only need annual imagery, which would be a low temporal resolution data set. We talked about how the temporal resolution mostly depends on two things. What orbit you're in and what the swath width of the sensor in that orbit is. So the orbit, in particular, we talked about how the geostationary orbit gives us a very high temporal resolution because it's always looking at the same place on the surface of the Earth. We then talked about how we have the close elliptical orbit, the near polar orbit, and the sun synchronous orbit. So those are the orbits that look more like this, that are going north to south from pole to pole. And when we have a larger swath width, so the sensor is imaging a wider area as it's traveling over the surface of the Earth, a larger swath width. So that would look like this. This is an example of a larger swath width. We're covering a wider portion of the Earth as we take images of it as the satellite's traveling, say north to south. A wider swath width gives us a finer temporal resolution because it doesn't take us as long to image the entire surface of the Earth if we have this larger swath width. A smaller swath width gives us a coarser temporal resolution, generally also associated with a higher spatial resolution, is a coarser temporal resolution because with higher spatial resolution data, we have a smaller pixel size, which generally results in a smaller swath width, and thus a coarser temporal resolution. Now, we know kind of some of the characteristics now of each of these sensors, we can describe them using the different resolutions. Most satellites do not have a lot of onboard recorders in order to kind of record and store the data that they're taking as they're orbiting the Earth. They need to downlink their data to essentially ground stations that collect the data. So this is an example of land stat stations all across the world, and their range. So each of these symbols is a station, and then the kind of elliptical circle around them describes the range of those stations. When a satellite is within the range of those stations, so it's within this kind of area for this one, for example, then that means the satellite is able to send down information and data to that ground station. Now, they can also use other satellites that are in different orbits and transmit data to a different satellite, which then transmits data down to a ground station. But you'll see here, this is an example of a satellite orbiting, and as it's orbiting around when it comes into range with a ground station, boom, boom, boom, it's able to send that data down to that ground station. Then it keeps orbiting around, it gets outside the range of that ground station, it can no longer send data to that ground station, but then eventually it'll come into contact with another ground station. It'll get into range with another ground station, and it'll send down its data again. Now, what's interesting is that some satellites in geostationary orbit, for example, they're in constant contact with certain ground stations. So some satellites that are orbiting around will just send data out to these geostationary satellites, like this one is doing, sending data out there, and then it'll transmit from those geostationary satellites down towards a ground station. So there's lots of different ways for us to retrieve the data from these satellites that are in close elliptical, polar, or sun synchronous orbit. One of them is just transmitting straight to the ground station. Other times there's these complex networks where data's getting transmitted to other satellites and then to other satellites maybe, and then eventually down to the ground. Now in Canada, the government operates three ground receiving stations, one in Quebec, one in Saskatchewan, and one in the Northwest Territories. Because of the large range in total between each of these ground stations, you can see these yellow circles are the range, we're able to essentially get close to near real time data collection for most of Canada and parts of the states, which just means that if a satellite is traveling over top of Canada and it's imaging, say, an area right around here, then it could immediately send that data down to this ground station or maybe down to this ground station. And so for purposes for applications that require near real time data collection, Canada is in a really good position because we're able to transmit that data almost immediately after it's collected by a satellite. Now, I mentioned a couple lectures ago that there are a few key satellite missions that we're going to focus on throughout this course. And what I'm going to do for the rest of this lecture is we're going to talk about the Terra and Aqua satellite, which has the MODIS sensor on board, and then the Landsat satellite, which has the thematic mapper and operational land imager on board, and then the worldview satellites. And we're going to break down the spatial, temporal, and spectral resolution of each of these satellites so that if you were to look up the resolutions, the characteristics of the satellites yourself, you would hopefully be able to piece together the puzzle a little bit and understand what kind of data set each of these satellite missions is collecting. So, first is the Terra and Aqua satellites. So the Terra and Aqua satellites are two sister satellites. They're essentially the same satellite, but there's two of them, and they're just in a staggered orbit around the earth. Both of them have a MODIS sensor on board. So both of them have a sensor that collects literally the same identical data. But because they're staggered at different times in their orbit, we're able to get a really nice temporal resolution combined from MODIS between the two satellites of about one to two days. So we can image the entire earth with this data set in one to two days. Now the MODIS sensor is the one that we're going to focus on that is onboard the Terra and Aqua satellites. There's several other sensors that are also onboard the Terra and Aqua satellites. The advanced space-borne thermal emission radiometer, Astor, and then there's a bunch of other ones here. We're not going to get too much into them other than acknowledging that they exist. We're going to focus on MODIS for the purposes of environmental conservation forestry applications. MODIS is the main sensor that's used onboard the Terra and Aqua satellites. Okay, so this is what those satellites look like. They're quite large. They are used for a large variety of applications of biological and geophysical applications, including measuring and imaging temperature of both the land and sea, understanding ocean color, global vegetation patterns, clouds and aerosols, and snow cover. It has a swath width of 2330 kilometers. So that just means that when the MODIS sensor is onboard the Terra and Aqua satellite, and it's traveling over the earth and it's taking images, it's taking imagery of an area that's over 2,000 kilometers wide. So it's pretty large. It's imaging quite a large area at once. The MODIS sensor has 20 spectral bands in the visible and near infrared portion of the spectrum, and then 16 spectral bands in the mid-infrared and thermal infrared portion of the spectrum. So here's a quick video kind of introducing MODIS and some of its applications. In all the instruments in NASA's Earth observing system, MODIS is proven to be one of the most versatile, producing both groundbreaking science and heat- and temperature. The modern resolution in the spectrum of radiometer on both the Aqua and Terra satellites has changed the way we look at our atmosphere, oceans and wind. MODIS, creaske, wave length, range, covered more physical properties of the environment and monitor. It may turn down a small surface of a few meters, a size of a couple of full fields of size, and many more spectral bands, instead of more aspect of the ocean biology of the land, fires, and it was a very technological advantage to those. And for a long time in the US community, it was referred to as the quintessential instrument in the US because it would be more thankful for people, more disciplines, than many other single instruments. The study of cloud is not surprisingly incredibly important for understanding weather and climate. And until MODIS came on, it was commonly accepted that any given time, the Earth was about 50% covered by clouds. But data from the instrument showed that cloud cover was actually closer to 70%. MODIS can also measure the temperature and height of clouds and differentiate between clouds composed of liquid water and those made of ice. MODIS also monitors the world's oceans, making seas with the stench of ocean coloring and clarity, and the basis of the marine food chain, height of length. MODIS has a very good job of getting this biological nature of chlorophyll A, which follows by the way of the ocean currents. You can see the seasonal variation, doing all new things where the biological productivity of the ocean. This is important because biological consensus takes carbon dioxide now, and uses oxygen and carbon consensus, so there's the sink for carbon we may have put in the atmosphere. MODIS also looks at the land monitoring fires, land use change, and various measures to be addressed planally. It's been used for a long time to monitor the growth of education, the seasonal cycle, and how it changes year to year with us to droughts, sociodal, you know, or spread it as a hair, but it's a very good index to monitor. The first story of MODIS is that measures land, sea, and air contribute to the wealth of information that will reveal about the aqua mission. Okay, so in that video they were just referring to the aqua satellite, but everything they talked about in reference to MODIS could also be applied to the terra satellite, because they're essentially the same satellite they both have a MODIS sensor on board. So they kind of summarize some of the properties of MODIS. It has a spatial resolution of about 250 to 500 meter pixels for land research, so for spectral bands that are focused on imaging and understanding land properties, those pixel sizes, the spatial resolution of those bands is 250 to 500 meters. And then they have 1000 meters or 1 kilometer pixels for ocean and atmospheric research, and again this one to two day return period. Now, this is the most important and probably valuable part of MODIS is this last part right here. We can get imagery of say, you know, sea surface temperature of NDVI, whatever it might be, for the entire surface of the earth every single day. So those fine scale processes that maybe we want to monitor, whether it's vegetation phenology or snow melt to understand fine scale changes that are going on, maybe due to climate change or other anthropogenic impacts are really only capable to be done with MODIS. So, for example, if I were to give you on an exam, a question that was asking you what kind of satellite would be suitable for an application that requires daily measurements, you know right off the bat, it's probably going to have to be MODIS. MODIS is the only sensor that we talk about in this course that collects imagery at a daily temporal resolution. So if you want that very fine scale information, it has to be collected with MODIS. Now, MODIS has a very large spectral resolution, so it has a bunch of bands ranging from the visible all through the eventually near infrared, thermal infrared, and shortwave infrared part of the spectrum. And they all have a wide variety of applications. A lot of these visible ones near infrared are for the purposes of land cover mapping and land use changes. And then they also have a bunch that are for monitoring the oceans, monitoring the atmosphere, monitoring clouds, monitoring, what else we got here, temperature, whole variety of things. We're not going to go into detail too much about each of these specific bands, but the capability of MODIS to look at upwards of 36 different spectral bands is really, really valuable. That's 36 different measurements for each individual pixel that it's measuring. 36 measurements of reflectance for each individual bandwidth that we're looking at here. Now, hopefully, part of the goal I'm hoping to get you guys out of this course is that if you were to look up what the spectral resolution is of MODIS or what the bands are that MODIS used, you would see this table. And so hopefully you could look at this and say, okay, I know kind of what is going on here. I know that these bands, this bandwidth over here is just denoting the wavelength size of that band. And then the spatial resolution here is just telling me what the pixel size would be for that band. And the spectral domain here is just telling me where that band is located in the electromagnetic spectrum. Now, with something like MODIS again, we can get something like this, where we're looking at the entire globe, in this case for NDVI for a vegetation metric. So it's telling us how green or how healthy vegetation is across the whole world. And again, something that is really only capable with MODIS data. When we're looking at the scale of the entire globe, it's really hard to do that with something that has a moderate or high spatial resolution. The low temporal or the low spatial resolution of MODIS makes imaging the whole globe a little bit easier, because we don't have quite as much data. We don't have as small of a pixel size to look at this whole area to look at this whole globe. So it's a little bit of a smaller data set relative to if you were to try to do something like this with say Landsat, which makes processing time and analysis a lot easier. So to highlight that again, if I were to ask you in an exam setting for an application that say required you to measure or take images with Earth observation satellites of the Earth's observation. So if it requires daily imagery, high temporal resolution, or it requires a high temporal resolution or a high temporal resolution, then it's probably going to need MODIS data. One thing, one particular application that's very valuable from MODIS, especially in today's day and age, is its ability to detect fires. So MODIS, because it's able to image the entire globe, every day we're able to get an image of one spot on the Earth revisited every single day. It does a really good job at detecting fires. It can detect fires on a day by day basis. So this is MODIS detecting fires at different places around the globe, California and these two in Australia up here. And then this is just put together a detection of all of the fires going on all across the world month by month, year by year. So again, a really, really, really valuable data set. Our ability to measure where fires are all across the world every single day is really, really invaluable, not just for our ability to predict and model and monitor fires and their effect on the environment as a whole and in the long term, but also just in our safety, in our ability to plan for communities and plan for natural disasters. Okay, so that's MODIS. So key hints about MODIS for an exam setting. If I am referring to things at a global scale or at a very, very high temporal scale, say at a daily scale, that it's probably MODIS that you need to be thinking about. The next Earth observation mission we talk about is Landsat. Landsat has a 30 meter pixel, so a moderate spatial resolution, has a 16 day return period, so it revisits everywhere on the Earth every 16 days. Similarly, it's able to take an image of the entire surface of the Earth every 16 days. It's a series of eight satellites comprised of four different sensors. There have been nine Landsat's launched, but one of them didn't make it to orbit, which is why we say there's only eight. And it has a 185 kilometer swath width. Now again, linking that back to our temporal resolution, think about what the swath width of MODIS was. It was about 2000 kilometers, so way, way, way, way bigger than this. Again, that's why we're able to get that very fine temporal resolution with MODIS data. With Landsat data, it has a much smaller swath width, so we're really only able to get this moderate to low temporal resolution of 16 days. Okay, quick intro video from Landsat. The music I will warn you about, like a lot of space videos is super over the top. I think it's very James Bondi, but anyways, it's a cool video. So, like, so unnecessary. Okay, so key thing about Landsat, you know, the way that we talk about MODIS and how I mentioned, if we're talking about things that require daily imagery or things that require us to image the entire globe, a key indicator that in an exam setting, you should be thinking about Landsat, is that Landsat is the longest continuous record of the surface of the Earth that we have. So Landsat 1 was launched in 1972, and this was, you know, very, very revolutionary, and the key part about Landsat is its data continuity. So, not only was it launched all the way back in 1972, but there's no gaps in the data. So this is the lifespan of each of the Landsat satellites, and you can see they're always overlapping. So, there was never a time where there is no Landsat data available dating from now all the way back to 1972. Now, you'll see here Landsat 6, that was the one that got launched, and there was essentially a math error, and it did not make it into orbit, so we don't have any data from Landsat 6. Luckily, Landsat 5 lasted a really long time, so we survived without Landsat 6, and then Landsat 9 was the newest Landsat that was just launched, and then in one of our lectures towards the end of the course, we'll talk about the next Landsat satellite that's going to be launched. But key thing to note here is that if we're talking about an Earth observation application that requires us to take imagery or look at imagery beyond the 80s and 70s into the past, if we're looking at any change that occurred from 1970 or 1980, we have to be looking at Landsat data. The MODIS data doesn't go back that far, it only goes back as far as 1999, I believe, or 2000, around there, and worldview, the high spatial resolution data sets we'll talk about were in the late 2000s and early 2010s. So, anything that requires us to look at data from the 80s and the 70s requires us to use Landsat imagery. Now, brief summary of kind of the history of Landsat, it was originally called the Earth Resources Observations Satellites Program, and then it was changed to Landsat in 75. And over its history, it's kind of passed from public, from government-owned to private industry back to public, and ultimately will stay in public ownership operated by NASA and the Geological Survey, which is essentially just US government, and that's in law now. So there was the Land Remote Sensing Policy Act that was passed back in 92, and this authorized the procurement of Landsat 7 and future Landsats, which just meant that the US government was to own and operate all of the Landsats going forward, and they were going to ensure data accessibility for everyone at the lowest cost, which is what it is now. It's open source and it's free, so anyone can access Landsat data at any time. On board, the different Landsats are different satellites, so Landsat 4 and 5 had the thematic mapper, which is really the first sensor that was kind of more modern, that gave us a much higher level of data quality than what was on Landsat 1 to 3. And so that's where we'll start in terms of the sensors we'll talk about. So you have Landsat 4 to 5 with the thematic mapper, Landsat 6 never made it to orbit, so we don't have it here. Landsat 7 had the enhanced thematic mapper plus, not just enhanced, enhanced thematic mapper plus. No, you guys are tough, okay. And then we had Landsat 8 and 9 had the operational land imager and thermal infrared sensor or tears. Now one thing that's kind of interesting to note on these if you want to try to link back to some other concepts of this course, and a good kind of exam question, is why do you think for, on the enhanced thematic mapper plus sensor, this band here, band 6 here, why do you think it has a higher or larger pixel size, lower spatial resolution than the rest of the bands here? Similarly, band 6 on the thematic mapper, as well as band 10 and 11 on the OLI and tears sensor, each of those have a slightly higher, well, lower spatial resolution, higher or larger pixel size than the rest of the bands. And I want to guess why that might be, I'll give you a couple minutes to brainstorm, I'll give you one or two minutes brainstorm with a neighbor sitting next to you, think it through, it relates really nicely to a concept that we talked about when we discussed the electromagnetic spectrum. But take a moment, brainstorm a little bit and then, and then let's see if we can try and get an answer. Okay. All right. Are you guys okay? This is the quietest I've ever heard you kind of concerned. You guys just like not back in the, not back in school mode yet, still in reading break mode kind of thing. That's fair. Okay. You guys are really hard to make laugh. You're killing me. So, so what do you guys think, any ideas why those bands might have a slightly larger pixel size or slightly lower spatial resolution. Anyone have any guests? Anyone discuss anything? Yeah? Good guess? No, that's not the case for these. They are valuable. You could argue they're less important than some of the other ones, but that's not why they have a lower spatial resolution. Anyone else? Yeah? Just like require less resolution for some reason? Yeah. So that's totally on the right track. So notice here for each of these bands that have a lower spatial resolution, all of the wavelength sizes are much, much larger. See here we have in micrometers, 10 and 11 up here. We have 10 to 12 up here. We have 10 to 12. The rest of the wavelengths are all, you know, micrometers of 1.5 0.5 0.5 0.5. 0.6 0.4. So what is the relationship, if we can remind ourselves, what's the relationship between wavelength size and frequency of a wave of a photon? When we have a larger wavelength size, we have a lower frequency. When we have a lower frequency, there's a smaller amount of energy associated with those photons or with those waves. So that means that when we're imaging or trying to sense portions of the spectrum that are out in the thermal infrared part of the spectrum, where wavelength sizes are a lot larger, those wavelengths, those photons also have much less energy associated with them. That makes them a lot harder to sense and to measure. So to make up for that, to account for the fact that they have less energy associated with them and they're much harder to measure, we just image them over a larger area. So we collect a larger amount of them by looking at a bigger area and then we can still get a nice clean measurement of that part of the spectrum. Does that make sense? See, okay. So this is an example of, again, looking at a change through time with Landsat data. You would know if I just gave you these images and said, okay, we're looking at deforestation through time here from the 70s to the 90s. If I asked you what kind of satellite or which satellite system in particular was responsible for measuring this data or could you measure this change with, you would want to say Landsat data. We know Landsat data is the only data that we have available that's all the way back to the 70s with a continuous record all the way through time, all the way up till now. Now, one other kind of example here is from Landsat change detection in Las Vegas, Nevada. We have on the left here an image from 1984, on the right here an image from 2007, and you can see the abrupt change in the amount of urban areas around Las Vegas. Now, again, if I were to show you these two images or ask you to compare or look for a data set that was able to compare or measure the change in urban sprawl in a particular city from the 1980s onwards, you would say, okay, well, I have to do that with Landsat because there's no other satellite imagery that looks back that far. You'll notice also with these Landsat applications that we're discussing, they also generally aren't looking at quite of a large area as we did with MODIS. When we were talking about MODIS, we talked about a lot of global metrics, a lot of measurements that we can get at a global scale, whereas these Landsat examples were really just looking at, you know, in the case of Las Vegas here, one particular city. It's more of a regional application. And that's partly because that moderate spatial resolution imagery, that 30-meter pixel size, is perfect for looking at a spatial scale like this, a regional scale. If we wanted to use Landsat to look at, say, the entire globe, we could, but the amount of data to do that would be huge. Our processing times would be much, much larger, something like MODIS is much more suitable for that. I don't need the music here. So this is just a composite of images through times that you're going to be using for one of the assignments coming up. Believe it, it's Mount St. Helens before and then after it erupted. And you can see kind of the change through time due to all the debris that came out of the volcano. You can also see through time kind of the increase of forestry cup blocks or clear cuts kind of around the area. Like you can see down here, they're really picking up. And then you can also see some areas that were harvested, where trees were cut down. Back in, you know, the 70s, 80s recovering and vegetation starting to regrow. So really cool example there and one that you'll be looking at in detail in one of the assignments coming up. Now just a quick note that Landsat data officially became free and open source here in 2008. And since then there was initially this exponential increase in the demand of Landsat data. And ever since then it's kind of been steadily increasing in a linear fashion. So the need and use of Landsat imagery is always increasing. Mostly for monitoring land use and land cover change. That's probably the most common application of Landsat data. It's spatial resolution of 30 meters makes it really ideal for that. As well as fire science and management, some education purposes here and a whole wide variety of other purposes all around here. I'd encourage you to take a look at if you're interested. Okay, the last satellite mission that we're going to talk about today is the World View Satellite Mission. So World View is launched and operated by a company called Digital Globe. It's one of the largest and most successful private satellite companies. So World View is a digital globe which launches and operates the World View constellation of satellites. It's a private company. It's not government owned. It's not government operated. It's a private company which means the data is not open source. It's not free. It does cost money and a license in order to get this data. Now they have three, well, four now World View satellites. And those are the satellites that we'll talk about. Digital Globe also has a variety of other high spatial resolution satellites. They were kind of the mother or father of high spatial resolution satellite missions. And nowadays they have sensors in the case of World View 1, 2, 3, and 4, where you can get imagery as low as 30 centimeters. So a 30 centimeter pixel size. So, you know, about the size of a ruler. So almost not quite. You can't quite make out people. But we might be getting there eventually. Just kind of scary. But we'll talk about that actually later in the class about the kind of little bit about the ethics and policy behind that. So World View 1 to 4 launched in 2001. Then World View 2 in 2009, 14 was World View 3, 2016 was World View 4. And World View 1 was really the first generation or the first satellite in this next generation of high resolution satellite imagery. And World View is capable of imaging up to 750,000 kilometer squared per day of this very high or high spatial resolution satellite imagery. Now this is some imagery you can see over here. One very common example of using World View imagery is for urban mapping. You can make out individual buildings, individual roads, which makes it really nice for that high resolution urban mapping applications. You can see World View 4 here has 8 bands. So it's got a 2 blue bands there, a green, yellow, red, red edge, and 2 near infrared bands. And then again, if you were to look up the spectral resolution or just Google, what are the bands that World View 3 has? You'd come across a table like this. And again, I'm hoping that after this course, you guys would be able to look at a table like this and make out a little bit of information about it. So you'd say, okay, I know that I have a bunch of different bands here that I'm measuring the reflectance for. This is where they're located in the electromagnetic spectrum. And then this is their bandwidth size. This is the smallest and largest wavelength range for that particular band that I'm measuring. Now you'll notice here there's a up here a nadir and off nadir measurement. So there's different spatial resolutions and different temporal resolutions associated with each of the different World View satellites because they have one thing that's very different about them compared to the Landsat and MODIS data sets. And that is that World View has a tiltable sensor. So Landsat and MODIS are always looking straight down, whereas World View can actually tilt its sensor side to side. That's part of the reason that it makes a really good private satellite data set because someone can essentially pay World View to say, hey, keep your sensor always tilted at this specific target so I can get an increased temporal resolution of the area that I'm interested in. And I can also get images from different angles of that target that I'm interested in. So a little bit different in terms of World View's capabilities and how you describe its spatial and temporal resolution. In general, for exam purposes, you can just assume World View satellites have a spatial resolution of about 30 to 50 centimeters. And this roughly describes their spectral resolution. And I wouldn't really ask you too much about their temporal resolution because it is a bit tricky. And it depends. Now, we've looked at all of these tables that show and describe the different bands for each of these different satellites. I'm not going to ask you to list off or name off the bands by heart for the final exam. What I at most would want to get you to do is ask you a question like, describe to me, you know, I'd give you an example of a specific application. And I'd ask you what satellite mission would be best to use for this specific application and why? And the why part you would describe with, well, because it has this level of spatial resolution and it has this level of temporal resolution and it has roughly this level of spectral resolution. It has bands in the visible and near infrared portion of the spectrum. So it would be really good for measuring vegetation health or measuring forests and things like that. So that's kind of the nature of the kind of question you can expect in terms of this content. Now, just some examples here of World View imagery, some famous places around the world. You can see again really, really beautiful imagery. You can even see kind of shadows of individual buildings. Really, really cool. Here are some images of areas in BC. So that's the Science Center there. That's BC Place there. Again, really, really high detail. Pretty amazing imagery. But very, very expensive. And this is the last thing I want to mention. World View 2 and World View 3 imagery is at a price of about $24 per square kilometer in US dollars. And the minimum order is generally about 100 kilometers squared. So you have to pay for at least $3,000 worth of imagery at once. So it makes it really hard to use this imagery unless you have some sort of private funding from some source or something like that. So some sort of companies generally paying for this. Sometimes there are large licenses that institutions and organizations can use. I know that there's a different company that UBC has a license for for their high spatial resolution satellite imagery. And I think you can just obtain that through the UBC library. But just an example here to kind of just nail down that it is very expensive. And it's hard to access much harder to access than say Landsat or modus data, which you can access at any time totally free. Okay, shorter lecture today. That's all I really have. Got a couple of practice questions here. If you want to practice them, stay for a couple minutes and then go over them with me. You are welcome to. If you don't, then you are welcome to head out now. And I'll see you guys tomorrow in the other building. Tristan will be here and Evan will be here and we will talk about the midterm tomorrow as well. Thanks guys. All right. Hi, everyone. So sorry i'm late. I'm in the right spot, right? This is constant of seven. Yeah, okay. Cool. I went to our other lecture hall. And then i got in there. I was like, where's everyone? Did they all think class was cancelled? And i was like, oh no. I'm in the wrong lecture hall. And then i sprinted over here. So one second. Sorry. That's what i thought. I thought it was decent. Not going to lie. I have no cardio, but halfway through i started cramping up. I thought i'm not going to make it. Oh, gosh. Okay. We're here. It's all good. Okay. Mid-term scores are going to be posted after lecture today. So, Evan's going to come here towards the end of class and just give a quick rundown of average time to complete the exam. Average score. High scores. Things like that. Then we'll post them right after class. If you have questions or disputes about the grading on them, email Evan and we'll put a time limit of two weeks from when we release the exam marks, which will be today. And essentially that timeline is just if you do not reach out to us within two weeks of us releasing the grades, then we're just not going to consider changing any marks whatsoever. So if you have any issues with the grading, anything you want to dispute, email Evan, do it within two weeks after that. If you email us asking about marks being changed on the mid-term, we're just going to say no. It's past the allotted time that we had to do that. Okay. Today, and I kind of mentioned this yesterday a little bit, but in general, the rest of the course content that we'll talk about from here onwards, kind of all the post midterm content, I think personally is a lot more fun. It's a lot more interesting. So if you are still coming to lecture, I hope it'll make your time worth it. There's some really fun concepts and really interesting concepts. Today we are going to talk about active remote sensing. So far up till today, we've been really talking about passive remote sensing systems. So today we're going to introduce active remote sensing and just what it is, how it differs from passive remote sensing. We'll then talk a little bit about LiDAR, radar, and sonar. I'm going to just define what those are today. And then we'll talk about radar today in detail. And then next week on Monday, we'll talk about LiDAR in detail. Then briefly, we'll just talk about some applications of how each of those systems work, how LiDAR works, how radar works, how sonar works, some applications of each of them. Again, today just focusing on radar. Okay, so I'm going to start with a video that kind of outlines and reviews some of the concepts we've talked about so far and then also introduces some of the concepts surrounding active remote sensing. If we move the earth from the ground, we can get a good local picture of what is going on around us. But if we move the nature of our proportions up there, then we'll need to use remote sensing. Remote sensing measures the earth has features without being in physical contact. We can gather data from entire continents over longer time periods, so we can look at how the earth is changing. Next, we use a specialized aircraft and sophisticated satellites to gather data using both passive and active remote sensing methods. Passive remote sensing measures the natural energy or radiation of the earth. Active remote sensing gathers data by actively sending out signals that interact with the target of interest. Using both active and passive remote sensing techniques, NASA can look at soil moisture maps to monitor drought. Estimate snow packing areas where snow is crucial for freshwater. Measure the change in pipe sheets and sea level, tracking storms that can impact human lives. And observing how precipitation changes affect where we get our freshwater. The Global Precipitation Measurement Mission helps fill in the gaps where ground measurement isn't enough. Places with rugged terrain and lots of signals from ground weight-ups. The oceans are too vast to cover with enough ships and measurement stations on the surface. And places without network of instruments needed to measure freshwater for people and agriculture. We can then unify the measurements to create the consistent and accurate picture of no matter where we are. Because satellites get more complete coverage than ground-based instruments, we can use remote sensing to better see how the whole earth is changing over time. With the long data record, we can make better predictions about the water cycle, the climate, and the impact on humans. By inserting our earth from above, we get a much better understanding of what is happening on the surface in the atmosphere, underground, over the globe, and in our only backyard. The first thing the main point of showing that video was really the first thing that they illustrated, which was very brief, but we're going to go over in detail. And that is the difference between passive and active remote sensing. So up until this point, what we've been talking about is passive remote sensing. Passive remote sensing measures energy that is naturally emitted, typically from the sun. So that's again what we've been talking about so far. We've talked about how radiation is emitted from the sun, travels through space, ultimately through the earth's atmosphere, bounces off different surfaces of the earth, reflects back to our satellites or other remote sensing systems, where that reflectance is measured. By doing that, we're able to monitor things through time, how forests are changing, how oceans are changing, and we're able to even just go away and detect and classify different surfaces that we see on the earth. One more time just to drive it home. Passive remote sensing energy comes from the sun, bounces off targets or surfaces of the earth, travels to the sensor where it's measured. Active remote sensing does not use energy that is emitted from the sun. Active remote sensing instruments provide their own energy and just send radiation from the system itself, so from the remote sensing instrument. It sends radiation or energy towards the target, that radiation or energy bounces off of that target and then its reflectance is measured by the sensor. So fundamentally different than passive remote sensing. Passive remote sensing, we only measure energy that is naturally emitted, typically from the sun that comes down and bounces off the surface of the earth to the sensor. Active remote sensing, the instrument itself sends radiation, sends energy from it towards the target, towards the surface of the earth. That energy bounces off the surface of the earth, goes back towards the sensor, the remote sensing system, where it's measured. So this is how that looks in comparison side by side, passive remote sensing, energy from the sun, travels down, bounces off different surfaces, travels to our sensor where it's measured. Active remote sensing, the instrument emits its own energy, so no energy in this case is coming from the sun. The active remote sensing instrument emits its own energy which travels through the atmosphere, bounces off a target of interest, some surface on the earth, and then travels back towards the sensor where it's detected and measured. So for a kind of more of an exam style question, how might you answer the difference between passive and active remote sensing? You can really use this slide here, there's a couple key points. So passive, as I've mentioned, passive remote sensing uses energy that is naturally emitted from the sun. This energy is reflected off the surface of the earth, and that reflection of this energy is measured by the sensor. Active remote sensing instruments produce their own energy or radiation that energy travels towards a target where it's reflected, and then that sensor detects and measures that reflected radiation. So why might we want to use active remote sensing then? What might be its benefits? What might be some of its downfalls, some of the issues with it? Well, with active remote sensing, we often have, particularly with radar, a weather-independent instrument. So radar can actually see through clouds, rain, different kinds of weather. Passive remote sensing instruments can't. If we are using, say, Landsat or MODIS, which are forms of passive remote sensing instruments, and say, you know, Landsat is traveling over the surface of the earth, it's orbiting earth, and it's taking an image of the earth where there happens to be a bunch of cloud cover, then that image is just going to be a bunch of cloud. We're not actually going to be able to see any of the landforms in that image. With radar, in particular, the microwave energy that is emitted from radar actually penetrates and travels right through clouds, unobstructed, so we can image surfaces of the earth under any weather conditions. And for radar, doesn't apply to LiDAR or sonar. What does apply to all of the different kinds of active remote sensing is that it is sunlight independent, meaning that we just get to choose what form of energy or radiation is emitted from the active remote sensing instrument. With passive remote sensing instruments, we're just measuring, you know, the entire, whatever bands we choose to measure that are emitted from the sun. With active remote sensing, it's sunlight independent. We don't need the sun in order to remotely sense with active remote sensing systems. So we can survey at any time of day, night or day. Again, passive remote sensing instruments, if we want to measure the reflectance of visible green, blue, red, light, we have to be doing that during the day. If there's no sun there to be emitting radiation, visible radiation in the example that I'm referring to, then there's not going to be any visible radiation being reflected off that surface to be measured. That's not the case with active remote sensing. Even in the middle of the night, a radar satellite can still measure surfaces of the earth. It can still emit energy that goes down to the earth, bounces off the target, and travels back to the sensor. And as I mentioned before as well, you can also just control what energy is emitted. With radar, we generally use microwave energy. With LiDAR, we might use visible light energy, or we might use near infrared energy. But we get to define what kind of energy we're going to be emitting from the active remote sensing system. And lastly, one of its key, arguably the most valuable part of active remote sensing systems is that we can often get 3D information from active remote sensing systems. We can penetrate vegetation, soil, ice, and snow. So we can actually get a sense of the structure of forests and canopies. We can get a sense of layers of soil below just the surface. We can actually measure different properties of ice and snow below the top layer of ice or snow, say, on a frozen lake or on a glacier. We can actually see below the surface. So in that way, we get a kind of 3D or 3D dimensional type of information. So I think that's really, really hard and most of the time impossible to get with passive remote sensing systems. We'll talk a bit, well, we'll talk a lot in detail about how that's actually done, how we go away and get 3D information from active remote sensing systems. That'll be for later in the lecture and for next week. So we can get information on surface layers and structure, 3 dimensional information with active remote sensing systems. But then why wouldn't we just want to use active remote sensing systems all the time? Is maybe a question you could be asking yourself. Well, key problem or a key downside to active remote sensing systems is a very limited amount of spectral information. There's also quite a higher and more complicated level of analysis that's undertaken with active remote sensing systems and they're a lot more expensive. In short, passive remote sensing systems are simpler, easier to analyze and give us kind of a wider breadth of spectral information. Active remote sensing systems give us much more limited spectral information and are a lot more complicated and costly, but still sometimes can provide very, very valuable information despite those costs. So, question I have for you guys to try and brainstorm that I kind of briefly mentioned is why would active remote sensing systems have limited amounts of spectral information? So, spectral information, you know, if you think back to our resolutions lecture and what spectral resolution is, our spectral information is just, you know, the breadth of different bands or different portions of the electromagnetic spectrum that we're measuring. So, why might active remote sensing systems have limited spectral information as opposed to a passive system? I want to give you two or three minutes to brainstorm with a neighbor, ideally, or just by yourself. Take a couple minutes, think about it, and then I'll try it and crowdsource an answer from you guys and then we'll carry on. So, go ahead.................................................................................................................................................... You................................... And you mentioned two slides ago. The big thing that we'll talk about, at least in lecture today, is its detection. Its ability to measure how intense the microwave energy is, that it's emitted, bounced off a target and traveled back to the sensor and then measuring how intense that energy is. That's how radar is able to go away and classify different materials on the surface of the earth by measuring how intense that microwave energy is after it's been emitted, bounced off a target and traveled back to the sensor. And there's three factors that govern how intense that back scatter is or how intense that reflected energy is. One is surface roughness to his dielectric properties and three is moisture content. Surface roughness is the easiest one to wrap your head around because we've talked about diffuse and specular reflectors. I think? Have we talked about that? I'm not sure. Either way, it doesn't matter too much. A diffuse reflector, if we have talked about it and we're reviewing, if not, a diffuse reflector like this is when energy that is emitted from some source comes and hits that surface and then bounces off in all directions. So you can see here, energy, in this case, we're thinking about microwave energy, comes down from the sensor, hits the target, bounces off in all different kinds of directions. A specular reflector is where the angle of incidence is the same as the angle of reflection. So this angle here, that would be the angle of incidence, this would be the angle of reflection, and you can see it's the same angle. Specular reflectors are things like very flat, very reflective surfaces. Mirrors is the perfect example, but also in nature, things like glaciers, things like ice caps and ice sheets, very smooth, white, snowy icy features are often specular reflectors. And then oftentimes, if we have other kind of features like buildings that kind of create more of a corner, we call those corner reflectors, and you can imagine, depending on where this, you know, in this case, it's a plane, could be a satellite. Let's think of it, you know, in the situation of a plane in this case. If we have a radar instrument mounted on board this plane, and it is sending microwave energy to each of these different kinds of reflectors, you can imagine we're going to get a different level of microwave energy measured that's bounced off of those targets. If our airplane is right here, and thus our radar instrument is right here, and we send down energy towards this target here, say it's a bunch of ice, all that microwave energy is pretty much all going to travel this way, away from our instrument. If we have a corner reflector here, it's all going to travel, boom, boom, right back to the instrument. If we have a diffuse reflector here, some of that microwave energy is going to travel back to the instrument, some is going to travel away. Generally, the best kinds of surfaces for using radar are things like diffuse reflectors, where you get an equal amount of reflection in all directions. But in reality, when we're using radar to measure things like ice and glaciers and ice sheets and ice caps, we're often dealing with more of a specular reflector, which can be a big problem because the angle at which the energy is coming towards the target is going to heavily influence how much energy is backscattered or reflected back towards our sensor. So I'll give an example a little bit later about how exactly, or just a more specific example of an application of how understanding different surface roughnesses can allow us to use radar to detect different features. But for now, just know that with these different kinds of reflectors, they can heavily influence the amount of backscatter or reflected microwave energy back to our radar instrument. These two other properties are very important as well, but we're not going to talk about them in depth because they're very complicated and they're very physics heavy, and I'm not a physicist, and they mostly are over my head. But in short, dielectric properties are just the ability for a material to hold and transmit electricity or electric properties. And then moisture content is just how much moisture or liquid water is in a given surface. Generally, things that have a higher dielectric constant are going to reflect or backscatter more microwave energy, and generally things that have a higher moisture content are going to increase the level of backscatter or reflected microwave energy from a radar instrument. You don't need to know that I'm not going to test you on that. Just know that these are the three things that influence radar backscatter and just know kind of in a bit more detail how surface roughness works in terms of how it influences the backscatter you'd get from a radar instrument. Okay, now in radar, we also have to specify what band we're going to use. We have to specify the range of wavelength sizes of the energy that we're going to emit from the sensor. The same way how impassive remote sensing instruments like Landsat, like MODIS, we specify a wavelength range in the form of a band for reflectance that we're going to measure. This is the same concept really. It's just with radar, we're choosing not only the band that we're going to measure from energy emitting from the sun and bouncing off the surface of the earth. The band that we're choosing is the wavelength range of energy that we're going to emit from the radar sensor or from the radar instrument, and that we're then going to measure the backscatter or reflectance of. So there's four that we talk about in this class, the X, C, L, and P band. So they're all denoted with letters. This is their frequency and their wavelength size here. Not going to ask you to remember these values off by heart or anything. Note though, one thing kind of relating back to earlier topics from this course so far is we have our smaller wavelength size here, greater frequency. Larger wavelength size here, lower frequency. Each of these have a different purpose, have different applications in their use, which I've kind of listed on the right here. But what I mostly want you to remember about these different bands is A that they exist, that the X, C, L, and P band exist, and just relatively what their wavelength size is. So know that X is the smallest, P is the biggest, C is a bit bigger than X, L is a bit smaller than P. Some of their applications are here, I'm not going to test you on their applications. But I do want you to know that along with wavelength size comes the ability for these different bands to penetrate the ground or penetrate surfaces. The larger the band is, the larger the wavelength size of the band that you use in radar, the more ground penetration you're going to get when you're imaging a portion of the surface of the earth. So the longer the band, the further underground you're actually able to see and sense with a radar instrument. Now in general, when you're trying to pick which band you might want to use, the rule is you want to choose the wavelength size or the band that approximates your object of interest. If you want to look at a smaller target, say something like rain droplets. Rain droplets are quite small, therefore you probably want to use the X band, the X band is the smallest band that we've talked about. If you want to look at a medium size target, maybe you want to understand leaf structure in a forest or in a tree, then you're probably going to want to use something like C band, kind of a medium sized wavelength or band. And then if you want to be able to measure and get information about tree branches and tree trunks, then you probably want to use a larger band, maybe the L or the P band. And again, the band size or the wavelength range, wavelength size range of the different bands, relates to the size of the targets that you're interested in measuring. So rain droplets, quite small, if we go back here, rain droplets are often kind of in this size. Similarly, larger things like branches and tree trunks are often about this size. So the size of the wavelengths that you want to use for your band should be similar to the size of the targets that you're interested in measuring. Now we've kind of already briefly talked about radar sat one and radar sat two, which are Canadian owned and operated radar satellites. And radar sat two was launched a bit more recently in December of 2007. They both use the C band and have a spatial resolution of about eight to 100 meters. And with both of these satellites, we have the ability to see through clouds and it's very good at detecting sea ice and snow. Radar sat covers polar regions daily and then tempered zones and tropical zones every three to five days respectively. The third radar sat is the radar sat constellation. So radar sat one and radar sat two have already been launched. And then recently the radar sat constellation was launched. The radar sat constellation is a bit different because it is three identical satellites. So radar sat one was one single satellite. Radar sat two was one single satellite. The radar sat radar sat constellation, which is the third radar sat mission, is a set of three identical satellites. They have a higher spatial resolution of about three to eight meters for pixel size. And we're going to watch a quick video, believe now, about building and launching the radar sat constellation and then quickly another video on some of its applications. Our country is vast with very different challenges from coast to coast to coast. With the help of satellite technology, the Canadian space agency provides solutions to some of those challenges. The radar sat constellation mission uses a trio of satellites to take daily scans of our country and its waters. If radar and ship identification systems may collect invaluable information about our country. Satellite assisted to terminate nice conditions, helping captains navigate through our particular waters and were surprised to our own villages and helping northern communities travel safely over ice during hunting and fishing expeditions. Satellite monitors soil stability and changes in the permafrost and gather important data that contributes to understanding climate change so we can better protect our environment and our wildlife. To maximize crop yields, far-resused satellite data to measure moisture levels and soil, reducing how much water, pesticide and fertilizer may use in order to protect and improve the environment for years to come. The radar sat constellation satellites work together to bring solutions to important challenges that affect all Canadians. The Canadian space agency finding solutions for a better Canada. That is radar the radar sat constellation. One question again for you guys to think about, I'll help you answer it, but why do you think radar is particularly suitable for a country like Canada? Not every country in the world that is really into Earth observation systems has invested as heavily in radar as Canada has. Canada has been at the forefront of this kind of technology with the first launch of radar sat one and then the follow-up of radar sat two and then this radar sat constellation that's going to go up. Why do you think radar might be particularly suitable for a country like Canada? Think about Canada's geographic location and some of its properties as a whole. Anyone want to brainstorm any ideas? Because Canada is such a big country, we have so many different types of biomes, planes, we have farmland, forests, we have even deserts, because it's so large, there's lots of applications such as looking at it for crops, looking at it for ice melting in the Canadian door. What do you think, is there anything though that in what you just mentioned there that couldn't be done with a passive remote sensing instrument versus something like radar? Radar would be able to go through clouds. Yeah, great point. Cloudy city. Yeah, totally. So we got kind of a large coastal area that has a ton of clouds all the time. So radar would be really good for imaging those areas that are super cloudy. That's a great point. Building off of that in terms of its geographic location. Canada, yeah? Yeah. Exactly, yeah, that's one of the big points. So Canada has a really northern latitude. A large part of Canada up here is in the Arctic. A lot of those areas are dark for vast and very long amounts of time. So radar sat is super valuable for collecting data up in those areas because Landsat and Modus isn't able to collect data in the Arctic. And there's one other thing that comes along with that which is that again because we're such a high latitude northern latitude country. We also have a ton of ice. All up here is a lot of tundra, a lot of permafrost, a lot of ice. Something that radar has to be done with the water. And again because we're such a high latitude northern latitude country. We also have a ton of ice. All up here is a lot of tundra, a lot of permafrost, a lot of ice. Something that radar happens to be particularly good at collecting data for. The big, the kind of main answer that I wanted to get out of this was what was answered in the back there which is that Canada being a northern latitude country has large portions of its land in darkness for large portions of the year. So radar sat is super valuable because we get a continuous data stream even through the winter in those kind of Arctic portions when it's dark for most of the day. Okay. Couple of last kind of three things I'm going to talk about are some examples of radar applications just to give you a bit more of a specific sense of what radar can do. So we're going to talk about ice mapping, oil spill detection and ground ground penetrating radar for archaeology. So the image you see on the left here was actually the first seamless mosaic which just means the first kind of entire image that didn't have any breaks or anything in it. One whole continuous image of Antarctica. And it was compiled by radar sat between about September and October 1997. This was actually one of the largest motivations to initially launch radar sat one. We did not have a continuous full mosaic image of Antarctica until radar sat one was launched. So soon after it was launched, it, you know, it's one of its main purposes was to build up this map of all of Antarctica. Now we've moved on from just, you know, mapping what Antarctica looks like to trying to understand ice patterns and ice flows and topology around Antarctica. To be able to predict and understand influences of climate change as well as other, you know, other anthropogenic influences. So this image on the right here, this map, this was the first complete map of the speed and direction of ice flow of Antarctica. And it was derived from land, or from radar sat to with the addition and in collaboration of two other Japanese and European satellites. But radar sat to was really fundamental to building up this map and again gave us the first complete map of all of Antarctica to understand ice speed and ice direction flow, which was super, super, super valuable and continues to be valuable so that we can understand the ice dynamics that are occurring in such a place that is really, really hard to get to. You know, radar sat saves us from having to go down there and try and take these measurements by hand. Granted, people still do that because they have to validate the data. They have to ensure that what radar sat is telling them is actually true, but not nearly as, not as nearly small of a scale, you know. Generally, we'll talk about later in the lecture how data validation works with remote sensing. In this case, from this radar sat to map, there'd be a couple scientists that would go to a couple specific spots all across Antarctica and take some field measurements to ensure that what we're measuring with radar sat is actually pretty accurate. Okay, so big application of radar sat one and radar sat two in particular, both mapping Antarctica and understanding ice flow speed and direction in Antarctica. Quick video, I think about that here. For decades, scientists have been probing the green ice sheet from the ground, air and space. Now, a new study uses those observations to see within the ice sheet, laying bare a tail for more than 100,000 years in America. When we look inside the ice sheet, we can see the sink layers, formal 5,000 to years or so far. At snow and snow and noise, these layers get progressively impacted into ice, which then flows under its own weight. To get a precise history of a particular spot on an ice sheet, scientists drill into it and recover ice cores, which provide a record of the ice's age and with the past climate of this life. Seasonal variations, along with ash and volcanic eruptions, show up in the cores, allow it to date the ice and correlate simples from different sites. Please send this age information across the nation. The best tool that we have is ice penetrating radar, mountain on aircraft flying low over the surface. Radar transmits electromagnetic pulses into the ice and records the reflected signals, allowing us to track the depth of the layer detected in the ice. Since 2009, NASA's Operation Ice Bridge has flown over green on more than 100 times with a wide variety of instruments, including radar and produced vast quantities of data, adding to the work from many other missions. This is a lot of researchers who generate a three-dimensional map depicting the age of the ice throughout the green life. This 3D-8 map shows that three distinct periods of climate are evident within the ice sheet. The Holocene shown here in green, the last ice age, shown blue, and the Indian shown here in red. The top layers from the Holocene period formed during the last 11.7,000 years, and a fairly flat and uniform, though the thickness varies depending on how much snowfall occurred. Below this, deeper within the ice sheet, we see layers that formed during the last ice age. Layers from this period are darker and more complex, having been further squeezed and sometimes folded as they float over the rugged bedrock below. Deeper still, our layers of ice left over from the warm period before the last ice age, more than 115,000 years ago. Indian ice can reveal how the ice sheet responded to a period of warmth, similar to the one we are experiencing today. Several ice cores will recover Indian ice, but it is difficult to introvert. This new map of the age of the ice sheet shows that there is more Indian ice than expected in northern green room, where it may be easier for scientists to collect and analyze. This new analysis reveals a 3D map of the age of the greenland ice sheet, from the oldest Indian ice to the layers deposited during the last ice age, to the ice formed during the Holocene. The response of the ice sheet to past climate change led to its current age structure. Further studies will help us better understand how the greenland ice sheet will respond to today's changing climate. Okay, key differences there in that video from what I was talking about. First thing was I was talking about ice mapping in Antarctica, so that was obviously in Greenland. Second thing was I was those two images I was showing you of Antarctic maps, where collected with radar sat, so a space-borne satellite, whereas the application they were talking about there was a ground penetrating airborne radar instrument. So a radar instrument just mounted on board a plane that they just flew systematically over the ice sheet and were able to use to penetrate through the top of the ice surface and get a sense of the different layers below the surface. Okay, another really useful, really common application of radar is oil spill detection. So this is an example of an oil spill detection from a radar satellite in Wales on the right here, so you can see the oil spill there. And the way that it works is oil floats on top of water. When oil floats on top of, in this case, oceanic water, or just oceans, it suppresses oceanic capillary waves. Those are just kind of the little micro waves that occur on the surface of the water. So when oil is on top of the water, those capillary waves kind of disappear. It's much flatter, so it is a much less rough surface. Ocean water without the oil on top of it is a rougher surface. Oil on top of that suppresses these capillary waves, makes it a smoother surface. That difference in surface roughness, which we talked about earlier, which is one of the things that governs the backscatter we get from radar, this change in surface roughness created by an oil spill is very, very easily and accurately detected at night, at day, under any weather circumstances with radar. So that makes radar, again, very, very valuable for detecting something crucially important like oil spills, because we can do it at day or night in any weather circumstances, and the properties that oil change when it spills and is on top of oceans that change in surface roughness is very, very easily detectable and classified by radar data. Make sense? Okay, see. Last application, also a pretty fun one, is geophysical, archaeological study. In this case, several Viking Age and Medieval sites in Denmark. So this is how terrestrial ground penetrating radar works. So this here is a bunch of archaeologists, remote sensing scientists, that are driving ATVs or tractors on top of the surface or the area that they are researching and looking at. What they are towing in the case of these two, or pushing in the case of this top image, is a radar instrument. So you can see here, they're either pulling it, they're pushing it, and as they're moving along, that is that radar instrument, see they're being pulled behind them or pushed in front of them, and as they're traveling, that radar instrument is sending microwave energy through the ground, which is eventually hitting different layers below the surface of the ground and backscattering or reflecting back to that radar instrument. So this is the area that they were looking at on the right here, and then this is the radar image actually overlaid on their study area here. So they would have systematically, just in these little ATVs or tractors, gone back and forth and back and forth and back and forth and back and forth until ultimately they were able to build up an image of that entire area. And what they were able to do with that data was actually go and map medieval Viking buildings. So you can see here, all these buildings here in gray, denoted by H, are different buildings that they were able to detect, and how they were able to detect them was looking at a radar image like this. So this is a zoomed in radar image of one of the buildings that they were able to detect, and you can see here, you see these little dots here, there's like, dot, dot, dot, dot, dot, dot, dot, dot, dot, dot,.... and then next to it, parallel, there's these dots, dot, dot, dot, dot, dot, dot, dot, dot, dot, dot, dot. So those are pegs from a medieval Viking building. And you can see that building here, so that's the H1 building, that's this guy right here, and you can see the shape of it that they've mapped out, it's just denoted by these pegs that they were able to find that were used to hold up the building. Okay, couple of review questions for you guys to go over. I'll leave you for a couple minutes to brainstorm them, try to answer them yourself. I'll come back, answer them with you, and then I'm going to hand it to Evan, he's going to quickly talk about the midterm, and then I'll hand it to Tristan to talk about the assignment that's due this week, and then that'll be it. So if you want to head out now, you're welcome to, and in about three minutes or so, two and a half minutes, I'll go over the answers to these, and then we'll get Evan and then we'll get Tristan. Whoa, you know the secret power plate. Amazing, you gotta show me that. No, no, I didn't see that. I'm glad you guys made it. Yeah, thank you. Okay guys, let's crowd source these answers. So how does an active remote sensing instrument differ from a passive one? Can someone explain, please? Yeah, and then while passive remote sensing just receives the wavelengths which come from the sun, active remote sensing is like, giving them out and then making the waves reflect off of the earth and then sensing them again once they come back. Exactly, so passive remote sensing involves energy being emitted from the sun, bouncing off the surface of the earth, and then being measured by the sensor. Active remote sensing involves energy being emitted from the instrument itself, traveling towards the surface of the earth, reflecting off the surface of the earth, and then being measured again by the sensor. What are two advantages of using radar? There's two key ones we talked about, yeah. Yeah? Alright, so the two key things that were mentioned up there were A, that it is a weather-independent instrument, it can see through clouds and storms, and then also it can be used at any time of the day, so we can get radar imagery at night as well as daytime. Okay, what are the common radar bands used in practice that we discussed in class, and which of them are the largest? So there were four, someone just list them off for me? Yeah? XCLP. XCLP, and what was the largest? P is the largest, exactly. And then what types of waves, this was at the start of lecture, what types of waves do radar, lidar, and sonar each use? So what types of waves do radar use? We talked about that one the most. Yeah? Micro waves. And then what kind of waves does lidar use? Anyone remember? Top right. Top right, yeah? Laser light beams. Laser light beams, exactly. So laser light beams of either visible generally near infrared light, but just saying laser light beams is sufficient for that. And then sonar, what kind of waves does sonar use? Yeah? Sound waves, exactly. And then lastly, what do you think was the most interesting application of radar from what we discussed in class just now? We just discussed three, someone can just shout out one to me. Yeah? Mapping in the Viking Village is perfect. Ground penetrating radar used to map medieval buildings. Awesome. Okay, I'm going to hand it to Evan now. I'm just going to switch slides here. Is this one? Still working. Everyone, so I've got my normal stuff today. Tristan has office hours, two of them tomorrow, one of them on Thursday. So the next time was about an hour and 22 minutes. So don't come to us at the end of the semester right after the final saying I need this 2% on the midterm. Before you email me, make sure that if you have something like this, see this question 32, how you've answered geodetic points and the correct answer is geodetic point, it looks like you're incorrect. But if you check in the top right corner, right there, you've got four to four points. So before you email me about these, fill in the blank questions, please check this over. Every year I get a bunch of these ones and I just respond, you got the question right, but there's a bunch of time out of my hands. That's it for me, so if you have questions about the midterm, but the grades are not yet, so you shouldn't. If not, I'll hand it off to Tristan, he'll go over assignment four, three, three. Any questions about the midterm before the grades come out? Excellent. Okay, so there's not a whole lot to go over at this point because I haven't had too many questions. So yeah, so feel free to drop into my office hours tomorrow and Thursday, and I just want to make one clarification. I'll do that first. So for question 14 and 15, it says to deal with this date, August 21st of 2017, but the timeline you're looking at actually goes beyond that. So you're going to look at this entire chart, so it goes a bit into the 22nd as well. So just make sure you look at all of that to answer that question. And for question 14 and 15, you will be, you do choose all of those options between the two questions. Also for the DOP questions, remember that dilution of precision is it decreases your precision. So if you want to take a field measurement on a good time of day, it will have a low dilution of precision. Yeah, I think that's about it. Now I'll take any questions if anybody has them, and you can come down here and ask me them. Any questions? Did it ask for the lowest dilution of precision? Yeah, are you thinking of them when there's two different times? Yeah, yeah, you can pick either of those for the correct answer. Anything else? I've really looked at this from a state-bound chart. Is a higher numerical value for the dilution of precision mean that it's good dilution of precision as in like the satellites are more spread out? No, you want a lower dilution of precision. Or the numerical value is that means the satellites are more spread out so you have a good dilution? Yeah, yeah, correct. Higher value means the precision is more dilution. Essentially, yeah, yeah. Okay, you'll pick them down if you have any questions for me or Tristan or otherwise see you next week. That's when I got normal dilution. Oh, see, thank you. Yeah, thanks. Hi. So, really the most important part to remember here is that active remote sensing. So, I think for specifying, you know that very much like a satellite system to many satellites, but by doing that, whether or not they're tightly clustered together, looking at how they would make any difference. So, in case they're sitting in the radar, we're only looking at the same. With the past active remote sensing systems, say, modus and land side, we can look at 10, 20, 30 bands because we can silently use the meds. So, the reason you're seeing it, I've got every one, you have to look at what the auditing zone is. Sir? Yeah. Does that make sense? Yeah. Yeah. Yeah. So, with radar, you'll specify, you know, with band at least, but you're only ever seeing it. Well, so the fact that it is large is one reason you're seeing it, and then another reason is because the place you're looking is in this one latitude zone. So, you just need to find out which of these latitude zones are basically the GPS code. That's then damaging on the target, like North America's. It's really hard to just google that if you look at the basic mid-scene mode type, you'll see which one's in here. This is? Ultimately, that would be done by... So, this one is kind of... Separate instruments. With a passive... Oh, no, just this last one. You can measure the attitude. Yeah, yeah. So, you basically would... Yeah, yeah, you want to look between these three. Yeah. Active remote sensing. Okay, hope that makes sense. Yeah. Part of that is just practicality. If you view it really hard, then all the possible forms of radiation from a sensor at the same time. And then the other part of that is just... Yeah, just let me check the idea of useful information. With active remote sensing, pick what band we're going to use, and then part of the... There are two types. Yeah, so you can pick either of those. Or you can pick harder, which is how long it takes that energy to travel towards the target. That was off that target. Yeah, yeah. Travel back towards the center. Which tells us how far away that target is. Then the case of radar will also measure how strong that is. Because the passcode is still valid, because that is the most recent. But again, will be the information that it's received. We'll just want to see if something can be passed in. But it can be healthy, something different. If you put the forms in the store, you can help rate our bands. Once we're getting the measure of backscatter of the Earth-like-ish from the energy that we've emitted, and we're generally getting the information that we want. So that's kind of a very long tangential answer to your question. But there are kind of several different reasons why we do it all whenever you just want to be in. But the fact that we only view every one band, kind of the inherent answers to that question. Of why we have a spectral information. Yeah, that makes sense, because if I think about the gravitational pull from the sun, and what is the other, the solar radiation, those would be pretty weak effects on a satellite. So they would not likely affect the actual position of the satellite. So fine measurements. So that looks good there. Yeah, you're welcome. Yeah. Hi. Okay. Yeah, I mean, for both of these, it's like a combination of the lecture slides and videos. But yeah, so you're confident in the three U-bri-linder. And so you can go to a satellite receiver, see where sources of errors. Usually you won't be also getting error ground cases, because you'll just store that. And then you'll reflect a server. I think this isn't a feature of an order. But yeah, basically if your signal is bouncing off on the floodless surface, you would... You would still store that data. You'd still have the kind of drill that's in it. This one. Yeah, so look at atmospheric modeling different from... Yeah, so this is more on an application tool. These are all... You can also kind of classify these all those types of error modeling here. So the only one that I would look at is this front right here. As a pull-out radar. You're a pull-out radar. One standing. Yeah, you have to check. And then it's penetrating to a specific depth. And then bouncing off of whatever's down there. Coming back to the sensor. So by doing this, again and again, systematically across the study area, you're able to build up the entire image of that whole area of what the new-out penetrating area is seeing. And then in this case what they were able to do with that. Oh, sorry, so when you go to the quiz, you'll see that for this choice, for A, and this is just an example. You'll only have one of the two options. So precise or down for size. I think on the ground. Anyway, but I think you should build the lunar cycle. I would not look at those options. Yeah, you're welcome. Hey, I saw that all across that study area. I'm using that to make it go away. Where did you build this unit? In this case, this building better is this piece of one build on there. You weren't about that, but this one. Yeah, yeah. So that one made me kind of a prick. Because that one we didn't cover in class, but that's kind of a red herring. Oh, so I had to give a heavy penalty to Aya. Yeah. Yeah, that's a good point. You could put it, but could you learn anything about the satellites? There's not really a means that there's a constellation. Okay, because that's what I mean. I didn't care about anything, but we thought it was assuming it wasn't that one. But I didn't know whether it was that. Yeah, yeah. No. And then if you think about, like, all those dilution and measurement are related, and they are based on the relative positions in the satellites. Yeah. And then that thing? Oh, no, it was, you know, also dilution. Especially given the amount of people who can take their idea. And then looking at the individual's opportunities. They're all like, you know, pain number, not only for the blood, they're most different. And having, like, that thing, like, that pattern, do it, it would, would you consider it a 10% match? Also a factor in that, because I am here. Yeah. I think I will. And then you set it all around in the area. But there were lots around, but they weren't, like, so they were trying to capture. So you're using the Skytraits answer. Yeah. I would not use the Skytraits. Oh. Yeah. You're all free here, changing. I would use, you know, a lot of them. Yeah. I would use, you know, a whole period of style. Yeah. So, Keith, I would use these charts. I would use these charts. Whether or not that's true. I don't know. But Keith claimed that he was still eating his own words. I would use both as, so, just, on a good time, we won a high number of times. And they, and it copied it, and you, and, or, you played it? Yeah. So you just go over the time, see if it corresponds to any, in the number of satellites. And it's sort of, and they, they're, they're, they're, they're, right to you. But I think you can all get into the, so you close. Yeah. Then, then, can you, like, how many years have you done? A bunch of different kind of things. And that's what I use. But yeah, I think it comes, like, that's what they're telling me. Yeah. And it just might be, that, instead of, like, I, but, obviously, for the story. Even that, what's the, the part that's like, okay, yeah, yeah. We're the, the answers you had were those, the ones you'd said, just translate. Yeah. Yeah, yeah, yeah. So I would, I would double check this. Yeah. And if you can email me to, uh, to can you answers, and I can tell you if you're on the right track. Yeah. Yeah, yeah. Yeah, yeah, yeah, because those aren't correct. Yeah. So, yeah, yeah. You follow up here. I'll tell you when I stick them. Yeah, sure. Okay. Yeah. Yeah. And it seems like, I was, like, waiting. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Well, is the, like, when you look at it, is it in the aggregator that passed? Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. The content into a key for the, that's super sense. Right. Yeah, that looks good. I didn't even know about the final topic things from yesterday. And I just read your reply to the book. Yeah, it's really clear now. Only thing I'm wondering is if you ask about the actual... Yeah, you too is a pretty good threshold. Yeah, because it's really a combination of high numbers that love you. But you ended up being the good threshold. It's kind of an overall... It's like high. It's kind of an overall... So, like, you would say in that case, you would just say, bands in the near-imperrende character school. Okay, that's all. Okay, I just wanted to make sure I had the turn call. Are you the other ones like spatial resolution? Yeah. High spatial resolution, which you can't give in your mirror. Or high-time room. Oh, yeah. Yeah, that looks good. Yeah, that looks good. So, you'd need to use a 5-weather... Yeah, just like a brain. And you'd be like, exactly as it's a mouse. So, could you say, like, a long time, for example, like, a little bit under the computer. Yeah, that's different. Just give it, you can just... Oh, right. Yeah, that's good. Yeah, that's good. Yeah, that's good. It's looking good. You're like a moderate to the... Yeah, okay, exactly. That's great. Thank you so much. Yeah, they're broke. That's your shoot. Yep. So, yeah, so, like, a healthy satellite, like, the measurements, like, I guess, ideally, would be for the birthdays. So, but even if it's in the past day, it is... Yeah, it doesn't mean that's doing health. It means it has the latest update. It's so weird. So, that can still mean it's health. Yeah. I mean, this guy kind of... Inhumanely is like, it was pretty... It was pretty flagrant. All right, hi everyone. Welcome. Happy Monday. I wanted to, i'll go over this. This is kind of a quick side note to start off. I'll go over this again when we get closer to the date and How the weeks are going to shape up and how the syllabus looks. I just realized for a couple of classes that i teach that the Last weekend where classes are still happening, which is the Weekend of the 8th and 9th, it is a holiday the friday and the Monday before and after that. It's a holiday on April 10th for Easter and the university is Going to be closed and then we technically have a lecture Scheduled for the following day on the 11th. So, you know, if we were to shape out normally we'd have a Lecture on the 3rd and the 4th of April, which is the Monday Tuesday and then one following on the 11th. The 11th realistically would be our review session but maybe I'll, you guys can think about it and maybe i'll kind of pull You guys in a week or two to see what you want to do. Just in the sense that, yeah, i don't know, i feel like it's Going to be a bit awkward to come back and try to lecture after A long weekend on this one day before the end of class and Before exam start. So we could just take that day off. We don't have to have a lecture that day. We don't have to do the review session that day. I could move it to the 4th, essentially the week before and Then the 4th would be our last day of class. So maybe think about how you want to do that. It would involve me maybe condensing a couple lectures, Maybe moving a little bit quicker than i was planning to. But it honestly wouldn't result in that much of a change and Would just kind of, at least for 1-2-7 it would relieve you from Having to attend lecture anytime after April 4th. And then we have the final exam which i'll talk about in a Moment later. So maybe think about that. I'll maybe give you guys to give me a vote next week or the week After that or something and see how you guys would like to do it. But i don't mind either way so it's kind of up to you. Okay. So today, quick notes about what you Should be working on. Blog post four is do march 9th. Assignment four is do march 16th. March 16th is next thursday. So liana has some office hours Four that should say assignment three. Evanman. Come on. I should say assignment four. Right. Assignment three you already submitted. I'm not losing my mind right? Yeah. But so liana will have some office hours this week and next Week for that. If this wants to load i shouldn't have Click save. There should be now an introduction Video posted on canvas on the assignment four submission page So i usually tell the tas either they can come into Lecture at the end of class and give a bit of an introduction to Help provide you guys some context about what you're doing for The assignment or they can just make a video and post that on Canvas. So she opted to make a video. There's a video on canvas on the lab on the assignment four Submission page for assignment four. So go check that out when you Get the chance and then next week, monday tuesday i'll get liana To come into class and give you some tips and tricks. You can ask her questions if you can't make it to office hours And so on. These are her office hours this Week and next week and so the last set are the day the Assignment is due from two to three pm and she's also gone some Next week from ten to eleven and some this week as well. Okay. Final exam date has been given to us. It is dreadfully at seven pm. They always, i don't know what it Is with this course. I guess i'm just, i'm pretty low. I'm probably the lowest in seniority in terms of instructors Across all of ubc. So they probably are just like Whatever chris is just a lecture who cares, give them a Crappy time. So i'm sorry for that. So april 27th at seven pm. That i think is the second last Day of exams. So it really, i mean, obviously, you know, seven P.M. not a deal for you guys and i'm sorry it's kind of at the Very end of exam season. Also sucks for us because we have a Super tight timeline to submit final grades for. So we essentially Have to have this marked by the following end of the following Day. So it blows for us too. But yeah, sorry about that. But that's what we'll work with. It'll also be fully online and Also open buck. So if you're planning on leaving Vancouver Internet, we'll give some more details and obviously post Practice questions and things like that closer to the exam. Okay, any questions about course logistics? Assignment, blog posts, exams, et cetera. Yeah? I'm not sure if the end meant for it, but the slam up Boarders can know video yet. There's no video yet? Okay. There's instructions, but no. Okay, i told her i just talked to her and she said that she Had accidentally uploaded it to the wrong course section to like An older year's course section. So she said she was uploading it Right now. So maybe refresh in 30 minutes or so and it should be There. I'll send her a quick message as well and just say Hey, it's still not up yet. She just messaged me at 507 saying should be up. Do you see it now? If you refresh. Nothing yet. Okay. Okay, i'll look into that after it should be posted very soon I'll get that sorted out with her. Okay. Okay, going back then to lecture content what we're talking About today, we are continuing on with active remote sensing. So just to quickly remind us what we're talking about, i know It's been a little while since we've seen each other, we're Talking about active versus passive remote sensing. Passive remote sensing up until now has dominantly been what we've Been talking about. Passive remote sensing involves Energy, radiation, being emitted from the sun, traveling Through space, getting towards the earth, traveling through the Earth's atmosphere, bouncing off of some surfaces of the earth, Traveling then back to through the atmosphere, some sort of Remote sensing sensor, some sort of remote sensing system Where we measure how much reflectance for different Bands or different wavelengths is being reflected off that surface. That's passive remote sensing. Active remote sensing, we Emmute energy from the instrument itself. So our active remote Sensing systems send out energy from the instrument, just like this. Boom, it travels towards a target or towards the surface, bounces Off that target, bounces off the surface of the earth, travels Back to the instrument, and that's what we then detect and measure. Now we talked about briefly i just introduced three different kinds Of active remote sensing, radar, lidar, and sonar. Radar was the one that we focused on the most, but we generally Just noted that radar uses microwaves, lidar, and That's what we'll focus on mostly today. Sonar uses sound waves, so Sound propagation. I got a slide or two about sonar. We don't talk about it that Tort much because it's not very applicable To earth observation remote sensing, but i'll briefly discuss it and some Applications of it. Last week on Tuesday we mostly focused on radar. We talked about how radar asshole works. We talked about how radar Instruments emit micro-sensors. We talked about how radar instruments emit microwave Energy, so even though it's called radio detection and raging, they Emit microwave energy. That energy travels towards a target, and Then it bounces off that target, and depending on different Properties of that target, such as the surface roughness of that target, The moisture content, and the dielectric properties of that Surface, we might get a different level of back scatter or Reflectance of that microwave energy. After it's been emitted, Bounced off that target and traveled back to the sensor. We talked about how radar is advantageous because it's an all weather System. Microwave energy can penetrate through clouds, through Storms. Wow. Didn't see that coming. Microwave energy can penetrate Through clouds, through storms, through all kinds of weather. You can also use it at night or during the day, so it's an All time of the day, all weather remote sensing system. The bands that we talked about used in radar, the XCL and the P bands. We talked about briefly just how The band size that you pick should be Related or associated to the size of the targets that you're Looking at. You want to use smaller radar bands for looking at smaller Targets. You want to use larger radar bands for looking at larger targets. So if you're looking at big tree trunks, you want to use Larger radar bands. If you're looking at tiny little leaves Or little rain droplets, you want to use smaller radar bands. We also talked briefly about how radar can be used to Penetrate the ground. And that the amount of ground penetration That you can get with radar also depends on the bands you use. The larger the band, the more ground penetration you're going to get With radar. We briefly talked about the different radar Satellites, particularly that Canada is involved in. We talked about radar sat 1, radar sat 2, Then briefly mentioned the radar sat constellation, which is Three identical radar sat satellites. And then we talked about three Applications of radar data. We talked about Monitoring and detecting oil spills. We talked about antarctica And we talked about the use of ground penetrating radar in archaeology Specifically for mapping of ancient Viking villages. So now today we're going to talk about lidar. Lidar is what's used to create something called a point cloud. This is an example of what a point cloud looks like. A point cloud is a set of or a collection of three Dimensional points where each point represents A three dimensional coordinate in space. So each one of these Individual dots that you see here on this point cloud Has an x, y, and z coordinate associated with it. So it has an x and y coordinate as well as an elevation A z coordinate associated with it. And when you get a Large collection of these point clouds, you can see You can start to make out some features and some attributes of different kinds Of targets. In this case, and what i'll mostly be talking About today, is the use of lidar in forest tree. So in this case we've used a lidar instrument to derive a point cloud Of a forest. So you can see here kind of the individual trees That are making up this forest. You can see the top of their canopy here. You can kind of get a sense of the shape of these trees here. And then you can also get a sense of where the ground is located below those trees. So we're going to talk about in depth how you go away and create this point Cloud using lidar data. So lidar is an acronym for Light, detection, and ranging. And how lidar works Is it sends a pulse of laser light towards a target. A laser is just a narrow beam of light all with the exact Same wavelength. So this that i have in my hand here, this is A laser. This is sending pulses of laser light. This is just one constant stream of laser light that's being sent Towards whatever target i'm pointing at. But it's a laser because it's This discrete narrow beam of light. And all of this light Has the exact same wavelength size. In this case, on my laser pointer It's red. So with lidar You can pick which wavelength size You want to use to send your laser pulse. You can use visible red If you want, just like i'm using on this laser pointer. You can use visible green You can use near infrared. You can use short wave infrared. You get to pick What you want your laser pulses to be. But ultimately Whatever wavelength you choose, you're still going to be Pulsing laser energy. Which if i were to Say a lidar instrument and i were pulsing it at a target of interest Say my target was the screen, it would look like this. That would be pulses of laser light. It's sending and then it stops. Then it's sending and then it stops. Now, why do you think Or what do you think might be the most common types of Electromagnetic waves or radiation used in lidar For environmental applications. Specifically for say measuring And looking at vegetation, forestry, what kind of Laser light. If i have a lidar instrument on Board this plane here and i'm sending lidar pulses down towards this forest Here and i want to get a nice clean response Of that laser light back towards the sensor. Given what we've talked about And say our spectral signature is lecture. What do you think might be A good wavelength size to use or a Good portion of the Electromagnetic spectrum to use. So say visible red, visible green Visible blue, near infrared, short wave infrared. What do you think? Near infrared for vegetation for Forestry purposes is generally the most common because we know From our spectral signature's lecture that healthy vegetation in particular reflects near infrared light very very very Strongly. So when we send individual Solar pulses down towards say forest we're going to get a strong Response a nice clean bounce of that near Infrared laser light off of each individual leaf Which ultimately is going to give us some very very high resolution Three dimensional information. Okay i have a quick video Here that just kind of demonstrates it actually uses A space-borne lidar instrument as an example to look At the terrain of Mars i believe. It's not earth But it's a good depiction and a good illustration of exactly how light our works If you want to find out how to call mountains or other planets How would you do it? If you're under it, it's easy. You can take a picture If you're under it, it's easy. You can take a picture Fly over the mountain or you can actually go there and measure how high it is Another planet is much more difficult. You might be able to estimate how to use your shadows Or even take three pictures from a satellite. But what do you want to know about the mountain look like is a 3D model? To find out NASA scientists can use a precise measuring tool called LiDAR. Now to not a satellite or any high level climate LiDAR instruments Are able to accurately measure the distance between instrument and landscape below using laser pulses To make these measurements, the LiDAR instruments first send a laser pulse down to the planet's surface The pulse hits the ground and reflects back to the instrument for an onboarded animation The time is of the pulse to make it straight That gives a precise measurement of the distance between the instrument and the ground with respect to the planet's gravitational center As a satellite passes over the landscape, the instrument sends out a series of regular pulses By recording and combining these measurements, scientists can use the instrument to gradually build up a map of the height of the terrain After many more measurements, the end of the build is a high resolution 3D model That scientists can view is if they are actually on a planet flying over the terrain They can then set up a shape in more detail, looking for clues to the relative ages of craters, the shape of values and landscape features, and much more The LiDAR is far more versatile than simply measuring the shapes of mountains and craters Earth scientists, for example, use LiDAR to measure the height of density in the Earth's forests Other with huge LiDAR, it's a small change in the height of the Earth's major ice caps over time Still other scientists use LiDAR to study the composition and structure of Earth's atmosphere, as well as the atmosphere of other planets And they can do all that with a gravity plumber mountain Okay, so just to break that down, in a bit more detail, I'll go over the nice illustration they gave to show exactly how LiDAR works But LiDAR sends pulses of laser light towards a target Most times, often times, LiDAR is mounted on an airplane, sometimes a drone, that's becoming more common nowadays It's not as frequently used as a spaceport device, it still is, sometimes, ice-at, which we'll talk about is a spaceport LiDAR instrument Most commonly, though, it's onboard an airplane, so we'll talk about it in that context Imagine there's a LiDAR instrument onboard this airplane The airplane is flying over a surface, and is continuously sending down pulses of laser light towards the surface Generally pulses of near-infrared laser light Now, that's kind of the first technology involved in the makeup of a LiDAR instrument, is laser ranging So, by sending pulses of laser light towards the surface of the Earth, and then measuring how long it takes those pulses to bounce off a target And come back to the instrument, we can determine how far away that target is from the instrument So, I'll say that again, if we have a LiDAR instrument onboard a plane, my plane is flying over a certain area The plane is sending down pulses of laser light, and those pulses of laser light are traveling at the speed of light, which we know is a constant speed Then we can time how long it takes for each pulse of laser light to be emitted from the LiDAR instrument, travel towards the surface of the Earth, bounce off of a target, travel back towards the instrument, and by using, by timing how long it takes to do that And then combining our knowledge of what the speed of light is, what the speed that those waves that laser light is traveling at, we can then determine how far away that target is So, that's the concept of laser ranging, the ability to measure time of how long it takes a laser pulse to be emitted from a sensor, travel towards a target, bounce off that target, and travel back towards the sensor The other technology that LiDAR takes advantage of is GPS, or GNSS, we've talked about GPS and GNSS in detail, but essentially what GPS allows us to do now is to determine our geographic location and height of the sensor with a really, really high accuracy So, that means that when we are again in our plane, flying over a portion of the surface of the Earth, sending our laser pulses down We know the exact location, we know our X, our Y, and our Z coordinate, so we know our exact location and height that each individual laser pulse is emitted from We know exactly where each individual pulse is sent from, because we have a really accurate GPS measurement If we know exactly where each pulse is sent from, and then exactly how long it takes for that pulse to travel towards the surface of the Earth, bounce off of a target, and travel back towards the sensor That not only do we know how far away that target is from the sensor, we know also exactly where that target is in three-dimensional space The last technology that is involved in LiDAR is the use of something called inertial measurement unit, or IMU, and it just helps determine the precise orientation of the sensor So, when you are onboard an airplane, an airplane can have pitch, yaw, and roll, so it can roll like this, it can pitch like this, or it can yaw like this Which essentially just means that the airplane can kind of distort the exact angle that it is flying at, or that it is positioned at, depending on winds, depending on a whole bunch of things in nature that we can't really control But what we can do is we can measure with an inertial measurement unit exactly what the pitch, roll, and yaw is of the airplane So if we know the exact orientation of the plane, we know the exact position of the plane, and then we measure, we time how long it takes that laser pulse to travel from the sensor to the target back to the sensor And then we can get an ultra-altrow precise measurement in three-dimensional space of how far away that target is on the surface of the ground that the laser pulse is bouncing off of Okay, just a bit more detail here, LiDAR systems emit pulses of laser light, again we just time how long it takes that pulse to travel to the target, bounce off that target, and return to the sensor And we use the speed of light, it calculates how far that target is from the sensor, and then we combine our GPS and IMU measurements to calculate where that target is in three-dimensional space So that means that if we go back, if you think about back to that first image we looked at of the point cloud, each one of those three-dimensional points in the point cloud is something that a laser pulse bounced off of And we get all of those really, really precise individual three-dimensional points because we have this laser pulse that's sending towards the surface of the earth, bouncing off a bunch of targets, and we get a really, really precise measurement of exactly where that target is in three-dimensional space So we're using the combination of LiDAR, which is laser ranging, GPS, and IMU technologies Okay, so just to look at that video that we showed or that I showed, we're not going to look at the whole thing again, but again, we have this instrument or LiDAR instrument So we're going to look at the surface of the earth, this is a satellite, could be a plane, sliding on the surface of the earth, sending pulses down, boom, time, how long it takes to get to the ground and back So each of these pulses is timing, it's timing, it's timing, it's timing, so it knows how far away, that's what this is, how far away those targets are from the sensor And we've got a nice measurement of the terrain, of exactly where the ground is, now we can do the exact same thing as that, but we can do it with a forest And we can get individual points, not just across entire hills like this, but for each individual leaf or each individual branch of a tree, which gives us that really, really high resolution three-dimensional information Now LiDAR system generally emits thousands to millions of pulses per second, the speed of light is very, very fast as we've talked about, about 3.0 times 10 to the 8 meters per second So LiDAR system is capable of emitting thousands to millions of pulses per second, so every second it's going baaaaa Sending tons and tons and tons of laser pulses towards the surface of the earth Each one of those pulses, each individual laser pulse that's being sent from the sensor towards the target may have 1 to 5 return measurements So that means that if we send one pulse towards the target, it might hit a leaf and then send back a signal to our instrument But the original laser pulse may keep traveling downwards past that leaf, and then hit a leaf below that, and then send us to the surface And then send us another signal back to the sensor, and then the pulse will keep traveling below that, maybe hit another branch, send another signal back to the sensor And then keep traveling below that, eventually hit the ground, once it hits the ground it'll send its final response, its final measurement back to the sensor So in that way, with a single individual laser pulse, we can get multiple three-dimensional measurements That lends itself in part to why we're able to get such high-resolution three-dimensional information from LIDAR If you're sending thousands to millions of pulses of lasers per second, and each one of those pulses is resulting in 1 to 5 three-dimensional measurements in space You can imagine the extreme wealth of information you're getting from that, the point cloud you would get would be very, very, very dense You might get a point cloud every couple of centimeters in three-dimensional space, which gives you a really good sense of things like structure of trees, things like terrain, three-dimensional information Now this is just a depiction of ultimately how that would work, so if this is our plane here, we're sending one individual laser pulse from our plane down towards the earth When it first hits the top of the canopy of this tree here, it'll send its first return back to the sensor So our laser light travels down, boom, it hits the tree, some of that laser light gets sent back to the sensor, so it records that as what's called the first return That's the first measurement that it gets from that laser pulse Some of that laser light continues to travel deeper into the canopy, and then it hits another dense layer of leaves, and it sends a second return back to the sensor It keeps traveling down, maybe hits another smaller tree below, it sends another return back to the sensor, and then it keeps traveling even lower, eventually hits the ground, sends a really strong return back to the sensor And the build up of those returns essentially looks like this, it looks like this graph here So as the sensor, once it sends one individual laser pulse, it's continuously recording how much of that laser light is being reflected back to the sensor So you can see here, the return energy on the x-axis here is just how much energy is being reflected back to the sensor after it's being emitted from the plane here You can see here, the first return, right here, that's kind of where there's a peak in the energy that's reflected, second return, there's another peak here, third return, there's another peak here, fourth return, there's another peak here So each one of these returns is essentially just a peak in the amount of laser light being reflected back towards the sensor And in this case, we see four returns, sometimes it might just be only one return, if there's no trees for example, it's just bare ground that the laser pulse is sending down to Then there's probably just going to be one return back to the sensor, because there aren't a bunch of different targets for it to bounce off of But if we have say, you know, a forest that has a very, very loose or very, very sparse canopy that has lots of space in it, then the laser light will continuously penetrate all the way through the canopy And intermittently send back returns or signals to the sensor as it does so Okay, any questions about that? Yeah? Like how long does that say it's like a peak, whether it's a large boulder, like, that's an impact, and then it's just like, is it less than different? Sorry, I can't, I didn't catch that. It's a large boulder on the ground, right? A large boulder? Yeah. So that's how big this boulder has to be created, like, since there is a difference in the canopy, meaning the sensor is about the counter So if it's a, so that's a key thing, the differently from radar, radar actually penetrates through surfaces, LiDAR doesn't actually penetrate through surfaces So the difference with LiDAR is that say a pulse, you know, say a pulse about this big is sent down to the ground, then a target smaller than that will partially send back some energy to the sensor But the rest of the light will keep going down. So if there's a boulder, say, a rock, and it's say this big, but the size of the laser pulse I send down is this big, and it hits that boulder right in the center, then it's just going to send back one pulse So it's not actually going to know where the ground is below that boulder. Yeah, no. I'm just saying how big this, like, whatever it is, the same difference in the sensing, so that we know there is something there. Like, can I feel like my dude, like, would we know there's a small rock in there, or would you not know? Yeah, totally. That's a great question. So it depends on a lot of things. It depends on how many pulses you're emitting, whether you're emitting a couple hundred pulses per second, or a couple million pulses per second. It depends on how wide that pulse is. It can be, say, a couple centimeters wide, or it can be 10, 20, 50 meters wide. Generally speaking with LiDAR, we can detect, you know, changes in terrain, into the system. And terrain, into topography, that are very, very, very minute centimeters in size. So it isn't, I'd say, you know, a matter of how big does say a boulder need to be to detect it. It's more of a question of how small can it be for us to still detect it. And that's usually around maybe, you know, we can't see, like, pebbles, but with high resolution LiDAR data, we can definitely see, you know, cobbles and things like that pretty well. You're not going to get a sense of, you know, how high the top of that cobble is below the ground, necessarily. But if it's just a cobble standing alone, you'd get ground measurements next to it. So you'd see this kind of like hump in the ground so you could maybe determine it like that. Yeah, did that kind of answer your question? Yeah? So for a peak, you're returning it, returning it, basically, you're turning it back to the system. Correct. Yeah, that's exactly right. Okay, so this is another animation to help us kind of wrap our heads around how LiDAR works. So let's say this blue dot here is a pulse of laser light traveling from an airplane down towards a target. Down, it's going to travel through this tree, through this tree here, hit the ground, and as it does so, on the right here, we're going to see the return strength and the elevation of that return strength on the y-axis here. So let's watch this play. So the laser light comes down, you'll see boom, and hit something. There's a jump, there's a jump, and then ultimately, once it hits the ground, there'd be another kind of quite large jump. But that's essentially how it works. So this laser light travels towards the surface of the earth. It hits some targets up here, some energy is reflected back to the sensor, some energy continues to travel through the canopy, then it hits this canopy layer here. It sends some light back to the sensor, some light continues down through the canopy, and then hits this part of the canopy here, sends some light back to the sensor, some light continues down towards the ground. So ultimately, once it hits the ground, all of the light is going to travel back to the sensor, at least all of the remaining light that made it that far. Now that brings me to that question, which is really good, because it brings me to a key point, which I just mentioned, but I'll reiterate, LIDAR does not penetrate through targets. Radar can penetrate through targets. It can go straight through the surface, straight through soil. LIDAR cannot. See that LIDAR ultimately enables us to get this structural 3D information of say the canopy of this tree here, is because some light from an individual LIDAR pulse will be sent back to the sensor once it hits a target. But if that target is small or sparse, say it's just a small leaf, and it doesn't make up or isn't as big as the entire footprint or the width of the laser pulse that's being emitted, then some laser light will continue to travel downwards towards the ground, even after some energy or some laser light has bounced back towards the target. That's ultimately how we're able to get more than one return per laser pulse, but it's a key distinction there. LIDAR does allow us to get structural 3D information, but it doesn't actually penetrate through things. It just might hit a target, and then some of that light be reflected, and then some of the light that missed that target will keep traveling downwards towards the ground, whereas radar can actually just see right through things. It can go right through ice, right through soil. Okay, there's two types. Any last questions about just kind of technically how LIDAR works? Yeah? So can you not use any wavelength you want on the electromagnetic spectrum for LIDAR? You can. You can just use the wavelengths that radar does to penetrate, or is that just the radar? Exactly. Okay. Exactly. You got it. Yeah, exactly. So you can use, generally speaking, for purposes, for LIDAR purposes, you'll see very rarely, maybe, visible blue or red light. Sometimes, more commonly than that, you might see visible green light. That far, the most common is near-infrared light, because most of the time we're looking at forests and vegetated areas, which reflect near-infrared light really strongly. In theory, you could use, say, shortwave infrared, but beyond that portion of the spectrum, as soon as we get into ultraviolet light, that doesn't really provide us any information, because that light doesn't have very important or very tangible spectral signatures that we can measure. And same thing on the other side of the spectrum. Thermal infrared are emitted radiation, so measuring its reflectance isn't really going to work. So generally, we'd use either the visible, somewhere in the visible or near-infrared part of the spectrum. And then, like you said, if you were to say, use microwave, then you would be using a radar instrument. And the thing is, with sending that and the big reason that you're able to kind of get this very fine 3D information with LIDAR does have to do a little bit with the size of the wavelengths you're using. Radar uses larger wavelengths than what's available in the visible and near-infrared portion of the spectrum. So if you're trying to get, say something like 3D structural information of a forest canopy, which you can get. You can get really nice three-dimensional information with LIDAR of the entire structure of a forest, all its branches, all its leaves, everything. You're not going to be able to get that with radar because radar's wavelengths are so big, they're not going to interact with all of those features nearly as much. Does that kind of help? Okay. So, a couple applications of LIDAR that we'll talk about. There's two key ones that I'll discuss in class that is the use of LIDAR to derive terrain data in the form of digital elevation models, and the use of LIDAR to derive structural information on vegetation. In the form of point clouds. Point clouds being that image that we looked at when we first started talking about LIDAR. Okay, so first we'll talk about digital elevation models. So digital elevation models tell us what the elevation is of a particular area. LIDAR gives us really, really highly accurate, high spatial resolution digital elevation models. So in this case, this is an example of a digital elevation model. This is just a 3D depiction of what a digital elevation model would look like. In reality, it would actually just look like something like this. So this is an example where they had, they used LIDAR. So each one of these boxes is describing a LIDAR footprint. So essentially an area where they were sending down pulses of laser light. And then the gray here, this kind of gray scale area here, is the digital elevation model that they derived from that. You can see here ranges on a scale from 2,250 meters to 1,050 meters. And essentially each pixel in this digital elevation model, each pixel, which represents a specific area, depending on the spatial resolution of that pixel. Say it's a 10 meter pixel, it's representing a 10 by 10 meter area in real life on the surface of the ground. But as opposed to the data that we've been talking about, that you can derive from passive remote sensing instruments, where each pixel represents the reflectance of radiation for a particular band or wavelength. In this example here, we've taken the LIDAR information and we've derived a digital elevation model, where each pixel represents the elevation of the area in that pixel. So 20 meters, 30 meters, 40 meters represents an elevation. You can get really, really, really accurate terrain information from LIDAR, because there's so many laser pulses being sent down at such a high frequency, you can get a very, very high spatial resolution when resolving digital elevation models. So that's all this is as well, this is just a 3D depiction of what you see in gray here. This is just kind of a classic raster where each pixel describes, based off this gray scale, whether it's very high elevation in light gray here, or very low elevation in black or dark gray. This is just a 3D depiction of that exact same thing. Now LIDAR is also a really excellent tool for measuring forest structure. It can be used to gather a tremendous amount of detail relating to forest structure and the vertical organization of plant biomass. So if we look at this example here, this is a point cloud on the right here. We got on the left here a pole or sapling point cloud. A pole or sapling in kind of forestry terms is just a very young forest. Very young forests are generally very, very dense, while old growth forests are generally very structurally complex. There's a lot of gaps in old growth forests, a lot of complexity in the branches, a lot of different levels to old growth forests, while young forests mostly have all of their trees, all the exact same age, all clumped together really tightly. And you can see that here, this is a young forest, a pole or sapling forest on the left, and then an old growth forest on the right here. There's a couple things that you can really clearly make out from this point cloud. And again, this is just a cross section of these point clouds, where each point here, each little bubble represents a dimension in 3D space. It represents 3D coordinates. But you can tell here from looking at the cross section of that, that A, the left, the pole or sapling forest is not very structurally complex. You can tell most of the biomass, most of the canopy in this forest, is right at the top, all at the exact same height. You can see the density in returns from LIDAR is very, very highly dense right in that area. And that's because most of the biomass, most of the leaves and branches of this forest are all very, very, very tight right at this exact elevation. Contrast that to on the right here with that old growth forest. You see a very structurally complex forest. You see that there's lots of light, lots of laser pulses that are not only able to make it all the way from the top through to the middle of the canopy of this forest, but also all the way down to the ground. You'll notice in the pole sapling stand on the left here, we don't really have many points that are denoting or describing where the actual ground is. And that makes it really tough to actually derive a digital elevation model when we're looking at these very young forests. If we can't get LIDAR pulses that get all the way to the ground, then it's very hard for us to determine what the elevation of the ground is. When we have these old growth forests that are very structurally complex, have lots of gaps in between their different branches and their different leaves, it allows for lots of space for those LIDAR pulses to get all the way down and interact with all the different layers in the ground. Ultimately, till it gets to the ground where it then sends all the remaining laser light back to the sensor. So you see a very dense canopy on the left here, and you see lots of canopy gaps on the right here. And you can see with this box in Whisker plot, which is right here and right here, this line, this line there to the top and bottom right here, that line, that just describes the variation in the elevation of the point clouds here. And you can see it's very, very small. All of the points in the point cloud, pretty much all of this kind of range of elevation, where as you can see the variation for the old growth forest is really, really large. We get lots of points coming at very high elevations, as well as moderate and low elevations, whereas with this very dense pole or sapling forest on the left here, all our points are very large. And you can see very dense, this very dense pole or sapling forest on the left here. All our points are just ranging kind of in that 20 to 30 meter height above the ground. Okay, this is just another example of some information that you could derive from LIDAR. So this is again just a cross section of a forest stand, but a little bit more zoomed out so you can see some other features in this area. So we can guess or we can see here that there's a lot of density at kind of this general information, or sorry, at this general elevation. So we can guess from that that this probably isn't a very old forest, but it's probably not as young as this forest because we are able to get some points that reach all the way to the ground. So we get LIDAR light coming down from our plane up here, zoom coming down, interacting with the top of this canopy, sending lots of returns back to the sensor. But then lots of that laser light is also able to get all the way down below the canopy. You can see kind of in this region there aren't many points. That's probably because there's not many branches or not many leaves at this general elevation. Most of the biomass in this forest is kind of all right around here. But then some of that laser light is able to get all the way down to the ground so we can get a sense of where the ground is below these trees. You can also make out the shape of these individual tree crowns. So you can see here if we look at this pattern here you can see a little kind of hop, and a hop, and a hop, and a hop, and a hop, and a hop, and those are all individual trees. So we can actually make out individual trees in the cross section of this point cloud. And then one other thing we can make out, you can see this kind of gap right here, and that looks like a road right there. So there's obviously no trees there, all the laser light, but go boom right down all the way to the ground and send a bunch of returns back from there. And so we can tell that there is a road right there. This is just another example of what that point cloud would look like. Again of a older forest on the right here, of a much younger forest on the left here, and we can get a sense of really nicely how this forest looks, what the elevation of the ground is in this area, and get some, you know, we can derive lots of different kinds of information from these point clouds. We're going to look at one example here of using LIDAR to understand the impacts of storms in different rain forests around the world. Pulses of laser light, 300,000 per second. Each one represented by a single leaf. Flying above a protected area of a Brazilian rain forest, NASA scientists measure changes in the canopy to understand how climate change affects the amount of carbon stored in the Amazon's mining trees. They flew the same transit through the forest three times over three years. First, comparing two fairly normal water years, 2013 and 2014. And then surveying again in 2016, after six years on the new drought. With trees more than 16 stories tall, three more measurements capture changes in forest structure, not possible from the ground to the forefront space. LIDAR is seen all the way here, representing limbs and home trees crashing into the ground as a result of storms and environmental scallops. As they fall, they take other trees with it. In collaboration with Brazilian scientists, the team also been up to ground service to measure the marine terrain on the forest floor. They found that 80% of the carbon losses came in front of the dead of a larger tree. Surprisingly, large trees were not heard comparatively more about the drought than in smaller trees, as hand-bent previously suspected. The team also surveyed areas of the forest impacted by the logging, or even more of a minute of changes can be seen. Researchers will continue to analyze how the changing climate and human activity affect rain forests, and how much carbon release forests will take up and release to the atmosphere. A couple examples there of specifically in a forestry context how the structural information of forests from LIDAR can be used to understand storms, understand forest harvest practices, things like that. This isn't an exhaustive list here, but some other examples of what LIDAR can be used for. Our habitat mapping, understanding the structure of habitat for birds, and other wildlife species, for resource management, community planning, environmental disasters, for predicting them, and for responding to them, for biomass mapping, which we'll talk about in our next lecture a bit more, and for forest growth models and for storm impacts as we watched in those videos there. This is one specific example of a wildlife application where there was a study area in Alberta, and they flew LIDAR over this whole area here, the areas that they've shown here, and they were able to derive information like forest height, forest canopy cover, and forest complexity, so how complex the structures of those forests were. They were able to then model and relate that to bird species richness and derive a map almost for the entire province of a predicted level of bird species richness. This is a really, really valuable data set for managers, for practitioners, for decision makers, because they can then go away and look at a map like this and say, okay, we should prioritize these areas for protection, for critical habitat, for conservation, because we can see that there is a higher level of bird species richness in this particular area, and in particular with LIDAR, its ability to predict and understand forest complexity and forest structure is really valuable for understanding bird species distribution. Birds, especially birds that live in forests, which are a lot of birds, birds that don't particularly reside in urban areas, exhibit really strong responses to different types of forests, to different types of heights in forest, canopy covers, complexity of forests, and so our ability to get that 3D structural information from LIDAR, something that we really can't derive from passive remote sensing imagery such as Landsat, such as modus, such as worldview, enables us to get that 3D information, which is really valuable for predicting something like bird species richness. Okay, last kind of active remote sensing that we'll talk about is sonar. Sonar stands for sound navigation and ranging, and essentially how sonar works is it transmits a signal, that signal propagates towards an object, so it transmits a sound signal, and then that signal is reflected off the target or object and is recorded by a receiver. Same kind of ranging system where we time how long it takes for that sound wave to travel towards the target, bounce off that target and travel back towards the sensor, and some sonar signals we can hear, because again it's just sound, it's just sound being propagated through an area, and others are at such high frequencies that we can't hear them, and they're completely silent to us. But very similar to some of the other active remote sensing systems we've talked about just with sound this time. Sound waves are propagated towards an object, they then hit that object, bounce back towards the receiver, we time how long it takes those sound waves to do so. Now I'm not going to talk about sonar in detail with its applications, because it's not as applicable to earth observation, and definitely not as applicable to earth observation from space, but some common uses are submarines use sonar to detect other vessels, to detect obstacles that may be in front of them. Fishing boats might use sonar to locate schools of fish, oceanographers might use sonar to map the contours of the ocean floor, so they essentially get a sense of the terrain and the topology of the ocean floor below the boat. Some animals might use biosonar or echolocation to see bats and dolphins actually send out sound waves and echolocate, which just means they time how long it takes that sound wave to travel to an object, bounce off that object and travel back to them, literally in order to see. But these are just some examples of some different applications of sonar that you might see all around. Okay, that is it for today. Got some practice questions here. I'll leave you guys for a couple minutes to practice answering them. I just checked that Leanna's introduction video is up now, just double-checked it on the assignment 4 page, so be sure to take a look at that before you start assignment 4. I'll give you three, four, five minutes or so to chat about these questions, then we'll talk about them. If you don't want to stay for that and you want to head out now, you're welcome to do so, and otherwise I'll see you tomorrow. I miss you a little definition. Wait, for GPS presentation, how are you interacting with Leidar? How it interacts with Leidar? So it is one of the technologies that is fundamentally a part of how Leidar works, and its role is that it measures the exact location in three-dimensional space of where the laser pulse is emitted from. Okay. Also, I slightly measure definition on the output definition of the... Of point cloud? I don't even quite know how features are for it. This is a point cloud right here. So all it is is a set of points in three-dimensional space. That's all it is. I kind of sketched this, but you know that return strength is different for both of these? Does that mean that this one is denser than that? Yes. That's why you kind of see the strongest level, the strongest return being this kind of really dense section of vegetation right there. Okay, awesome thing. But either way, when we take that data to create a point cloud, we essentially just set a threshold of return strength and then look at that peak in energy being reflected back to the sensor as just a point. If that makes sense. So we would just look at this, for example, and say, okay, above this threshold that we set is a point in three-D space. So we just say that's a point, that's a point, that's a point. This is just how we ultimately get to deriving the points from the return strength measurements. So anything above a certain is going to be a point? Exactly. Exactly. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay. Okay guys, let's go over these, the remaining stronghold of you guys. Appreciate you guys. So is lidar an all-weather form of active remote sensing technology? What do you guys think? Can it be used in all kinds of weather? No. No, cannot. Why? Exactly. Clothes, other kinds of poor weather will interact with the wavelength you're using. Essentially the key is you're not using microwave energy. You're not using energy that can penetrate clouds or other poor weather conditions. So it's not an all-weather remote sensing technology. Okay. What are the three technologies that lidar relies on? Yeah. Laser ranging, GPS measurements, and IMU measurements. Okay. What are the two types of information lidar can be used to derive? What are the two that we discussed in class? Just now. Yeah. Exactly. Terrain data and structural information of vegetation and what might be some applications of each of those. Maybe just give me one example. Anyone of an application of either of those kinds of information that you can derive from lidar. We watched the videos and then we kind of talked about some other ones briefly. Yeah. Yeah. Yeah. Exactly. Forest density is a really good example. Understanding how dense forests are, understanding the impacts of logging, what the shapes of the forest are prior to and post logging. For sure. And then what is one application that we briefly mentioned in relation to sonar? One thing that sonar can do. Yeah. Yeah. Some reinds will use it to detect other vessels, other obstacles in the water totally. All right. Awesome. Thanks guys. I will see you tomorrow. Bye. Hi everyone, welcome. Happy Tuesday. I wanted to see if I could change these quick. I don't know where to change it. Okay, so today we are talking about imaging the biosphere and just the biosphere a little bit in general. So we're going to talk about what the biosphere is. We'll start by defining it. We'll talk about briefly why it's important to monitor it, some metrics we use to monitor the biosphere. Then we'll kind of dive into remote sensing technologies used to monitor the biosphere with some specific examples and applications. You'll notice that for the rest of the class, whoa. That's fun. Oh, wait. Oh, that was just me. That's the start of the video. Sorry. So, you'll notice that for the rest of the course pretty much, all of the lectures are going to follow a very, very, very similar format. So today we're talking about the biosphere and then we're going to talk about the cryosphere and then we're going to talk about oceans and then we're going to talk about wildlife and then we're going to talk about monitoring change. All of these lectures are going to kind of have a very, very similar flow, a very, very similar outline where we'll essentially define what it is we're talking about, the general topic. In this case, it's the biosphere. In coming weeks, it'll be the cryosphere. It'll be wildlife. Other things like that. We'll talk about why they're important to monitor all these kind of different environmental applications and examples. We'll talk about then for each of them the different remote sensing technologies we have available to monitor them and then how those remote sensing technologies actually do monitor them. And then we'll talk about the different things that are being monitored by those Earth observation remote sensing technologies. So all of the lectures from here on out for the most part kind of follow this very, very similar structure and you'll notice a reoccurring theme at the end of the lecture where we talk about the advantages of using remote sensing Earth observation technology to monitor whatever it is we're talking about. So in this case, in this lecture, it's the biosphere, but in all the future lectures, you'll kind of notice very similar reasons for why it's useful or advantageous to use remote sensing technologies. And I say that just to note that in an exam setting, you can expect a question to kind of be surrounded around why it might be advantageous to use remote sensing Earth observation technology for a specific purpose. And just make sure that you are kind of answering that question specific to the application that is being given. So for example, today we're talking about the biosphere. We're going to talk about why remote sensing technology is useful for the biosphere. But again, the reason that it's useful, the reason it's advantageous is very, very similar to all of the other applications we're going to talk about in the coming classes. So just make sure when you're answering that question on the exam, you're kind of giving some specifics towards the biosphere, the cryosphere, oceans, whatever it is we're asking you to talk about. Okay, I'm going to start today with a video. It's about five minutes long, kind of introducing the biosphere and Earth observation from the biosphere. It's a NASA video here. But as soon as we meet at the other limits of our atmosphere, one of the first things we did was try our cameras around and look at this. The first human satellite was launched in 1988, but the last years before Neil Armstrong created a long-looking sea satellite set to take our understanding of Earth to new heights. In 1997, NASA launched a satellite that began a 20-year continuous global record of the variant being bad, as far as we know, makes Earth special. Right. While most satellite missions capture data on the physical characteristics of our planet and climate and weather, others allow us to measure life itself. For the result, the most complete view of global biology to date. The greatness of this data set has come hard to explain if a law needs to understand, ocean, and et cetera in an organic way. That's the voice of oceanographer Dr. Ivo Vasintenik. Ivoa and the rest of the NASA Goddard- NASA satellite mission. You'll see what looks like a repetitious end and flow of the land and surface of the ocean. We're actually watching a planet breathe. About half of the total photosynthesis on the planet occurs on land and have to be changed. That is Dr. Compton Tucker, the primary satellite monitoring and vegetation on land. The spring and summer months keep up with growing season for plants on land, illustrated in dark green, and tiny microsparket plant-like organisms in the ocean called final light food, seen in the light blue. They take carbon dioxide out of the atmosphere and use it for energy causing the total amount of carbon in the air to drastically drop. The opposite is to create colder months. During winter and the northern hemisphere, which is home to most of Earth's landlands, the carbon in the atmosphere increases as plants go dormant. And then there are extreme zones in the ocean. The total patches are nearly devoid of any dried up mountain. There basically deserts at sea, while the red zones tell us that there's either a high concentration of final plankton hunting the coastline, or our sunlight sensors are picking up on another area for changing the color of the water. We have a marvelous biological diversity of plants and animals both on the land and also a nature tree. But hold on. If we have an amazing biology for diversity of plants and animals, why do scientists spend all their time observing plants? You know how they say you are a mature eet? In a same way, if we're not understanding the ocean, a lot of the ocean, we have to start from a different base. If it's changing, the whole ecosystem will change. The changes that you go to as talking about are much easier to see when we can set a continuous global record. And that means not only being able to look into the past, but also into the future. It's a long-term data set, allowing us not only to see exactly what's happening, but to be able to swim much better way to predict what's going to happen. A global perspective gives scientists the power to forecast events like harmful algal blooms, disease outbreaks, and even family. Maybe one of the most useful applications of the data is its ability to show us where we've been. In 20 years, the plant has changed in noticeable ways, and this data set gives us a visualization to prove it. Arctic green and comfortable with retreating Arctic sea ice are probably one of the most well-known examples of this. If you look at the higher normal latitudes, you see in the white where there is snow, and that then moves further north in the seeds. It's then followed by very, very green colors because plants are really photosynthesizing in those dark green periods. Scientists think that there are many trillions of planets, yet there is still the only planet we know of in life. And with that in mind, our habitable home world sees an emerald fragile and beautiful and considerate the masses of space. I have several friends and acquaintances who are astronauts. They all stay the same way when they're in orbit. On the space shuttle or in the international space station and look down there, they see one climate to one planet. We're all in this together and we need to work together to make sure the light of you know continues on this one year old planet. Okay, so the data set that they were talking about is this sea whiffs or sea wide field of view satellite. So all the imagery, the kind of greenness on terrestrial areas and the amount of photosynthesis due to phytoplankton occurring kind of in the oceans here. Those were from this sea whiffs or sea weaves satellite. It was launched in 97. It died in 2010. It's not a satellite that we talk about in depth throughout this class, but it did give a really, really cool daily temporal resolution at about a one kilometer spatial scale of not just the amount of chlorophyll in the oceans, which was kind of a measure of the photosynthesis, the amount of phytoplankton in the oceans, but also NDVI in terrestrial areas. And it was kind of the first full system that gave us a really, really nice look and nice breadth of the base of all of the biosphere, the base of all of the ocean ecosystems and all of the terrestrial and freshwater ecosystems. And that's ultimately what the biosphere is. The biosphere is just a collection of all the ecosystems on earth. So that includes both terrestrial land ecosystems and marine, ocean and freshwater and other ecosystems. So all the ecosystems on the planet earth are what's considered to be a part of the biosphere. Now photosynthesis is the process by which plants convert solar energy, carbon dioxide and water into sugar and oxygen. So we have an equation for that here. We have light energy, carbon dioxide and water, add all that together and your output is sugar and oxygen. And photosynthesis, this process, this equation right here is the fundamental basis for energy flows in all the ecosystems of the biosphere. So that means that this little equation here, this little phenomenon, photosynthesis, forms the basis, forms the platform that all other ecosystems function off, whether that's marine ecosystems or terrestrial ecosystems. Now the biosphere through photosynthesis absorbs carbon dioxide very, very strongly from the atmosphere. And it's crucial to monitor the biosphere so we can understand carbon cycles. Now the base kind of understanding or in its simplest form, carbon cycles look something like this. So carbon cycles include photosynthesis, so plants photosynthesizing with sunlight, with carbon dioxide and then ultimately creating organic matter, which they store in the form of biomass, whether that's a forest itself or grass. And then ultimately that grass, that forest is eaten by, you know, in this case we have sheeps but other forms of wildlife, other organisms, which eventually die and create waste products, but that also just feeds into the ground where lots of more organic matter is kept. Then ultimately plants also respire, animals also respire, we respire, we breathe, we exhale carbon dioxide. And then as well we have factories that create emissions, things like that, all different sources of emissions around the world emit some sort of carbon dioxide or some other, or some other emission, in this case I'm talking about carbon dioxide because we're talking about the carbon cycle, but cars, factories, whatever it might be, they are emitting carbon in the form of carbon dioxide back into the atmosphere. That carbon dioxide that's gone back into the atmosphere, it eventually gets cycled back through ecosystems, through photosynthesis, through other organisms, back into the ground ultimately and then again through respiration and through the burning of fossil fuels and other things we cycle it back out into the atmosphere. This is a repeated process that's going on all the time. And we know that this process and the balance of carbon in the form of dead organisms, waste products and fossil fuels, as well as in the form of forests and organic matter that's living, as well as carbon dioxide that's ultimately in the atmosphere, has a really, really large influence on our climate. We know that as carbon has been increasing its concentration in the atmosphere due to anthropogenic effects, so just due to human caused effects, we've seen an associated increase in the temperature of the planet. So to understand climate change ultimately, to understand these carbon cycles we need to be able to monitor the biosphere because the biosphere through this key equation, through photosynthesis, is ultimately what's going to take that carbon from the atmosphere and put it back into the earth in some sort of solid matter form, some sort of form that just takes it out of the atmosphere. So that's ultimately why it's important for us to monitor the biosphere, so that we can understand carbon cycles, so we can understand why there are certain levels of concentrations of carbon in our atmosphere to help better understand climate change, better help us plan for the future, for our own communities, as well as for our resources and for wildlife that exists. Okay, so a couple of metrics that we'll talk about that can be used to monitor the biosphere. One is productivity, which is essentially directly measuring the levels of photosynthesis that are occurring. We'll talk about gross primary productivity and net primary productivity, and then just what effects productivity. We'll talk about what a carbon source and a carbon sink is, we'll talk about briefly, I'll define what biomass is, and then after that we will talk about the different remote sensing technologies we can use to monitor and measure each of these metrics, and then ultimately why those are useful. Okay, so primary production can be broken into two terms, gross primary production and net primary production. Gross primary production is defined as the amount of energy created by plants. So ultimately that's just how much photosynthesis is occurring. Photosynthesis creates energy, how much energy is created by plants is what we term gross primary production. Net primary production is the amount of energy stored in the plant, typically as biomass, after accounting for plant respiration. So cells within the plant have to respite, they have to breathe essentially, and they use the oxygen that's created by photosynthesis to do that. And ultimately that emits a little bit of carbon dioxide itself. So if we look here at just kind of this illustration of that, we have solar energy helping create photosynthesis. This photosynthesis, the amount of photosynthesis occurring, the amount of energy created we call gross primary production. Then some of that energy is used for cellular work, which is the respiration, and then some of that energy is stored as biomass. So as physical matter, as leaves, as stems, as physical matter in the plants. So our net primary productivity is just the amount of gross primary productivity minus the amount of respiration occurring. So that tells us how much net primary productivity there is, can also be very well related to biomass. It's essentially the amount of solid matter created and stored as energy from the gross primary productivity that's occurring. Now productivity can be affected by a variety of factors. We'll talk about a couple of key climatic factors that affect primary productivity. And we'll talk about light, air temperature, precipitation, and carbon dioxide concentration. And all of these factors vary quite a lot all throughout the world. So as we're talking about them, think to yourself, rationalize why certain levels of productivity would be different around the world. Why do temperate rainforests in British Columbia have a pretty high level of productivity? While deserts and other areas that are very arid maybe have a much lower level of productivity. Okay, the first one that we'll talk about and the one that we'll talk about in most depth is temperature. So we can see that as temperature increases, the amount of gross photosynthesis continuously increases. At about 20 or so, 18 or so degrees, the amount of gross photosynthesis occurring kind of tapers off a little bit. So it doesn't kind of increase linearly the entire way. Once it reaches a certain temperature, it starts to taper off a bit, but it's still slowly increasing. Now the key here is that high temperatures will induce stress in plants. Stress causes plants to respire more. So as we see an increase in temperature, we see this exponential increase in plant respiration. Because of that, we see this characteristic hump shape of net photosynthesis. And this net photosynthesis again is just gross photosynthesis minus the respiration. So you can see here where gross photosynthesis is at its peak or at its highest or close to its highest. And respiration is kind of at its lowest or furthest away from gross photosynthesis. So where this line, this green line is the furthest away from this red line here, that's where you see a peak in net photosynthesis. That's where you see the most photosynthesis, the most net photosynthesis occurring. That's where there's the greatest level of gross photosynthesis in a ratio to the lowest level of respiration occurring. And this kind of makes sense when you think about ecosystems around the world. You think about temperate rainforests, you think about tropical rainforests, whatever it might be, they're often hovering around this temperature. They're often hovering right around that temperature kind of all the time, very, very frequently. And in those areas, we get the most amount of net photosynthesis. Consequently in those areas, we also see very high levels of biomass. There's lots of net photosynthesis occurring. If it's peaking kind of around this temperature, we'll also see a lot of biomass because there's a ton of energy to be stored as physical plant matter because gross photosynthesis is really high and respiration is really low right around that temperature. So if I ask you on an exam setting to be able to describe how gross photosynthesis, net photosynthesis and respiration change with respect to temperature, then you could say, well gross photosynthesis, it increases up to a certain temperature and then tapers off. Then, slowly increases at first, but then exponentially increases at about 20 degrees. And net photosynthesis slowly increases, peaks at about 18 or so degrees Celsius and then slowly continues to decrease after that. Okay, so that's the temperature. There's a bunch of other factors that also will influence productivity. So that will also influence how much photosynthesis is occurring in an area, in an ecosystem, in a certain part of the biosphere. One of them is light. So the more sunlight there is, the sunnier a region is, the more photosynthesis there's going to be. Up until saturation. That just means that once we hit a certain level of light, then any amount of increased light past that won't further increase the amount of photosynthesis occurring. So that's all that's meant by until saturation. And dioxide increases the amount of photosynthesis because it is kind of one of the key inputs for that photosynthesis equation. Again up until a level of saturation. You can see that on the graph on the right here. We have rate of photosynthesis on our y-axis. We have carbon dioxide on our x-axis. You can see we are at about 400, that's a bit outdated. We are at about, think of about 450, 460 parts per million in our atmosphere today, on average. You can see kind of right around there, that's pretty much the saturation level for most plants. Any amount of carbon dioxide past that is not going to further increase the level of photosynthesis occurring. And this depicts a form of saturation. So at this point, it's saturated. The level of carbon dioxide that can increase photosynthesis is saturated. If we increase any more carbon dioxide, it's not going to proportionally increase the rate of photosynthesis. So you'd see a similar kind of curve like this. If you were to look at the relationship between light and photosynthesis, as well as water and photosynthesis. So with water, it also increases photosynthesis as there's more water. Again up until a point of saturation. And again we already talked about temperature. It increases photosynthesis, but levels off at a higher temperature. And net photosynthesis will start to decrease at higher temperatures because plants will get stressed and require much more respiration. And again if you think about all these things and you tie them all together and then you think about different ecosystems, different parts of the biosphere around the earth, it kind of makes sense. Again tropical areas get a lot of light, they get a lot of water and they have that perfect temperature kind of hovering in this range right here. On the other hand desert areas, they might have a lot of light, a lot of sunlight, but they don't have enough water or the perfect temperature to kind of have that peak level of net photosynthesis. So depending on these different characteristics, we'll see different levels of productivity, different levels of photosynthesis, all across our different ecosystems, all across different parts of the biosphere. Okay, so that is one way that we can kind of get a measure for different parts of the biosphere, is by measuring productivity, by essentially measuring the amount of photosynthesis occurring. Another way that we can monitor the biosphere is by understanding carbon sinks and carbon sources. So carbon sink is something that absorbs more carbon from the atmosphere than it releases. A carbon source is something that releases more carbon to the atmosphere than it absorbs. And by measuring carbon sinks and sources, we can monitor carbon transfer and carbon storage in the biosphere. So for example, how much carbon is in the atmosphere versus how much carbon is stored in forests. If we monitor things like deforestation, things like when forests are burning, things like agriculture, things that change areas from forest to maybe residential or urban areas, we can monitor what our carbon sinks and what our carbon sources are, how many sinks are there, how many sources are there, and in that way we can monitor carbon exchange, and in that way, subsequently we can monitor the biosphere as a whole. So looking at a couple examples here, I'll give you guys maybe a couple minutes to discuss what the neighbors sitting close to you, it shouldn't take too long. But which of these examples up here would you define as a carbon sink, which would you define as a carbon source? So we have here forest that's recently been cut down, we have some medium to old forest up here, we have some forest that's been infected by an insect in this picture here, we have a forest that's on fire here, and we have a forest here that's been recently cut down but it's starting to regrow. Maybe a couple minutes, brainstorm with someone sitting next to you, or someone sitting close to you, and then we'll come back and try to answer them. I'll give you about two, three minutes. Alright, what do you guys think? Let's start with this one up on the top left here, would you consider this a carbon sink or a carbon source? Put your hand up, shout it out, yeah? Yeah, carbon source, that would make sense, there's been trees cut down. That decomposition is releasing carbon into the atmosphere, probably the carbon source. What about this one up in the top right here? Yeah? Yeah, carbon sink, yeah for sure, it's absorbing carbon. We got lots of healthy trees in there, they are acting as a carbon sink because they are photosynthesizing, they're taking carbon from the atmosphere, putting it into a solid organic matter form. Okay, what about this one here where we got the insect infestation going on? Yeah, carbon source, that was a tougher one, we do have some healthy trees in here, but we got lots of dead and dying trees as well, there would be a balance going on there, there would be some individual trees acting as a bit of a sink, some individual trees acting as a bit of a source, so it would be kind of a balance of the two. Probably in this example, there are more dead and dying trees than there are healthy ones, so we could say overall, it's probably close to a source. What about this one in the bottom right here, the wildfire? Carbon sink or carbon source? Mm-hmm. Source? Yeah, exactly, yeah, it's burning off the trees, it's burning, it's taking solid organic matter, releasing it as carbon into the atmosphere, definitely a carbon source, and then what about this one right here? We got some area that was cut down, and then there's a little bit of kind of regrowing bushes and stuff, carbon sink or carbon source. Yeah. Yeah. Yeah, for sure, yeah, it's probably net, it's probably a little bit more of a source than a sink because we don't have a ton of vegetation growing here yet, as this kind of forest starts to grow back in as there's a lot more vegetation that comes through, it probably result in being more of a sink. The point of this, this exercise is to get you thinking about dynamic sinks and sources. So forests are a really good example of a dynamic carbon sink or source where forests can act both as a carbon sink and a carbon source, depending on what's going on there. If we have a nice, healthy standing forest, it's probably acting as a carbon sink. If we have a forest that's been recently harvested, it's probably acting as a carbon source, but if we get some regrowth going on in that area, if it's regrowing, if it's being replanted, there's vegetation and trees growing back in that area that was cut down, then maybe once again it would eventually be a carbon sink. So in an exam setting, I'd be really specific. I'd tell you if it was a forest, for example, what was going on with that forest, whether there was a fire going on, whether there was an insect infestation going on, and I'd want you to be able to tell me whether it's a source or sink or kind of something in between where there's a little bit of both going on. Okay, so that's carbon sources and sinks. That's one way that we can monitor the biosphere. So we've talked about two now. We've talked about productivity. We've talked about monitoring carbon sources and sinks. One other way that we can measure and monitor the biosphere is by just measuring biomass. So biomass is the dry weight of living organic matter in an area. It can include above or below ground or both. So you can say, okay, I want to measure above ground biomass in this area. That would essentially be every piece of biomass, every piece of living organic matter that is above the surface of the ground. So in this case, above ground biomass of this tree would be everything from here up. The below ground biomass of this tree would be all its roots, and then both above ground and below ground biomass of this tree would be both the roots and the tree truck and the leaves and everything that is involved in this tree altogether. The dry weight of all of that matter would be what we call the biomass or what we define as biomass. And it's a really good reflection of the productivity of an ecosystem because increased biomass, high biomass areas generally are associated with areas that have very high net primary productivity. Areas that are kind of that perfect combination of all the different factors that produce high levels of productivity and allow large levels of biomass to be stored as solid organic matter in the biosphere, in the trees, in the vegetation, whatever it might be. Okay, so three ways that we can measure the biosphere with productivity, with carbon sinks and sources, with biomass. Now monitoring the biosphere historically before we kind of had remote sensing, earth observation data available to us was mostly done with field based estimates. So we would go out into the field, we would measure how wide are trees, what's the canopy cover of a tree or what's the height of that tree to what this guy is doing. So he's using something called a vertex where you can look at the base of the tree and then look to the top of the tree and get a sense for how tall it is. So you'd measure how wide trees are, how tall trees are, how much root biomass is there below ground for that tree. Maybe if there's not a big tree there, you just put a little kind of plot like this down and cut away all the grasses, all the vegetation that's above the ground, dry it out, weigh it, that would give you a sense of the biomass for that particular plot, for that particular little area. You can imagine though that this is super, super time consuming. This does not give us an estimate or a measurement of our biosphere or an ability to monitor our biosphere at a large or at a large kind of area or small scale. It doesn't allow us to zoom way out and say for an entire forest, for an entire country, this is what the biosphere is kind of doing, this is what's going on. These are point based estimates that take a very, very long time to go out and measure, require a lot of costs, a lot of energy because you have to go out and physically do these measurements yourself. So eventually, one technology that was developed, this isn't a remote sensing technology, but one technology that was then developed was the Eddy Flux Towers. So these Eddy Flux Towers are just towers, they look something like this, and they have some sensors on board that measure carbon flux at a single point. So they monitor gas concentrations at very, very high frequencies, well above the height of vegetation, and by analyzing and processing that information and that data, you're able to get from Eddy Flux Towers estimates on the exchange of carbon dioxide from the atmosphere to vegetation. Now again, these are point based estimates. So this example of an Eddy Flux Tower right here, you would get a sense of the carbon exchange going on kind of right at this point or right in this particular area. Again, we can't be setting up Eddy Flux Towers every couple of hundred meters all across the world. So it's a very small area that this is covering, it gives us just a point based estimate. Again, you have to take the money and the time to go out and set these towers up. So still not a great way to be able to monitor regional, national, or even global biospheric metrics. So we'll talk about a couple of remote sensing earth observation technologies that are really useful for monitoring the biosphere and give some examples of what they can measure. This isn't an exhaustive list, but these are the kind of things, these are the topics in terms of monitoring the biosphere that I would examine you on for the final exam. So first we'll talk about MODIS. We've talked about MODIS a bit already in this class. We'll talk about MODIS and how it can measure productivity, how we can take information from MODIS to build models using the known factors that affect productivity to be able to estimate gross primary productivity and net primary productivity and environmental respiration all across ecosystems. We'll talk about LIDAR. LIDAR we talked about yesterday, how you can get 3D or three dimensional data sets from LIDAR. So we'll talk about how you can use LIDAR to estimate above ground biomass. And then lastly, we'll talk about Landsat, which we'll talk about in the context of monitoring carbon sinks and carbon sources to be able to detect disturbances and understand how forests are changing when they're losing carbon, when they're taking on carbon and creating more of a sink. Okay, so first is MODIS. This is a map derived from MODIS's global annual net primary productivity product. And this is over the period of 2000 to 2015. And you can see here again what we've been talking about. You can see these high areas of net primary productivity, tropical areas, areas down here, high areas here, areas that have that nice, perfect level of temperature, a good amount of light, a good amount of water, those factors that contribute to high levels of net primary productivity. MODIS is really, really good because it has that large pixel size, that low spatial resolution and that high temporal resolution of daily imagery. It's really good for getting global estimates of things like productivity. So if I'm asking you what you might want to use to measure global productivity across the biosphere, you'd want to say MODIS. Now MODIS ultimately goes away and takes spectral information. So if you remember back to our resolutions lecture, MODIS has 36 bands onboard it. So there's a ton of spectral information associated with MODIS data. And we know that productivity is affected by light, by temperature, by carbon dioxide and by, there's one other thing that I forget off the top of my head. But it's affected by those four things that we talked about earlier today. And what we can do is we can take all that spectral information that we can get with MODIS, use that to get an estimate of how much light is in an area. What's the temperature of an area using the thermal infrared band? We can use other bands to get a sense of moisture content, to get a sense of how much water there is in areas. So we can get a sense of how much water there would be for net primary productivity. We can also get a sense of how much carbon is in the atmosphere, all these kinds of things. We can input that into a model, get an estimate from MODIS for all across the world at a 150,000 to 1,000 kilometer spatial resolution of what net primary productivity might look like. This is just another example of that, just breaking it down into the couple of metrics that we talked about, gross primary productivity, net ecosystem productivity or net primary productivity, and then environmental respiration here. So again, this value here, this map here, is just taking the values here, subtracting the values here, and then getting the difference of those values for this net ecosystem or net primary productivity map on the right here. Okay. So that's MODIS. That's MODIS being able to monitor very small scale or large areas of net primary productivity or gross primary productivity or just productivity as a whole. Another example that we can use and that I've kind of already introduced is LIDAR and LIDAR's ability to collect three-dimensional information. So we know with LIDAR from talking about it yesterday, we can get really efficient three-dimensional data collection. And that allows us, in the case of LIDAR, to get very high accuracy above ground biomass estimates. So this map that you're looking at here is a study area from California where they were able to detect with LIDAR individual trees. So each one of the dots in this map is an individual tree that they were able to detect with LIDAR data. And then from the structural information of each of those individual trees that they got from the LIDAR data, they were able to estimate how much biomass, which they've termed here above ground biomass, there is for each tree. So each tree, which is denoted by each of these little circles, that is very green, are very big trees, trees that have large amounts of above ground biomass associated with them. Small trees, which are denoted by red here or orange, have very low amounts of above ground biomass associated with them. So this ability with something like LIDAR at a very, very, very, very small scale, or a very, very large scale, very small area for each individual tree, the ability to estimate the biomass associated with each of those trees is invaluable. It's something that we didn't think that we'd be able to collect and monitor nearly as efficiently as we're able to do today with something like LIDAR. If you can think back to what I mentioned before, which is that historically, to monitor the biosphere, we had to go out and measure each one of these trees individually. We had to go out and measure with a measuring tape, how wide it is, how tall it is, and from that derive a biomass estimate. We can now just fly a plane with a LIDAR instrument over a pretty large area like this and get a biomass estimate for each individual tree. So a data set that is really, really come a long way and really valuable to us today. Okay, last example we will talk about is LANDSAT. So LANDSAT has a couple of significant advantages to it over something like MODIS or LIDAR. They all are useful for their own things. In the case of LANDSAT, we have a really large time dimension, which just means that LIDAR is one of our oldest satellite programs that we have data available for all the way back to the 70s and 80s. So we can monitor change in the biosphere all the way back to 70s and 80s with the LANDSAT data set. Has a moderate spatial resolution, that 30 meter spatial resolution, which actually makes it really ideal for monitoring changes in forests. A 250 meter spatial resolution that you get with MODIS isn't nearly as useful as a 30 meter spatial resolution for monitoring and detecting things like forest harvest, for monitoring and detecting things like wildfires, etc. LANDSAT is really, really good at that. It still has a pretty moderate temporal resolution, so we can at minimum get really nice composites for every single year of an area at a 30 meter spatial resolution. And that's what you see up on the right here. This is a composite of Canada using a best available pixel method, which just means each individual pixel in this composite is being pulled from a set of potential images all throughout that given year. And the pixel it's pulling is just representing the best data set for the best pixel available for that area. So the one that has the least amount of noise, the least amount of clouds in it, etc. And here we're getting a nice clean composite, a nice clean image of all of Canada at a 30 meter spatial resolution for each year. In this example, I think it's all the way back to 1984 from LANDSAT. That allows us then to compare year by year these images and look at something like NDVI, look at the spectral information in each of these pixels year by year by year and detect and quantify how much forest harvest might be going on, how many wildfires there might be, where landslides are, how much areas getting converted into agriculture, etc. So this is just an example of an output from that kind of analysis where we looked from 1984 all the way to 2011 in this case. And for each 30 meter pixel, we classified whether or not a change had occurred. So anywhere that there's gray on this map, no change had occurred. But then all of these colors here represent a year. So all the colors on the map here represent a year. And the year that it's representing is when the greatest amount of change occurred for that specific pixel, which is when essentially there was a large disturbance in that area. And it could have been a wildfire, it could have been forest harvest, it could have been a change of forest to agriculture. Some sort of change occurred in these pixels if they're colored for that specific year. We zoom in, we can look at a couple specific examples of some different changes and some different disturbances that we know to exist across these provinces. So this is looking at New Brunswick here first, and you can see here, these are the kind of raw composite images. And you can see the landscape changing in this case due to agriculture. So you can see lots of farms and other things like that popping up. And you can see here on the right, this is just a data set from here showing us when the largest change in this area, in this case in New Brunswick occurred. So what year was the largest change occurring for each individual one of these pixels? So this is an example looking at agriculture. In Manitoba, they get a lot of wildfires. So this is an example looking at wildfires. You can see the fire scars popping up here as we look through time. And then you can see associated with that, this map on the right here that shows you essentially when that fire occurred, what year that fire occurred. Again, at a 30 meter spatial resolution, we can get a data set like this for every single year, in this case all the way back to 1984. Okay, so this is fires in Manitoba. One other example, this is forest harvesting, so forestry. So the creation of clear cuts or cup blocks in Alberta. So you can see kind of the checkerboard shape looking cup blocks or clear cuts occurring on the landscape. And then again, on the right here, we can just see in what year that forest was cut down. And then that clear cut was created. Again at a 30 meter spatial resolution, we can check that for every single year all the way back to 1984. Super super super valuable data set for us to understand carbon sinks, carbon sources, the dynamics of our forest. And something that we can collect freely. This is Landsat data. We're able to download this data for free. Anyone can download it. It's a really really valuable data set that's really accessible to everyone. Okay, finally I'll just mention, you know, we've talked about MODIS, we've talked about Landsat, we've talked about LIDAR. There's always the possibility to combine and fuse the information you can get from these different data sets as well. So in this case, using that same data set, this is a map in the top left here, just using Landsat showing us where there was wildfires, where there was forest harvest, where there was some sort of non-stand replacing disturbance occurring. That's just this blue right up here. And then for that area, this whole study area, so pretty much all of the forests in Canada, we combined the Landsat data with LIDAR data, so with 3D structural information and created estimates across all of Canada for all the forests at that same 30 meter spatial resolution for above ground biomass. So this is an example of above ground biomass in 1984, and then above ground biomass in that same area or in those same areas in 2016, and then just the variation or change in above ground biomass during that time shown up in the top right here. So again, a super super invaluable data set, 30 meter spatial resolution, we can do this for every single year, dating back to 1984. Okay, just to sum up here, we talked about a couple of, we talked about historically how we monitored the biosphere, we then talked about the remote sensing, earth observation methods we use today to monitor the biosphere. The historical methods we talked about were field methods going out and collecting field data as well as carbon flux towers. And just to kind of summarize the differences and key points about each of these different types of ways we can monitor the biosphere. And field data and carbon flux towers, we get point based data collection. So we got really limited spatial coverage. We're not able to look at an entire region, an entire country, an entire province. We only get data for that specific area where we either set up a tower or where we are going out and physically measuring the vegetation and biosphere that's there. There's a lot of operational costs associated with that. And the case of Eddie flux towers, they're not cheap to install, so you have to get, you have to go out and contract someone to build that, to put it in, to then be able to monitor it. In the case of field work, you have to pay individuals to go out and do that field work. And it's super inefficient, again, because you're out there physically monitoring and measuring each individual piece of vegetation or tree that you see. And there's limited collection for those two data sets, Eddie flux towers and field data. There's limited collection of standardized data. This is a really, really common problem with field data, with historical data, with essentially a lot of non-Earth observation remote sensing data sets, is that they're often not standardized. If I go out into the field and I want to measure vegetation, I want to measure the biosphere in a particular area, and I'm going to conduct some field work to do so, I might go out there and say, okay, well, it makes sense that I measure how wide these trees are, how tall these trees are, and, you know, how many trees there are. Okay, sweet. Someone else might then go to the same area and say, okay, we want to go out, do some field work to monitor and measure the biosphere in this area. But we're going to actually measure how many roots there are below the ground for each tree. And we're going to measure the leaf area index, how wide of a surface area or large of a surface area there is for all the collective leaves in this forest. And that's going to be how we monitor or measure how much biomass there is in this area. The point is that both of those, in a way, are measuring or monitoring the biosphere, but both in completely different ways. So that's not an example of a standardized data set. You can't compare those data sets through time because they're very, very different. You can't compare those data sets in space because they're very, very different. That's not the case for Earth observation remote sensing data. With Earth observation remote sensing data, we always get a standardized type of data collection. Landsat data is Landsat data. It's always collected the exact same way. Motus data is modus data. It's always collected the exact same way. It adds a lot more objectivity to the data and to the analysis. It's not subject to one person's perception of what they should measure or how they should measure something. It's just very, very objective data. So it allows us to have a very nice standardized data set through a large amount of time and across a vast spatial scale. Now, not only that, it also allows us with Earth observation remote sensing data in the case of monitoring the biosphere, allows us to have very efficient data collection. We get, for example, in the case of modus, 250 meter spatial resolution imagery every single day of the entire Earth. And we being, you know, me, for example, if I want to use that data, I don't actually have to go out and do anything to collect that data. All I got to do is log into the website that I downloaded that data from, pick the areas I want to download it for, boom, download. It's all my computer. So really, really efficient data collection and really, really efficient movement of data collection all the way to me, the user, ultimately being able to analyze that data. With Earth observation remote sensing data, we also get large spatial coverage and high spatial resolution. So we get these images, say, for example, in the case of Landsat at that 30 meter spatial scale for all of Canada, for all of our forests, all of the biosphere. And we get it at a pretty decent spatial resolution of 30 meters. Now note that, you know, our ability to get large spatial coverage and high spatial resolution data depends fully on the data set. We might not get very large spatial coverage when we're flying, say, a drone-based LiDAR system or even an aeroplane-based LiDAR system for that matter. We're going to have to just fly it over the specific area of interest. We'll get decent spatial coverage considering how high resolution that data is. But the level of spatial coverage and the level of spatial resolution we know directly relates to the instrument or the data set that you're actually using. So you know with MODIS, sure, you get that really good global coverage, but you only get it at maybe a 250 to 1,000 meter spatial resolution. Again in the case of Landsat and MODIS, if I, the user, want to download it, it's completely free. I don't have to pay someone to go out and collect that data. I don't have to go out and physically collect it. It's just free. It's online. It's that standardized data set, which we've already talked about. And it has a really nice, large temporal dimension. In the case of Landsat going back all the way to the 80s and 70s, in the case of MODIS to the late 90s and early 2000s, and then with kind of LiDAR data, it varies depending on when an area was flown. But generally we do get large collections of data. Now I put this slide up here to kind of end off because we've really only kind of scratched the surface of the different types of spectral information that we can ultimately use to monitor the biosphere. So this illustration up here kind of alludes to a lot of the different things that we can measure and monitor in the biosphere and all of the different spectral bands that we can use to do that. So you can see here everything from the visible to the near infrared to the shore wave infrared to the thermal infrared to the microwave part of the spectrum and a couple of the biosphere metrics that we can derive from these different data sets. Okay, that is essentially it for me lecturing today. As usual, going to give you guys a couple minutes to practice these questions. Evan's here too. He's going to give you some reminders of what you should be working on. And then if you have any questions for Evan, of course logistics, blog posts, you can come down and chat to him. So I'll give you a couple minutes to go over these questions, try and develop an answer for them. We'll crowdsource them. Evan will give his little spiel and then I'll go over the answers and then you guys can get out of here. If you want to head out right now and not go over these, you are welcome to. Please do so swiftly as usual. If you're doing that, I'll see you next week and if not, then you guys are welcome to stay. Oh, she goes. You're slides. No clue. I got them. Yeah, sure. So essentially for the interaction, the first of the things is actually decreased with net photosynthesis, right? But it decreased more slowly in a lot of ways. Also net photosynthesis is just the product or the difference between gross photosynthesis and respiration. So really you should think of this curve kind of being an outcome of this curve and this curve. So how would growth photosynthesis fit into this interaction? So gross photosynthesis increases up to about 20 degrees or so. And then it slowly tapers off. It does continue to increase for the most part, but kind of slowly decreases the level at which it increases. So this is kind of more of a logarithmic curve and this is more of like an exponential curve on the bottom. And then the net photosynthesis here is just the difference between this and this. So you can see when these exactly kind of how you've drawn here as these are where these two lines are the widest apart, that's where your net photosynthesis is the highest. I think you're talking as much. Yeah, the problem. Green name. Green, yeah. You just caught that when you caught your card with things. I knew it. Of course, but America. Really? It was cheap. When were you there? Florida. Oh, yeah. Mom took me out with shelving. I hope you know this is being recorded. Sorry? I hope you know this conversation is being recorded. Hell yeah. Well, I'm not going through and editing the audio, so. Of course not. Did I put 07 on this? There's 06 history. I changed it. Mom and me. Because it's 07 today. This was wrong though. Ow. It'll survive. You never find errors in your slides? I mean, like a little bit here and there, but I'm making like 30 to 40 slides a lecture until you're making one to two. Oh, man. Talk tonight? Yeah. Right now? Yeah. Talk tonight and the. I'm ripping Costco, so hot dogs. Nice. Let's go stuff. You're in downtown? Yeah. Friends driving. Yes. Who left the bale bomb in and the bale bomb me, which nothing I can do about that. So? What? What bale do you? Like if they're just not going to. Oh, okay. They just don't go. Okay. They'd be able to do. Get this. They're walking shadow first too. It's not like pretty clutch friends. Pretty clutch friends. Yeah. Okay. All right. Watches that I have to pay for wait for. It's yours for everyone. I can go sit and answer if you want. You want to. I'm going to speed things up. Okay. Let's try to answer these guys, everyone. So what is the biosphere? Someone defined for me. What is the biosphere? Yeah. All ecosystems on earth. Terrestrial and marine. All ecosystems on the earth. Great. So can someone describe to me the pattern of gross photosynthesis and net photosynthesis as temperature increases? How does that kind of graph look? Yeah. Yeah. Exactly. So they're both going to increase. They're both going to decrease at about 20 degrees Celsius. Net photosynthesis is going to really drop off steeply and continue to decrease. And then gross photosynthesis is kind of going to taper off. It'll still stay pretty high, but it's not going to drop right off, but it will kind of level off and be kind of steady as temperature increases. Exactly. Okay. We already got went over these examples. You can go over them again quickly, which of these are carbon sinks and sources? Maybe just shout out to me a sink or a source. So growing forest is a sink or a source. Sink, wildfire sink or source? Source insect outbreak in a forest sink or source? Source. Depending on how intense the outbreak is. And then how has the biosphere been monitored historically? Two examples that we talked about. Yeah. Yeah. Field-based estimates and then any flux towers. Yeah. Field-based estimates and eddy flux towers. And then lastly, describe the use of Landsat to monitor the biosphere. So what can it track and how is that related to carbon? So what did we, kind of the last example we talked about in terms of how Landsat's used to monitor the biosphere? What were we actually tracking? What was I showing the ability of Landsat to be able to track? Yeah. Carbon losses and gains. That's ultimately what it was being related to. That's really good answer. The more correct answer is we were looking at disturbances. We were looking at disturbances in forests in the biosphere and then using those to kind of quantify how much carbon sinks there were, how many carbon sources there were all across ecosystems in Canada. And the example we were looking at, we were looking at all of the forests in Canada and monitoring how many disturbances there are, how large they are, when they occur. So looking at forest harvesting, looking at wildfires. When these disturbances occur, where those disturbances occur and then ultimately we can relate that to things like carbon dynamics. Things like how much carbon source are these forests? How much of a carbon sink are these forests? Okay. And then lastly, kind of a broad question. Why is that advantageous? What is advantageous about Landsat specifically for monitoring the biosphere? Looking at the case of specifically looking at disturbances, detecting and monitoring and looking at disturbances, looking at wildfires, looking at forest harvesting, looking at land conversion to agriculture. Why is Landsat really good for doing that kind of thing? Yeah? Yeah, exactly. So we have a 30 meter spatial resolution. That's probably the most important factor in this case. We have a nice moderate spatial resolution. We can't detect, or it's really hard at least to map the extent of wildfires, of clear cuts, of cup blocks with something like MODIS. Landsat at that 30 meter spatial resolution is really good for that. It also has that really large temporal dimension. Dates back all the way to the 70s and 80s, all the way up to the present time so we can track lots of changes that have occurred through time. Those are the big ones. Generally, you could apply some of the other examples of why Earth observation remote sensing data is useful for monitoring the biosphere as a whole to help answer that question. So things like, oh, not very more. Things like the fact that with Landsat we got really efficient data collection. With Landsat we get a set of standardized data. We also get large spatial coverage. We're able to cover the entire surface of the Earth with Landsat every 16 days. So you can apply those to help you answer that question. But the most important factor there was exactly what you said, which is that it is a 30 meter spatial resolution. It makes it super useful for monitoring disturbances of forests in particular. Okay. Yep. Yeah. Yeah. So a disturbance in the context that we've talked about there is essentially when there's a certain level or certain magnitude of change that's associated with an ecosystem in the case that we were talking about. We were looking at a forest. So a disturbance can be either standard placing or non-standard placing. We mostly talked about standard placing disturbances where in the case of say forestry or a wildfire all of the forest is burned down or all of the forest is cut away. None of the trees are left there. So you've gone from in a specific area a forest to essentially no forest in a very brief time for a discrete event. That discrete event being the forest being cut down or the wildfire occurring. So that's ultimately what a disturbance is. It's a discrete event that produces a particular magnitude of change on a forest. In the context that we talked about it today, it can be applied in many different scenarios with slightly different definitions. But for this course and for how we'll be talking about it in the case of Earth observation data and the case of Landsat data in particular, that's how you can. That's how you can think about it. Does that make sense? Okay. Yep. All right. Thanks guys. See you next week. Oh, sorry. Give Bud your ear for a quick second. He's got a real short thing. Yeah. So what you should be working on. Blog post four. It's about finding a sensor. It's due March 9th. That's this Thursday. Sign up for with Leanna. Do next Thursday. She's got office hours. As follows. Today at 10 to 11 a.m. That one's gone. Thursday 2 to 3 p.m. for the next two weeks and the next Tuesday from 10 to 11 a.m. If you have questions about the blog post or the midterm, come ask me. Remember to check over your midterms and send me your questions, comments, concerns before next week. That's it for me. Awesome. Thanks guys. See you next week. Thanks. All right, everyone. Welcome. Happy Monday. We are getting towards the end of the semester. So that's exciting. Congratulations. Today, we're going to talk about the cryosphere. So we're essentially going to talk about ice and snow for the whole lecture today. I have Leanna coming at the end of class to talk about the assignment that's due this week. So if you have any questions for her, you're welcome to ask her when she's here. And I think she'll also just provide some tips and tricks and some suggestions to work on the assignment that you have that's due this week. So assignment four, that's the one that Leanna's going to come and talk about. It's due Thursday. And then for the next two assignments, I believe, assignment five and assignment six, you only have one week for each of those. So assignment five is due March 23rd, assignment six is due the following week after that. And then you only have one assignment after that. That's assignment seven. All of most of Leanna's office hours, except the Tuesday ones, those haven't happened. So pretend there's no cross there. So Leanna's got some office hours tomorrow and on Thursday, the day the assignment is due as well. So if you are not in class or you need to talk to her more outside of class or you just want an extended period of time to be able to talk to her about the assignment, you can attend those office hours. So that should be good. And so today we are going to talk about the cryosphere. We're going to talk about what the cryosphere is, why it's important to monitor the cryosphere, how we've historically monitored the cryosphere, couple of remote sensing technologies used to monitor the cryosphere. We'll talk about Landsat and MODIS, radar and ISAT, kind of the key technologies that we really focus on throughout this course. And then we'll touch on some specific examples and applications of being able to use those different data sets for monitoring different features of the cryosphere. So first off, what's the cryosphere? So the cryosphere are portions of the earth's surface characterized by frozen water. So that includes anywhere where there is an area covered with snow, with ice or with permafrost. Permafrost is just ground that is permanently frozen. So some solid ground that's just completely frozen throughout the year, so it's not necessarily ice because ice is water that is solid frozen. And ice can be broken down into ice sheets, ice shelves, glaciers and sea ice. Ice sheets are these massive glaciers that are on top of land parcels. So you can see ice sheets are mostly what covers Antarctica here and most of what covers Greenland up here. And then we have ice shelves which are above water, so they're above sea, above oceans, but they're attached to ice sheets. So you can see a couple of ice shelves here that are attached to these Antarctica ice sheets in brown there. And then we have glaciers, which are essentially just smaller versions of ice sheets. Glaciers are anything that's less than 50,000 kilometers squared in size, whereas ice sheets are anything larger than 50,000 kilometers squared in size. And then sea ice is just any free floating ice that is above sea or above oceans floating on the top of them. So the difference just between sea ice and ice shelves are that ice shelves are connected to an ice sheet. So they're connected to some larger parcel of ice whereas sea ice is just free floating in the water. So we have all these different kind of features of the cryosphere, and all of them we can monitor in different ways using Earth observation data. But why monitor the cryosphere? Why is it important for us to monitor snow, to monitor ice, to monitor glaciers, to monitor ice sheets, ice shelves, whatever it might be? There's a wide variety of reasons that it is very important and useful to monitor the cryosphere. One is that it is a really good tell for us of climate variability and change, which is really important for community impacts as well as biodiversity impacts. So we know that there's a lot of ice, for example, that's melting at northern and very southern latitudes. This has a lot of impacts for rural northern communities. Areas that maybe were once covered by permafrost, once covered by lots of ice are no longer, are covered by more water, areas are melting a lot more. So that has a lot of impacts on communities, abilities to obtain resources, whether that's sustenance, hunting, food, whatever it might be, a lot of community impacts there, so impacts on people, on humans. There's also biodiversity impacts, so there's impacts on species that we know that live in these northern latitudes, famous ones that you might be familiar with, like polar bears, kind of the example of the charismatic megafauna, so just animals that are, you know, in the case of polar bears, really cute and well known to the public for the most part, but also lots of other animals that live in these northern communities or these northern areas that rely on ice, that rely on snow, that rely on that seasonal variation of snow and ice cover to perform and live in whatever habitat niche they might live in. It also is a really useful way for us to monitor weather and predict climate, so it's really important to monitor patterns of the cryosphere, patterns of ice and snow, so that we can predict things like storms, so that we can go away and plan for communities and things like that, and also that we can understand transportation corridors. It's really easy to access communities that are really far north when there's a lot of areas that are frozen, as opposed to areas that have water in them. It's really easy to take a snowmobile or a big truck and drive it across a big lake or portion of the sea further north than it is to bring a boat throughout that area. So there's a lot of transportation impacts that have to do with the availability and cover of ice and snow, especially in those northern areas. And probably the most important one or the one that's most impactful to us in general, kind of down at more of a mid-latitude where we live here is that a lot of the cryosphere, most of the cryosphere, houses a very, very significant, very large portion of fresh water storage. So 99% of all fresh water storage is stored in just ice sheets. So just those very, very large pieces of ice, essentially, and snow, has the vast majority of fresh water storage, which is really important for us because we need fresh water in order to survive. There's also a lot of then subsequent and indirect impacts on ecosystems and on hydrologies. So there's a lot of flooding impacts. There's a lot of impacts on the moisture content of forests that these glaciers and ice sheets are maybe melting into. There's lots of impacts on ecosystems, including plant species and wildlife species, due to areas maybe being covered by snow, maybe not being covered by snow, maybe being covered by ice, maybe not being covered by ice. So a lot of different reasons why it's important to monitor the cryosphere. This is just a couple of examples here, not an exhaustive list. But remote sensing makes it really easy to monitor the cryosphere nowadays. We get really good global coverage, lots of different kinds of data sets and information that we can get with Earth observation remote sensing data in order to go away and monitor things that ultimately may impact communities of people, may impact biodiversity very strongly, may impact our transportation corridors and our water resource management. Now, historically, we generally monitor the cryosphere using systematic annual measurements. And we have these from about the 1940s or 1950s, and they can provide a very powerful time series of changes in the cryosphere. So we often monitor the cryosphere historically by looking at things like glacial mass balance, glacial extent, and sea ice thickness. But historically, this could have been from a couple of different data sources, a couple of different methods, one being going out in the field and people just on their feet, on their hands and knees, taking physical measurements with literal measuring tapes across large glaciers, whatever it might be. Obviously, something that is not very efficient, requires a lot of time and requires a lot of effort. But you could do that if you wanted to measure things like glacial extent. If you wanted to measure sea ice thickness, maybe you would just be drilling essentially a big hole all the way into the ice if you're out there in the field. So this is just an example of some historical data sets, a bunch of different data sources here, including aerial photography. And then here, one of the kind of more historical examples of a data set we have of glaciers all over the world and the number of glaciers, the extent of those glaciers, where they're located. So this still provides a really, really valuable data source because we only have Earth observation remote sensing data for most data types, dating back to, at least in the case of satellites, the 70s, that's kind of the oldest that we have satellite data available for because the Landsat mission was launched first in the 70s. So prior to the 70s, 40s, 50s, and 60s, we don't really have much, at least much satellite-based, remote sensing, Earth observation information. So these data sets, these historical data sets are still really, really valuable because we have to compare something. We have to have some sort of baseline to be able to compare our satellite data to. Now, I'm gonna talk about three different ways that historically we went away and monitored the cryosphere before satellite-based remote sensing, Earth observation, really became more prominent. One was aerial photography. So something we've talked about a little bit, but being able to go away in a plane and just take either oblique or vertical images of areas of interest. This was a little bit trickier back in the day because when we're going away and taking these aerial images, you might get a bunch of images like this, but they're all potentially gonna be from slightly different angles. We didn't have really accurate GPS measurements to be able to track where we're taking these images from and what angle we're looking at. So we'd go away and maybe get a bunch of images from one particular mission or one particular outing, and then we could print them out all on film, and then we could put them up on a wall and kind of make a mosaic by overlaying them all together, but not a particularly efficient or particularly standardized way of us to be able to go away and monitor ice sheets and glaciers and things of that sort. So one other version or kind of derivative of that that we do have is repeat terrestrial photography. So especially back when we didn't have GPS available to us, when we didn't have something to be able to monitor and track where exactly we are in the world in three dimensional space when we're taking imagery, aerial imagery in this case, then maybe we could use something like repeat terrestrial photography instead. With repeat terrestrial photography, you would essentially just go pick a landmark. So somewhere just on the surface of the ground that you could walk out to in the field, you'd pick a certain direction, get a certain bearing at that specific location. So whether it's north, south, east, west, whatever it might be, and then you could continually go back to that exact same landmark, to that exact same position, put your camera in the exact same position, find that exact same bearing, that exact same direction, and then just repeat photography. Just take photos of the exact same target over and over and over again, every couple months, every couple years if you wanted, and this is one of the most valuable historical datasets, particularly for monitoring the craft sphere that we have available to us. It's very high resolution data for the most part, because we're so close to the target. You can see here in this example on the right, this is looking at the glacial extent of this glacier in Alaska right here. You can see we're pretty close to it, so we can get a pretty detailed level of information about the extent of ice on this glacier in this example. But again, still not super standardized, we still have different scientists going out in the field, maybe collecting their photography in slightly different ways. Maybe we didn't have GPS to be able to track exactly in space where this photography was being taken from, so if someone was picking a landmark from where they could choose to take the photos from, then if we find that landmark, how do we know we're placing the camera in the exact exact exact same position? It was difficult, still not super standardized, but did give us a wide breadth of information with a pretty large temporal dimension, which just means we have data going back quite far of this kind of terrestrial photography of glaciers in particular. The other way that we have monitored the craft sphere and that we still continue to use to monitor the craft sphere is field work. So two examples of field work from monitoring snow and ice and glaciers and things of that sort. One here is a pit, a snow pit in this example, or an ice pit where you essentially take a cross section of somewhere in the glacier, in the snow, in the ice, and you just dig or drill down really, really far till you essentially hit either the ground, this case I think they've hit the ground there, so just as far as you can go essentially before there's no ice and you just hit the ground. And then you can look at that and say, okay, I can measure how deep this is from the surface, I can kind of monitor and look at all the different layers in this ice and collect a wide breadth of information about the glacier in this case or about the snow cover or about the ice if you just go away and dig a pit like this. One other very common way that we measure and monitor glaciers, especially historically, is with something called ablation stakes. So that's what this example is on the right here. And it's just taking a piece of wood or an iron rod or something that can just be used as a stake where you just stake it into the ground, it's essentially just a pole that you put into the snow. And by doing that, because glaciers move, because they ablate, they kind of move down whatever parcel of landscape that they're on top of, you can monitor where you put your ablation stakes and then go away for a certain amount of time and then come back and check where your ablation stakes have moved to. So by being able to measure and monitor how far your ablation stakes may have moved, you can measure things like glacier mass balance, things like glacier ice flow, things like glacier retreat. And so that's done with these stakes called ablation stakes here. Both of these methods, you know, relatively speaking, have been pretty standardized. There's relatively clear guidelines for how to do this appropriately. But it still has a certain level of subjectivity to it because individuals can go out and do all of these kinds of measurements slightly differently. So then enter remote sensing earth observation data where we can go away and get measurements like this of all of the ice, the sea ice in this example of the Arctic at a global scale every single day for all days of the year. So this is an example using MODIS data. MODIS data we know has global temporal resolutions with a 250 to 1,000 meter spatial resolution. And it allows us to go away and build up really nice visualizations like this and be able to track through time at a really nice scale how sea ice is changing, how things like snow cover and glacial extent might be changing all across the world. So specifically the data sets that I'm gonna talk about today for monitoring different portions of the cryosphere are Landsat, MODIS, radar and LIDAR. Now just to kind of overview them briefly before we talk about them, and this is kind of a good chance for me to remind you about the different advantages and disadvantages to each of these data sets. We've talked about each of these kind of a fair bit throughout the course, and we're gonna continue to talk about each of these data sets, Landsat, MODIS, radar and LIDAR, but now more with regard to specific applications. So in this case for the lecture today with specific applications toward monitoring the cryosphere. So towards monitoring snow and ice. But the fundamental characteristics of each of these data sets remains the same. And so because of that a lot of their advantages remain the same even though we're looking at something slightly different. Even though we're looking at the cryosphere as opposed to looking at the biosphere like we did last week, a lot of the advantages of each of these data sets remains consistent. So in an exam setting, if particularly on a short answer question, I'm asking you to describe appropriate data sets that could be used for a specific application, then really the advantages and disadvantages and your reasoning for using a specific application is gonna be relatively the same, you just have to apply it to whatever specific example I'm talking about. Whether that's specific to sea ice and snow or specific to monitoring primary productivity of the biosphere, et cetera. So for Landsat for example, we know Landsat gives us that fine to moderate spatial resolution of 30 meters. We know it as data dating back to the 70s and 80s and we know it gives us a spectral amount or a spectral type of information because it's a passive remote sensing instrument. Now we're gonna talk about specifically the kinds of applications for monitoring snow and ice that that ultimately makes Landsat good for. But whether I'm asking you on the exam about the cryosphere, about the biosphere, about oceans and fresh water, if you are thinking about describing why Landsat might be a good thing to use to monitor that specific application, you should always be kind of thinking back to something like this where you're thinking, okay, Landsat I know has that fine to moderate spatial resolution. So it gives me a good amount of spatial detail. It dates back to the 70s or 80s. So if I wanna monitor something back to then, I need to use that, et cetera, et cetera. So I'll give some examples of that and I'll try to go through and give some examples of that as we talk about some different applications of each of these data sets today. So MODIS on the other hand, just to remind you guys, gives us that fine temporal resolution, that daily revisit time. It's applicable for imaging much larger areas, things like global coverage or in the case of the cryosphere, looking at sea ice across all of the Arctic, for example. It's also a passive system, so it provides us that spectral information gives us a range of measurements across different bands of the electromagnetic spectrum. Then we have radar, we know we have radar sat, the Canadian owned and operated satellite mission. We also have different forms of airborne and terrestrial radar, which I'll talk about briefly today. And we know that that's an all weather system, it can see through clouds, it uses microwaves and it has the potential to be surface penetrating. Then lastly, we have lidar, either ice sat, which is a satellite or a space-borne lidar system or airborne lidar, where we just have a lidar instrument mounted on a plane. And we know that can be used to measure elevation and give us really high resolution 3D information on topography. Okay, so just then thinking in general, about each of those applications or about each of those data sets, what characteristics of the cryosphere could we measure or monitor with Earth observation data? And then maybe which Earth observation data sets might be the most applicable for measuring that characteristic? This is a really nice example, I think to kind of try and get your minds working for questions you might see on the final exam. We know that the main data sets we've talked about throughout this course are things like Landsat, MODIS, lidar and radar. So in the case of the cryosphere, what can we measure and then which of these data sets would be most appropriate for measuring those things? So for the cryosphere, for ice and snow, maybe we can just measure cover, maybe we can just measure how much snow cover is on the ground. Is there a 0% snow cover, so no snow? Is there a 25 or 50%, 75% or 100% snow cover? We can measure things like extent, what's the level of surface area that snow or ice might cover? We can measure things like depth, how deep is snow? How deep is the ice in a particular area? We can measure things like type, is the snow or ice in a particular area? Is it glacial fern, is it fresh snow? We can measure things like ice and snow age, particularly with something like ice penetrating radar when we're able to see below the surface, we can get a sense of what the different ages are of the different layers of snow we can see in glaciers and in different portions of ice. We can also get a sense for movement with things like lidar, things like radar that give us that three dimensional information, we can get a sense of how glaciers are moving, how their topography is changing. So hopefully, if I were to just kind of give you this list of different characteristics of the cryosphere that are known to be measured by different forms of Earth observation data, you could maybe hypothesize a little bit which of these might make sense for each of these applications. Maybe we'll come back to that, I don't expect you to be able to do that before I talk about it all. So I'll talk about it a little bit, we'll talk about some specific applications of each of these data sets, and then that's ultimately what I want you guys to be thinking about in terms of trying to apply kind of what I'm gonna lecture about for the rest of the class today in terms of a final exam setting. It's based off of what I'm talking about, based off the examples I'm giving, then which of these data sets would make the most sense for monitoring each of these different forms of the cryosphere or each of these different metrics for snow and ice that we can look at. Okay, so the first kind of category of data sets that I'll talk about are spectral data sets. Now, spectral data sets, I will refer to as Landsat and Modus for this course. Spectral data sets are essentially, they don't have the same definition as a passive remote sensing data set. Technically passive, just means that we are measuring energy that's come from the sun and reflected off the surface of the earth, while spectral information just refers to the diversity of spectral information we can get, meaning that we can measure the reflectance of different bands all across the electromagnetic spectrum. But you can kind of for now at least think of spectral remote sensing data sets kind of in coherence with passive remote sensing data sets, just because we've talked about what passive data sets are and what the definition of those are. In an exam setting, it'd be good to understand the difference between those terms and I'll make sure I clear that up in our review session. But in terms of spectral data sets, we're able to get spectral information, which means that if we look at the electromagnetic spectrum, we can use our knowledge of spectral signatures in order to gain some sort of information about, in this case, snow and ice cover. So in this example up here on the right in this graph, why might it be that we can see fresh snow here at the top, so this is the spectral signature for fresh snow, versus dirty glacier ice kind of down here. And we can see that the reflectance of glacier ice, particularly in the visible portion of the spectrum here, is very, very, very low compared to, say, the spectral reflectance of fresh snow way up here. Now ice, snow, they're not the exact same thing, but they're pretty close to one another. So why would we see this massive difference between the spectral response of glacier ice versus something like fern or fresh snow, these different kinds of snow and ice that we can see when we're looking at the cryosphere, when we're looking at a particular glacier? There's two main reasons why, particularly with dirty, well, with glacier ice, the term dirty kind of gives it away. But with glacier ice, there's always lots of impurities and debris cover. So you'll often see lots of dust, lots of dirt, lots of debris, essentially rocks, cobbles, things of that nature, on the glacier snow. So that ultimately results in it having less reflectance because it doesn't have that kind of clear, white appearance that really, really, really fresh snow has. It also has a higher degree of surface roughness. Now that matters because kind of fresh, smooth snow are more like pure, specular reflectors, where all the light coming down kind of gets reflected straight back up. So there's a very strong signal that might get sent back to the Lance Satter modus sensor. If you have a high degree of surface roughness, which you often do on glacial ice, then a lot of the energy coming down and hitting the glacial ice is kind of being bounced around in all kind of different directions. So you don't get as much overall signal back towards the sensor of that light. So that's kind of, there's some other reasons as well that are a little bit more physicsy and a little bit more aggressive in nature in terms of their science. But the two that I want you to be aware of, for example, is of why do glaciers have lower reflectance than say fresh snow? It's because glaciers have a higher level of impurities and debris cover, and they have a higher degree of surface roughness. And you can kind of see that in this example here. So kind of at the back towards the top of this glacier, you can see some fresher snow. And then the older glacier snow, you can see down here has kind of this gray appearance to it. There's lots of dirt, lots of debris cover on it. And that's something that we're able to pick out with these spectral remote sensing data sets because we can measure different bands in the visible portion of the electromagnetic spectrum. So we can kind of identify and differentiate, okay, this is older glacial snow versus this is much newer, wider snow, and that has a lot of implications for melting patterns on glaciers like this. Areas that are much darker like this, areas that have much less albedo, that have a lot of impurities and are very dirty, are gonna melt much, much, much quicker than areas like this that are reflecting all of that energy. The wider areas, the newer snow, if they're reflecting more energy, they're absorbing less energy, which means that they don't have to melt nearly as quickly. Because these kind of dirty portions of ice and snow down here on that portion of the glacier are absorbing much more energy because they're not nearly reflecting as much radiation, they're gonna melt much, much, much quicker. Okay, so I just have an example here, a quick video. There's some, there's no kind of voiceover to go with it. There's some music that's kind of soothing if you guys wanna sit back and relax. No, it's not that exciting. But this is just to give you a sense of all of the different kinds of textures and colors that you can see from a satellite of something as simple as a glacier, and just different kinds of ice. time.... And every once and a while I'm going to fall down a little bit of something just to get into the Christmas and enter the new year. And I am just a know the weary Christophoke, a lot of us I'm sailing this promotion with a head of life. And all these choices are made me straight, taken me away from the place that I'm seeing in. And all these choices are made me straight, and I'm just a drunk amongst the whites when I'm right here to live. Oh, I'm just a drunk amongst the whites when I'm right here. And now we've got our stuff and think about the future now. Listen very closely for a slice of reason. It's hidden in my head but it's not turned in my blood and I'm not my hero. We have to be just a beauty inside. And now I'm just a rich man. I live by eight decisions that you will be born to be by eight. And now I'm just a rich man. And now I'm just a rich man. I live by eight decisions that you will be born. And now I'm just a rich man. And now I'm just a rich man. And our hearts and blessings, We will be away under the mist of tonight, We will have to know where you are from With my God, I would like to know That I don't know where you are from With my God, I would like to know That's not me, by the way, just in case, not my video. But what's nice about that video is you can kind of get a sense of, Even for something that is as simple as a glacier, Which you might think would, generally speaking, always have the same Spectral signature and thus always appear pretty similar to a satellite Or to Earth observation data, could be very, very, very, very different Depending on its characteristics. And we can analyze that and sense how those things are changing With something like Earth observation data. So this is an example of just up here a Landsat image Just the true color composite of the Landsat image, and then what seems to be what they've applied As a snow detection or ice detection algorithm, where they've mapped out all the areas in this area Or in this study area that are covered by ice or that are covered by snow. Now, with Landsat, it allows us to really at a fine spatial scale Make out a lot of those differences that were shown in that video just now. A lot of those kind of changes, those different characteristics of ice and snow On ice shelves, on glaciers, whatever it might be Wouldn't be able to be discriminated without something that has a Fodder, fine to moderate spatial resolution like Landsat. Now inherently that also means that Landsat isn't great or isn't super useful For monitoring things like global sea ice cover. But for monitoring in a more regional area, for maybe a particular range of mountains Or maybe say the size of provincial parks that we have in British Columbia We can really accurately add a nice, fine spatial resolution Monitor and detect ice throughout the year And get a sense of how the ice and snow in the glaciers and the mountains is changing And how their dynamics look across time in different portions of the season. So Landsat, really applicable in terms of the craft sphere For making out those very fine scale changes in whatever it might be If it's color or those different patterns that you see us all, whether it's braided or hummed A lot of those very fine scale changes that you're able to detect with Spectral information, Landsat is really, really good for But inherently it's only good for kind of a smaller area Something like a regional scale, something the size of maybe a couple of mountains Or provincial park. This is another example just specifically of using Landsat 8 to monitor glaciers Again, not made by me, made by the same, same researcher that we looked at that made that first video It gives you just a bit of a sense of the scale at which something like Landsat is being used for To monitor glaciers and ice and parts of the craft sphere This other example here, I am showing a series of modus images And in this particular series of images we're looking at a rapid breakup of an ice shelf in Antarctica Between July 28th and 31st 2008 Now what you can see here is kind of right around here, there's this big ice shelf And you can see it's starting to break off here and then fully breaking off in this last image here And what you might notice right away by looking at this series of images Or at least what I'd hope that you can kind of notice by looking at these is their dates So notice this is July 28th, July 29th, July 31st So again, modus data, revisit time every one to two days So we can monitor a change that's happening on a daily temporal scale We can monitor something like the rapid breakup of this ice shelf in this case Something that would be not possible with Landsat data Landsat data has a revisit time, a temporal resolution of 16 days So we wouldn't be able to monitor something like this that's happening on a daily scale in the cryosphere with Landsat So modus data allows us to monitor these kind of rapid fine temporal scale changes While Landsat doesn't allow us to do that Or something like modus is also particularly well suited for very large spatial coverages So because it has a low spatial resolution of 250 to 1000 meters It's particularly good for looking at very large areas So in this case, this graph on the right here is showing the Arctic sea ice extent Over the last 15 years or so So the gray shaded area here is plus or minus two standard deviations From the average pattern of sea ice extent in the Arctic So this gray line here is the average amount of sea ice in the Arctic from 1981 to 2010 On this kind of rate of change in this portion of the year from February to June And then it just shows for more recent years 2011, 2012, 2013, 2014 and 2015 What the pattern of sea ice extent was from February to June And you can see here the most recent one being 2015 So obviously the lowest run right here, that kind of bright blue here And you can notice that pattern where each year we're kind of getting this lower level of sea ice extent Throughout the entire year Now, again, this is derived from looking at all of the Arctic All of the sea ice in the Arctic You could technically try to do that with Landsat Realistically with Landsat, you would only be able to measure something like Overall ice cover across the whole year You're not going to get that fine temporal resolution data The way you do with modus So modus allows us to look at patterns, say, on this monthly scale Something that would be very difficult with Landsat And modus allows us to look at very large areas Because it's got that lower spatial resolution So I didn't necessarily give, you know, two specifics Didn't go into too much depth About exactly what Landsat can do In terms of monitoring the cryosphere And exactly what modus can do in terms of monitoring the cryosphere Gave you a couple of examples And a couple of key words, key topics that I would hope that you could touch on in an exam setting For Landsat, we know it's got that lower spatial resolution So it can make out some of those finer details In terms of differences of spectral signatures That you might see between, say, fern versus dirty glacier ice Versus clean, new, white snow on glaciers So you can get some of those fine scale patterns With modus, we know it's got that lower spatial resolution So it's good for looking at more global metrics And it's got that high temporal resolution So it's good for looking at things that are happening on a very short time scale Any questions about that? Yeah? You can see the extent of all the coverings And extend covering how memoirs show it like there Extent snow, sorry, like snow extent for snow cover Yeah, so snow extent is essentially related to the size of the area that snow is covering Cover is related to the percentage of snow covering a particular pixel generally So with snow extent, you would be describing it with an area With snow cover, you'd be describing it with a percentage So 0% cover, 25% cover, 30% cover Extent would be, is there snow covering this area or is it not? And then the total extent would just be the total amount of area that the snow is covering Does that make sense? Any other questions? Okay, sweet. So this is, again, an example of a data set of visualization You could get with modus data where we're able to look at, in this case, the extent of Snow across North America every month for a variety of years You can see the cyclical pattern of snow kind of coming down Covering these lower latitudes and then receding back up during the summer Again, a data set that would be really hard to put together and to collect with Landsat, but something that modus is super capable of because of that high temporal Resolution, that lower spatial resolution. Okay, the next data set I'm going to talk about in terms of its applications for the Cryosphere is radar sat. Now, radar, if you remember, we briefly talked about radar being able to detect things As well as radar being capable of making ranging measurements. Radar being able to detect things relating to essentially the back scatter, the type Of signal that's coming back after microwaves are sent down to a target, bounce off That target and come back to the sensor, and then the ranging part being radar being Able to measure how far away that target is from the sensor. So i've classified that up here, i've talked about that in terms of back scatter Versus ranging. So back scatter refers to detection, it refers to the ability to Discriminate different signals that you're getting from different targets just based Off of their properties, whereas ranging refers to kind of the 3d information that You can get from radar to be able to monitor things like topological characteristics. So with radar, i've brought broken it into essentially radar sat, which is our Spaceborne radar, and then airborne or terrestrial radar, which i'll talk about in a Moment after this. So in terms of radar sat, our space-borne radar, we can estimate Things like snowmass. So areas that have different levels of Snowmass because of their properties, because of their dielectric or Chemical properties, they'll actually give different signals of micro waves Bouncing off of them back to the radar sensor depending on how much mass there is Associated with the snow in that area. So using the back scatter, using the Detection capabilities of radar sat, we can get estimations of snowmass. We can also get estimations of snow cover or ice cover. Areas that are covered by ice Will have different back scatter, different types of detection from radar Signals than areas not covered by snow. So using the back scatter, using the Detection capability of radar, we can measure things like ice cover, where we Just say, okay, that back scatter looks different from that back scatter, therefore That must be ice, and that must be not ice. We can also get really interesting, very Cool measurements on things like permafrost. So with permafrost, we can get more of A, or we can apply more of the ranging capabilities of radar, and we can see Okay, how are areas covered by permafrost changing? In the sense that areas that Are generally frozen all year round are pretty stable in areas where there's lots Of permafrost. So in Canada, for example, areas like where there's very high Northern latitudes, like Nunavut, lots of these northern communities that generally Have permafrost all year round, as they lose permafrost, whereas, which happens As the temperature slowly warms, their soil stability gets much, much lower, which Creates lots of these sinkholes, things that are very dangerous and potentially Hazards for communities. So one thing that radar side is really good at, and it's Been applied to, and it's shown in this map here on the right, is its ability to Detect and monitor soil stability. So what it's using is it's Ranging capabilities, its ability to kind of measure topography, and how that Topography is changing so that we can understand soil stability. Areas where There's lots of changes in the topography, lots of dips that are being created, as That permafrost becomes less stable, can be detected and quantified by something Like radar sat. So you can see here, this is just a comparison on the top map here And the bottom map here, between summer of 2015 and summer of 2016, and you can see Lots more areas in this bottom map that are blue and kind of pinkish and purple-ish Those are areas that have much lower soil stability than in this upper map here Where you can see areas in general have a higher level of soil stability. We can also with radar sat, lastly, measure and monitor things like ice flow Mapping. We can use the combination of backscatter of the detection capabilities Of radar, in combination with its ranging capabilities, to monitor ice and Understand how ice is flowing across a glacier and actually get a sense of how it's moving. So we can measure things like ice flow rate. What's the actual speed in meters per Second of how fast ice is flowing and moving down through a glacier. So pretty cool applications of using space-borne radar. In this case radar sat. Now we can also use radar that is mounted on-board airplanes. We can also use radar that is terrestrial. So, like i said, airborne radar, pretty simple. You just apply or mount a radar instrument right onto a plain wing. Terrestrial radar, bit more complicated. Generally works something like this where you Have a sled that has the radar instrument in it, where it's pointing straight down, Sending radar microwaves into the snow or into the ice. And then you got some poor guy here who's towing this thing along the ice. He's got a gps antenna attached to him so he can monitor exactly where he is And two or three dimensional space as he's pulling this radar instrument along the ground. But both of these work in kind of the same sense where whether it's airborne or Terrestrial radar, they're just flying or walking transects back and forth and back And forth and back and forth and back and forth across a study area to build up an Image or a data set that is covering a kind of large, maybe rectangular or Square area of interest. By doing that they can go and create images like this. So this is an image that's derived from a airborne radar data set where they were trying To go away and map how deep the rock bed was below the ice. And so you can see here this layer right here, that is the bedrock layer right there. They were actually able to measure at what depth below the ice that bedrock was Located at. You can also get a sense here of all the different kind of layers in the ice that you can see. And so with the ice penetrating radar, different surfaces below the top surface, So different kind of layers of ice will give different types of signals back to the radar instrument And thus allow the radar instrument to discriminate what and where these different layers are Located below the surface of the ice. Really valuable data set and something that is Kind of groundbreaking in its ability to efficiently in a standardized way go away and collect data From below the surface without ever having to go away and dig a big hole or anything like that. This is another example from a terrestrial based radar system where their goal was to go away And try and identify how deep below the ice or snow was the water layer. And so you can see that here they were able to differentiate it. So this layer right here is where the water layer was located below the ice. And then again here you can make out all the different fern layers, all the different layers below the surface of the ice That were accumulated over years and years and years. Okay, any questions about radar? Radar applications? Crissed for applications of radar. Yeah? How does the back stuff move with a smash in a different way? Yeah. So it is very, very complicated. Radar is notorious for being very complicated and very hard to understand. It often involves very high level calculus and physics to be able to understand different backscatter signals. Essentially in kind of its simplest form, it is just the microwave signals that are sent down That bounce off of whatever target, whatever feature is below the surface of the earth. It will come back towards the sensor in a specific pattern. So in kind of a specific waveform depending on how deep it went, depending on what surface or what kind of material It was bouncing off of. And essentially they go away and apply, again, stuff that is way over my head, very high level calculus To be able to understand the signs and the waveforms of the microwaves that are reflecting off these materials And coming back towards the sensor. But the microwave is the microwave, the frequency of the microwave, the wavelength of the microwave doesn't change right? The wavelength of the microwave doesn't change. But the level of the type of backscatter you are getting, for example, the polarization that you might be getting, The different characteristics of the waves that are way beyond just the actual wavelength Are going to be potentially very, very different depending on what the material is that they are bouncing off of. That is kind of the basics of it. I know it is based off calculus, beyond that, I honestly don't know. Yeah, no problem. Do you have a question? Yeah, I think that would be more natural. Yeah, it is very complicated, it is not easy. To give you maybe a bit more background, these two images, this kind of image here and this image here Would take a ton of pre-processing, processing, calculus, algorithms, ton of computer power Way beyond what I am capable of wrapping my mind round to go away and actually output. So, yeah, it is a cool technology radar in general, but it is also very notorious in the remote sensing community For being very difficult to work with and very hard to wrap your head around. Yeah, hence I don't understand very well. But some people do, so we can get cool images like this. Okay, last data set I will talk about briefly, just LIDAR. So with airborne LIDAR, which we talked about, we talked about active remote sensing, we can get Very, we know we can get high resolution 3D information from LIDAR, things like digital elevation models, things like Structural information on forests. With ice sat, we talked about ice sat very, very briefly, but ice sat is a space born LIDAR system and kind of As suggested by its name, one of its key purposes was to go away and measure and monitor the Cryosphere and portions of ice. It was also developed for measuring clouds and elevation, which is where it is acronym comes from. It is actually ice cloud and elevation satellite. But one of its key things was to measure ice. One was the first mission from ice sat operating from 2003 to 2009. Ice sat 2 was launched in 2018. And one thing, one kind of key characteristic about ice sat that I always like to know to point out, Because it is kind of fun, is that ice sat uses visible green laser pulses with a wavelength size of about 532 nanometers. If you remember from our active remote sensing lecture, I mentioned that, generally speaking, with LIDAR, Particularly with applications in forestry and for monitoring terrestrial areas, we typically use LIDAR that has a wavelength in the portion of the near infrared spectrum. We use that generally because things like forests, vegetation reflect near infrared light very, very strongly. Now, ice sat doesn't use near infrared light in its laser pulses. It uses visible green light. And using this spectral signature here of ice as a bit of a hint, why do you think we might use green light, visible green light, as opposed to something like near infrared light on ice sat. On a LIDAR space-borne sensor that's kind of meant to monitor specifically ice. And the key here is really in this spectral signature. You can see that the peak of reflectance for ice is kind of in the green portion of the spectrum, kind of right up here. Unlike something like vegetation, where we know vegetation's peak reflectance is often in the near infrared portion of the spectrum, kind of around here. So for ice sat, it makes sense to use a green visible laser light because that is going to give us the highest signal. It's going to have the most reflectance of those laser pulses if we're monitoring something like ice. Okay, our quick video on ice sat and how it works. And then we will go over some practice questions. And then I'll give it to Leanna. We have a very good camera of how all of us looks like. You can see ocean, sea ice, and sea of forest. But as much as I have to measure, is how ice-eating sounds on a global scale almost impossible. And then we have a lot of custom-measured changes in the ice sheets or in the ocean or in land. Ice-a-2 is designed to measure the changes that are going on in the craft, what are we eating? All the change is at the edges. Those are the speedily sloping parts of the glacier that interact with the ocean. And that's where all the action is. That's where all the mass is being lost. The emission ice-a-2 carries a single instrument. It's called an anvilus. The anvilus is a topographic laser on a terminal system. Anvilus sets out small pulses of laser light 10,000 times a second. And by measuring precisely how long it takes, that light to go from the spacecraft down to the earth and back into the spacecraft allows us to figure out what the height of the surface is beneath that set tube. We need to measure the time of flight, the single photon, or the single laser pulse with a precision of a billion of a second. Mass engineers have to come up with entirely new ways of measuring time very precisely. A billion of a second translates to an elevation change precision of just a few centimeters. Climate change is amplified in the Puerto regions. Ice-a-2 is designed to measure those areas and will help us to understand what's going on in our plan. Okay, so there's just a bit of a... Oh, it's really lagging. Let me just look at that. Okay, so this is just an illustration, a little bit of how ice-a-t works. So these green kind of paths would be the swath or the area that the ice-a-t satellite was looking at as it passes over a polar region. And then in the top left window here, this would be its measurement. So this kind of dark blue here would be the ocean. And then the kind of little white peaks you see that it's measuring would be the topography of the ice on top of the ocean. So ice-a-t works. It doesn't kind of directly measure things like ice depth and ice mass. What it does is it measures the elevation or topography of ice. So of say Arctic sea ice, for example. And then it measures the nearby sea level elevation and then says, okay, if the ice is this high and the sea level is this high, then the ice depth is about the difference between that. And then using that as well as some other modeling and algorithms, they can also go away and determine how much ice there is below the surface of the water and how much mass there is in the ice and those kind of metrics. But it doesn't kind of directly measure depth in the sense that as we hopefully remember from the active remote sensing lecture, LiDAR doesn't penetrate through materials. So LiDAR doesn't penetrate through ice the way radar is capable of. But we can still, with combining it with other data sets, measure things like ice depth by understanding, okay, what's the sea level of nearby areas and then using some modeling techniques to figure out then what the difference is and what the depth of that ice might be. So this is just an example of a data set derived from ice set. It's showing sea ice thickness and meters across a portion of the Arctic. Okay. So lastly, kind of just to summarize, talking about using Earth observation data for monitoring the cryosphere, a couple of key reasons. It's useful versus the historical ways we used to monitor the cryosphere with aerial photography, with repeat terrestrial photography, with field measurements. One is we have standardized data. We have a standardized way of collecting data. It's always Landsat data. It's always MODIS data. It's always ice set data. It's always from the same data source. It allows us to have a standardized level of data collection. We also have a Fish and Data Collection. We're getting pretty good global coverage a lot of the time from a lot of these satellite missions. And we're not having to go out and collect it ourselves. We don't have to actually put too many resources into going out and doing that. We get good coverage in space. So we get satellite data that is covering large portions of the surface of the Earth. And we can get different types of information because of the different resolutions and specifications of those different data sets. With Landsat, we can get that moderate to find spatial resolution information to be able to make out some of those fine details in different portions of the spectrum. We can get that fine temporal level information with something like MODIS to be able to monitor changes in the cryosphere that are happening on a daily scale. We can get that pretty impressive level of spectral information from the same MODIS or Landsat, where we can look at the visible portion of the spectrum, but also the near infrared, the short wave infrared and thermal infrared portions of the spectrum, to be able to pull apart different types of metrics and different kinds of information related to the cryosphere. Now, if you were here for our last lecture, you may notice a pattern here where kind of at the end of, well, at the start of each of the lectures, I say this is how we used to monitor this kind of thing. This is why it's not very efficient. This is how we monitor it now with Earth observation data. This is why it's efficient. This is why it allows us to give standardized data collection. It has good coverage in space. It allows us different kinds of information with all these different resolutions. That's pretty much going to hold. So, I say that because for final exam purposes, a lot of the, or at least you can expect short answer questions related to, okay, well, why is Earth observation data good or useful for this particular application? Again, your reasoning is always going to be fundamentally very similar. It's always going to relate back to standardized data collection, efficient collection, good coverage, the different kinds of information we can get. You just need to be able to apply it to the specific example or application that I'm referring to, whether that's productivity in the biosphere, monitoring ice in the cryosphere. We'll talk about wildlife and oceans and fresh water tomorrow and then next week. So, just to give you a sense, just to give you kind of a reminder, I'm going to be drilling home this exact same thing, pretty much every lecture from here on out, but with specific applications to each of the topics that we're talking about, whether it's ice, whether it's wildlife, whether it's oceans, whether it's the biosphere. Okay, I have a couple of practice questions here. So, you're welcome to try these out yourself and then I'll go over the answers with you in a couple minutes, maybe about three, four minutes, and then I will give it to Leanna. Are you talking or are you just Q&A kind of thing? Okay, Leanna will give, pull up one thing quickly about the assignment that's due this week and then she'll be here if you want to ask any questions about the assignment that's due Thursday. If you want to head out now, you're welcome to do so and I will see you tomorrow. If you want to stay and go over to the questions, you are also welcome to do that. If you want to stay and talk to Leanna, also welcome to do that. Otherwise, I will see you tomorrow and, yeah, have a Monday. Thanks for coming. Okay, do you want to pull it up now? Oh, actually, I know mine I should. I should leave that up. We can pull it up on here. Oh, that's pretty good. Hey, dude. Can I let these up? Yep. Yeah, anyways. Okay, guys, let's try and answer these questions together. So, three examples that we talked about. How was the cryosphere monitored historically? Yeah. You got it. Yeah, aerial photography, terrestrial repeat photography as well as field work in the form of we talked about digging ice pits as well as using ablation. What I call ablation stakes. That's what they're called. Bllation stakes. Okay. Is snow always white? Why or why not? Anyone answer that for me? Yeah? Sure. It is not always white. It has different reflections patterns depending on the type of snow. Exactly. Yeah, it's got different. So, snow's not always white. It's got different reflectings patterns depending on if it's fern, depending off its old dirty glacier snow, depending on if it's fresh snow. And then how ultimately are we able to, with spectral earth observation data sets, take advantage of that phenomenon? What can we ultimately, what are we going away and measuring with say, land sat, that allows us to take advantage of the changes or the different kinds of spectral signatures or colors that we can see with snow? Yeah. Depending on the color of the snow or ice, it melts at different rates. Exactly. Yeah, that's a great example. Yeah, depending on the color of the snow, depending on the patterns of the snow that you can detect with something like land sat, it's going to melt at different rates, which is going to have different impacts for downstream ecosystems, different impacts for fresh water, different impacts for water management for people, all kinds of things. So, can you name please one cryosphere specific example of radar sat? We talked about a couple, just one. Yeah. Because they use, it uses microwaves, can like penetrate through ice and you can get that picture of like the bedrock and all the layer spice. So that was maybe that I'm glad you said that because that's a good chance for me to clarify that. Generally speaking, radar sat, which is the space for radar system, isn't ground penetrating. Generally for it to be a ground penetrating radar system, it's either an airborne system or a terrestrial system that's flown much closer to the surface of the earth. So for radar sat specifically, do you remember one? If not, maybe someone else? Yeah. Yeah, exactly. Yeah. Monitoring, measuring something like snow mass. The backscatter, the detection from radar will be different depending on the different levels of massive snow in the target area that it's reflecting off of. Okay. How is ice penetrating radar typically collected? You were on to that one. You want to go for it? Yeah. Exactly. That's a key point. Generally speaking with radar sat, we don't see penetration properties specifically of ice. If we're getting ice penetrating radar measurements, we're generally using airborne radar where it's just mounted on a plane or terrestrial radar where it's just being dragged along the ground. Okay. And then lastly, what is ice sat? Essentially what kind of remote sensing instrument is onboard ice sat? Yeah. So, lidar instrument is built for ice clouds and... Elevation. Yeah, you got it. Ice cloud and elevation satellite, but it's a space-borne lidar system essentially. And what type of laser light does it use and why? Yeah. Visible green. Yeah. Because unlike a wooden used mirror for red, like something that's supposed to do vegetation because meditation gives off our reflector for red light. Exactly. Yeah. The visible green portion of the electromagnetic spectrum is where the highest reflectance is for ice in particular. Exactly. Yeah. So the spectral signature of ice is such that it reflects visible green light more than any other portion of the electromagnetic spectrum as opposed to something like vegetation, something like forests where we often with lidar use near infrared light as our laser pulse. With ice sat, we use visible green light because our target ice for the most part reflects visible green light more than other wavelengths in the spectrum. Perfect. Awesome. Okay. I will hand it to you. There's your mic. Thank you. Hey guys, how we doing? Go for it. Cool. So this is the last question on assignment for... I'm just going to go over this super quick because this is the question that there seems to be the most... confusion around. I haven't really gotten many questions on the other questions. Questions twice, but so that's good. So I'm going to use this as an example. So we have our transect drawn over this area down here in red. So this is not the pattern that you'll be looking at when you're doing... This is the color composite for questions 19 and 20. So this is not the spectral reflectance pattern you'll be seeing. So this is just an example. But for the purpose of these questions, when you draw the transect across the color composite and you're looking at those spectral reflectance curves. So we have like the red channel, the blue channel and the green channel. And you're looking at, okay, band five is in the red channel. So that's near infrared. So how you would go about answering the question is to look at what each of the lines are doing. So we can see that the red channel band or the red channel line or the near infrared band is being very highly reflected. And I would consider these down here, the blue channel and the green channel to be absorbed for the purpose of these questions. So I've kind of like put... Sorry, I've kind of like in the introduction video, I think I might say that like if it's being absorbed, if it's at 32 or below. But if we're asking a question and you don't really have an option like, okay, all three of them are being reflected like this, just try to look at what the brightness behavior of the channel lines is doing in relation to the other bands. So again, for this question, red would be reflected, but blue and green would be absorbed. Does that kind of make sense? This is like definitely the trickiest part of the whole assignment. Does anyone have questions about that? Or any other part of the assignment? See, lines come down a few minutes for the end. Bye-bye. Thanks guys. Welcome. Today we are talking about oceans and fresh water. So far we have gone over kind of post-mid term. We've talked about resolutions. We've talked about active remote sensing. We've talked about then specific applications looking at the biosphere first and then looking at the cryosphere. And today we're going to look at fresh water and oceans. Next week, Monday the first lecture will be on wildlife. Wildlife. That lecture that I give for this course is probably my favorite lecture for the course because it's the most specific to the research that I've done and my experiences with remote sensing. So I'm excited about that one. Today we're going to talk about oceans and fresh water. I wanted to first just clarify something while I was struggling through my Monday yesterday talking to you guys about radar. We had a couple questions about just how radar measurements are made, how we actually measure and detect different things, different surfaces on the earth. And I wanted to just briefly remind you to kind of link back to our active remote sensing lecture when we talked about how dielectric properties and water content and surface roughness can affect the backscatter of radar. And the main things that the main kind of feature or characteristic of the backscattered radar waves or microwave waves that are being measured there is the intensity of the scatter. So that's kind of the, does that kind of help answer your question a little bit? Yeah. So it has to do with the intensity of the backscatter. So we know radar sends out microwaves, it bounces off the targets. How much of those microwaves get returned to the sensor as a portion or a function of the amount that was sent out is kind of what's being measured there to allow us to create detections and classify different things that we're looking at on the surface of the earth. That's kind of in its simplest form. I grabbed some resources about radar because it's a pretty complicated topic that we don't cover really in depth, at least especially in a technical sense in this course. So I gathered a resource, a good reading on that, and I'll post that online if anyone's interested or curious about it. Okay. For today though, we're talking about oceans. Evan's going to come at the end of the class. So if you have questions about any final questions about the midterm grading, last day to email either he or I about that is today. So he'll be here at the end of the class if you want to talk to him. Then Leanna will also be here at the end of the class today as well to talk about the assignment that's due today. Not today. Sorry. Sorry. Not scary. Assignment that's due Thursday. My bad. Okay. So today we'll talk about oceans and fresh water. What is it? Why monitor it? We'll talk about historically how it's monitored, remote sensing, earth observation, ways, technologies, methods that we use to monitor oceans in fresh water today. And we'll relate that to some specific applications and examples. Again, very similar outline format of this lecture as the past two lectures, very similar to the outline format we're going to have for the next two or so lectures after today as well. So oceans are the body of salt water that cover about 70% of the earth's surface and contains 97% of earth's water as a whole. So that's spread between the Atlantic, Pacific, Indian and Arctic Ocean. Then there's also the southern, which is this kind of newest ocean. It's not recognized worldwide, but it's often recognized in these, I think either it's recognized in the states or the states is the one place that doesn't recognize it. Can't remember off the top of my head, but there's this new ocean in the world called the southern ocean that is often recognized by a lot of political areas. Fresh water is just any water that's naturally occurring liquid or frozen water containing low concentrations of dissolved salts. So that includes glaciers, lakes, reservoirs, ponds, rivers, streams, wetlands and groundwater. It's only 3% of the earth's water. So only 3% of the earth's water is fresh water. And that's less than 1% of earth's total surface area. So on the scale of the whole globe, the amount of fresh water we actually have isn't huge. This is the map I was talking about that now denotes this southern ocean here. So in terms of ocean, we have the Arctic Ocean, the Atlantic Ocean, the Pacific Ocean, the Indian Ocean, the Pacific Ocean and the southern ocean here. I'm pretty sure it's the states is the only place that doesn't recognize the southern ocean. And I think that kind of goes in line with their, you know, empirical units of measurement and then also not being willing to do metric, not being willing to recognize the southern ocean. Anyways, it doesn't matter too much. The point is for the most part worldwide, we recognize there being the Pacific, Atlantic, southern ocean, Indian Ocean, Pacific and Arctic Ocean. So about 97% of earth's water as a whole, all of the global water that we have, is held in oceans, is in that kind of high salt concentration state. And then about 3% or so, or 2.5%, is held as fresh water. The majority of that is held in glaciers and ice caps. And then a smaller portion of that is held in groundwater. And then in surface and other fresh water, things like lakes, that's just 1.2% or 1 small percentage of all the available fresh water that we have. So all the water on the earth, about 3% of it is fresh water. Of all the fresh water, surface water only makes up about 1% of that. And then most of that surface water is actually ground ice and permafrost with about, you know, 20% or so in lakes. And then we have rivers and some other sources and atmospheric water as well up there. So that's kind of how water is spread all across the different sources and resources that we have on the earth. So why monitor oceans? Why monitor fresh water? Why is it important to monitor global water resources as a whole? Very similar reasons to the cryosphere, which makes sense. The cryosphere and water are inherently very linked. The cryosphere is just areas that have frozen water. So it's a very useful indicator of climate variability and change, monitoring things like sea surface temperature and how those metrics have changed through time. Potentially has a lot of impact on things like rising sea level, which again can impact people as well as biodiversity, impacting species habitats and the niches that they live in. It often has a very strong oceans in particular have a very strong hold on weather and climate. So understanding our ocean currents is a really big deal when it comes to understanding how our climate works, how our weather systems work. And oceans in particular are really important for regulation. So often continental areas will have a lot of fluctuation in their temperatures and in their weather patterns. And oceans often regulate a lot of that. So a lot of the ocean currents we have regulate a lot of the land surface temperatures that we live in and keep things nice and moderate and nice and consistent and comfortable for us. And then as well similar to the cryosphere, obviously water is important because of fresh water storage, because we need to know where our drinking water is coming from, as well as understanding how water is affecting other ecosystems and hydrological processes, things like flooding, things like how water is moving through the soil and forests, that kind of idea. So lots of different reasons that we might want to monitor oceans and fresh water. And historically monitoring oceans and fresh water was done in boats. It makes a lot of sense. So people were usually in boats placing nets or streams into the water and then just going and collecting data on composition of water and surfaces either below the water or layers of the different kind of columns of the water. You can also go away and historically we saw a lot of nautical charts. So we saw a lot of people going away and just kind of by paper manually, by hand, understanding tides and currents building up these big charts. But nowadays we don't really, I mean we still go out and use people in boats to go and conduct field work to validate a lot of the earth observation or remote sensing data that we collect. But the problem obviously with this is that it's not super efficient. We have to go and send people out in a boat. We don't get great coverage. Takes a very long time for a boat to cover a large area, a satellite can obviously cover a much larger area, much quicker. And we get now with our remote sensing data collection, much more standardized data sets that we can compare year after year after year after year. Okay, so different characteristics and features that we can measure and monitor in the oceans in fresh water include things like sea surface temperature, sea levels, fresh water storage, habitat characteristics such as coral reefs and salmon habitat and fresh water streams. So this list I've given up here is most definitely not a cumulative or exhaustive list. There's lots of other characteristics and features of oceans and fresh water that we could go away and measure. I'm going to focus on a couple of key ones that I've listed up here that are important on a global scale, things like sea surface temperature and sea levels. But then I'll also kind of focus on some more specific large scale or fine scale examples looking at habitat characteristics like the coral reefs and like salmon habitat and fresh water streams. Now as we go through and look at each of these examples, we'll also be relating them and talking about which Earth observation data set is used to monitor and measure each of those things. And we'll mostly focus on again similar data sets that we've talked about before, Landsat, MODIS, radar, LiDAR, and as well a couple of new ones that we haven't mentioned before that we'll kind of get to familiarize ourselves with a little bit today. Okay, so we're going to talk about sea surface temperature measurements first. So before satellites measurements were usually done from boys, those were the main tool, and they just provide a point measurement. So boys naturally only represent one single point in the ocean. You know, they don't represent a large area, they just represent exactly whatever point that that boy is floating in. But you can potentially get very, very fine temporal scale measurements with a boy. You can get measurements of temperature every hour, minute, second, every millisecond if you like, and they're all weather systems. So as opposed to having to deal with passive remote sensing systems like MODIS, like Landsat, boys are obviously an all weather system, they're functioning, no matter if there's a thunderstorm or rain or a windstorm, whatever it might be. They're pretty, you know, not completely indestructible, but decently indestructible. And so they're collecting lots and lots of data through time. Unfortunately, we just get that single point measurement with each boy, and we're not going to have, you know, millions and millions and millions and millions of boys all lined up one next to each other to be able to get coverage over the entire surface of the ocean. So enter Earth observation satellite data. We can get fundamental information on climate systems as a whole, such as identifying El Nino and Latin Nino patterns with Earth observation systems. These are typically measured with passive systems like MODIS, like Landsat. For sea surface temperature, generally we're looking at sea surface temperature for a pretty large coverage. It's not super common that we look at sea surface temperature for small regional areas. If that was the case, maybe you could use something like Landsat. But generally speaking, we usually measure sea surface temperature with MODIS. It's generally the preferable instrument to measure sea surface temperature because it's good for a core spatial scale because it has those small pixel sizes. And it's better temporal resolution. We can get those daily to by daily measurements of sea surface temperature with something like MODIS. So onboard MODIS, we have bands 31 and 32, which I don't expect you to remember off by heart, but just to kind of remind you, bands 31 and 32 use the thermal infrared portion of the spectrum. And so MODIS uses these two bands to get pretty precise measurements of sea surface temperature all around the world at a pretty incredible daily time scale. Now, unfortunately with MODIS, it's not an all weather system. And oftentimes over oceans, there's storms, there's clouds. There's lots of things that could potentially not allow the emitance values, in this case, of thermal infrared energy to reach the sensors. But we can go away. We can get really, really incredible data sets like this. So this is just an example looking at sea surface temperature all around the world every month of the year. So pretty incredible data set. Now, we kind of know based off of talking about the electromagnetic radiation, as well as talking a little bit about MODIS and talking about how we can measure sea surface temperature with the thermal infrared bands that MODIS has. We know that the ocean emits thermal infrared radiation. And that can be measured by space from sensors like MODIS. And it's proportional to sea surface temperature, just meaning that the more thermal infrared radiation that is emitted from the sea, the warmer the sea is. Now, infrared radiation, thermal infrared radiation in particular, comes from about the top 10 microns of the surface of the ocean. So the very, very, very, very top of the ocean surface, the top 10 microns, which is just another word for micrometers, a very, very, very small depth of the top of the ocean, is what is responsible for where the thermal infrared radiation comes from that, say, an instrument like MODIS measures. But the ocean also emits radiation in the microwave part of the spectrum. And that radiation is also proportional to sea surface temperature. Microwave radiation comes from the top millimeter or so of the surface of the sea. So you can see that in that, this little illustration on the right here. So this is, you know, microwave instrument versus infrared. And the measurements that you're taking with still a passive instrument, so not radar here, still a passive instrument, measuring the microwave radiation from the surface of the sea, in this case, allows us to also measure sea surface temperature. But it allows us to get a measurement that relates to a 1 millimeter depth of the surface of the sea, as opposed to just a couple of microns, the top 10 microns. So different measurements that you can get by measuring passively microwave radiation versus thermal infrared radiation. The reason I bring that up is because there is a satellite out there called the Tropical Rainfall Measuring Mission, or TRIM. And it has a microwave imager. So it's a passive system. It's a passive sensor. And it measures the emission of microwave radiation from surfaces. So it's not radar. It's not an active sensor. It doesn't send out pulses of microwave energy. It just measures the emitance of microwave energy. And it allows for really highly accurate sea surface temperature estimates. And what's nice about it is that it works in all weather conditions. So even though this is a passive sensor, this is kind of the one exception where a passive sensor can still work in really cloudy conditions. We know that microwaves can penetrate right through clouds. So the emitance coming from the top of the sea that say the TRIM instrument is going to measure of microwave radiation can go right through clouds all the way to the sensor. And it's representative of about the top one millimeter of the surface. And the reason that I bring this up kind of bigger picture is because this TRIM satellite was kind of very, very progressive and was a pretty big deal in terms of getting really accurate sea surface temperature measurements in all forms of weather conditions. So still a passive sensor, but allowing us to get more frequent measurements because we didn't have to worry about clouds and things like that. Unfortunately, it's because it was targeted for tropical areas. Its orbit was only restricted to plus or minus 30 degrees above the equator, which is why in this image where you have derived sea surface temperature measurements from the TRIM satellite, you don't see it going all the way to the north or south pole. So its latitude coverage was pretty limited, but you did get really, really accurate measurements of microwave emission emitance based sea surface temperature measurements in all weather conditions. So it was a pretty cool instrument. I just have a quick video talking about that satellite mission here. The TRIM is the Crock-O-Rame Fall Measure Commission. It was launched in November of 1997. It was originally designed as a five-year mission, but we've gone now for 15 years. 15 years and thousands of storms later, TRIM has contributed to the advancement of scientific milestones. TRIM has advanced research in the areas of agriculture, disease tracking, precipitation physics, and natural classrooms. We get roughly three hourly rainfall estimates across much of the globe at fairly high resolution, and these rainfall estimates are used to monitor major rainfall events and to look for events that might lead to significant flooding and even landslides, and there are a number of groups that have been using it as sort of an early warning system. TRIM's unforeseen longevity has provided more robust information on weather and climate products that can only be seen after years of observations. This decade and a half of data lets scientists see variations in rainfall from year to year, how an amino effect's rain patterns worldwide, and the anatomy and life cycle of major storms like hurricanes. It's also proven to be an extremely useful satellite for understanding hurricanes, part because its orbit stays within the traumas, so you get much more frequent observations, and it's just provided a wealth of rainfall information over a relatively short time period that in many ways surpasses all of the information we had prior to that. From giant storms to individual droplets, TRIM also provides scientists with data on the precise physics of falling raindrops. TRIM has a unique set of instruments including the first and only precipitation radar in space, also has a microwave commensurate of which is an instrument that can give you the equivalent of an extra rate of a storm whereas the radar is giving you more of a CAT scheme of the storm. It provides extremely valuable information on the rainfall structure of storms, which tells us something about how these storms respond into its environment, whether or not it might intensify or weaken in the upcoming hours. Building on TRIM's success would be the Global Precipitation Measurement, or GPM mission. Its two instruments are more advanced and more sensitive versions of TRIM's microwave emitter and precipitation radar. GPM's orbit will also extend coverage beyond the tropics and provide measurements of light to heavy rain and snow, expanding TRIM's light seat into the future. Okay, so just to clarify in case you kind of got confused at the start of that video, the TRIM satellite also has a radar instrument on board it, so it has a radar instrument which is the active remote sensing that's sending out microwaves and measuring the backscatter of those microwaves back to the sensor. In the case of TRIM they use bands that interact really well with rainfall droplets, so that's how they're able to get a lot of those rainfall measurements. And then it also has that microwave imager, which is that passive instrument that's able to measure emitons coming from the surface, traveling through clouds out to the sensor. Now, oftentimes we maybe don't want a sea surface temperature, maybe we want a measurement of below the surface, and those are primarily taken from mooring and drifter boys. We don't have a ton of really any Earth observation systems that are designed to measure below surface temperature. So mooring boys are good for measuring time series through the depths of the water column. Mooring boys are essentially just boys that are always anchored in the exact same spot, so it might look like this or it might look like this, where there's a subsurface float or a surface float, but either way these instruments can measure at different points of depth below the surface what the temperature is, and then most deeper ocean temperature data are measured with drifter boys. Drifter boys are just boys that aren't anchored to anything. They drift around with the currents, and for really, really, really, really deep portions of the ocean, drifter boys make a lot more sense because you don't have to have, you know, a crazy long chain like this that's maybe swinging in all kinds of directions and measuring different points. You usually just have a drifter boy that you set in a certain area and have kind of a calculated estimate of exactly where it's going to travel, and then you get really deep ocean temperature measurements from something like that. Now we also often sometimes want to measure sea or, you know, fresh water surface temperature, things like lake temperature, and we use very, very similar methods for monitoring lake surface temperature. Modus is still really frequently used, especially for something like monitoring the great lakes, fresh water systems that are very, very large. For smaller targets, again, thinking back to the difference of the characteristics between some of the data sets that we're talking about, Landsat is probably going to make a lot more sense. It's got that 30-meter spatial resolution for much smaller lakes. It's going to make a lot more sense and be much easier to use the Landsat thermal band to go away and try and measure things like lake surface temperature. Okay, just a brief kind of empirical example or comparison of really efficient data collection of a satellite versus kind of historical methods like boys. If you, in the mid-Atlantic, have a passing satellite measuring a sea surface temperature, then it might contain around 600,000 pixels of data and cover an area of about 250,000 or a quarter square miles, quarter million square miles. Now, in that same area, there's probably only about a dozen boys, and given the vast size and variability of the ocean, the applications of those boys become very, very limited. It's hard to use 12-point measurements of sea surface temperature over a quarter million square miles to try and derive any kind of meaningful information about sea surface temperature trends. On the other hand, a satellite that's passing over in seconds, maybe minutes, can collect data over that same large coverage and a fraction of the time, very, very little time. So, we still need boys and things that are collecting point data today to go away and validate our satellite measurements, but this is just an example of the extreme level of data collection efficiency that you can get with Earth observation data. Okay. So, that's sea surface measurements. We're going to talk next about sea level. So, in general, I'll start off by just mentioning that we know that sea level is rising, primarily because of two factors, thermal expansion of water from increased temperatures, so as water warms up, which is just kind of going along with the same trend that we see on the surface of land, we know that our air temperature is warming because of climate change, global warming, at the same time our oceans are also warming for some of the same reasons. As our oceans warm, they expand a bit. As they expand, they kind of cover more area, so we see this rise in sea level. We also, because of increased temperatures, see melting of land-based ice, glaciers, polar ice caps, things like that, areas where water was previously stored as solid ice is now being stored as liquid water. So, because of that, we're seeing also an increased sea level. And historically, the first satellites that we used to measure sea level were the Topex Poseidon mission and then the Jason series of satellites. The Jason series of satellites were essentially just a continuation of the Topex Poseidon mission satellites, so you can kind of think of these all together as one large kind of mission of satellites. The Topex Poseidon was the first satellite, then there was the Jason 1, the Jason 2, and the Jason 3. All of these satellites were a joint mission between the French Space Agency and NASA, and the Topex Poseidon mission, this first satellite here was the first time scientists were able to map ocean topography with a high level of accuracy all across the world. All four of these satellites, the Topex Poseidon, the Jason 1, Jason 2, and Jason 3, were all radar satellites. So they all used radar in order to measure topography and thus derive measurements for sea level all across the world. So that on the right there is just a visualization of sea level change measurements derived from Topex Poseidon and the Jason series satellites from when they were launched in 1992, all the way to pretty much now in 2022. So nice data continuation through each of these missions and a really, really groundbreaking satellite mission when it was first launched in 1992 because it was able to measure sea level to a high degree of accuracy. How do you say it measured sea level again? So with the, essentially with the ranging capabilities of radar, so it was able to say, okay, if my satellite is here, how far away is the ocean from where I am, and then based off that, get a measure of overall for the whole world, what the sea level measurement is like. I can see why, because when I first started to learn about this kind of stuff, it was a bit confusing, how can you actually get a measurement for where sea level is above land if you don't know where land is essentially. And so the kind of simple answer to that, there isn't one really, but the simple-ish answer to that is combination of a couple of different data sets. So these radar satellites, Topex and the Jason series satellites, they were kind of able to measure a, if something was water or land, and then b, kind of the topography of that water. They related that then to essentially what's called a geoid, which we talked about kind of way at the start of the semester, and is this very advanced, complex mathematical approximation of the earth, essentially. The way that we actually estimated our geoid, which is what we use then as a reference for measurements like this, for measurements of sea level, was actually this satellite here, which is called the gray satellite. That stands for the gravity, recovery, and climate experiment. So this was a satellite mission that measures gravitational pull across the surface of the earth. By measuring gravitational pull, we can measure the mass balance of the earth and then derive a geoid, which we would then go away and take these radar measurements ranging how far away water is from the satellite. Relate that to, okay, this is the shape of the earth that I have, and then based off that try and estimate sea level. Yeah. So how we estimate ocean depth is I referring to the geoid as like the thrust of... Exactly. Yeah, exactly. Similarly, there's lots of different ways, and there's definitely lots of different data combinations that you can go away and derive that kind of thing from. But generally speaking, ocean depth was first very accurately measured with the gray satellite here. By measuring mass balance, so it could estimate, okay, where is there the most mass of land? Where is there less mass of land? Assuming where there's more mass of land, those are things like mountains where there's less mass of man, those are things like really deep ocean trenches. So the gray satellite was capable of measuring sea level itself, but then we went away and combined that with those... these other radar satellites that we just talked about to get even more accurate measurements. The sea level measurements we got from grace weren't super accurate, but still were very, very useful for the time that the gray satellite was launched. We could also get estimates of freshwater storage, land mass storage, and as I mentioned, it was the main tool used to calculate the geoid, which was a really, really valuable application for a long period of time. So this is a video that kind of goes over grace and some of its applications specific to, I think, water. Around the world, water is constantly moving. From 2002 to 2016, a pair of NASA satellites with massive shifts in the brush roller-stroll on land related to water management, climate change, and natural cycles. The gravity, the power, rate, and climate experiment were race mission, whose precise measurements of the motions of two spacecraft in Earth's orbit to track the movement of water through the oceans, land, and atmosphere. NASA scientists combined grace data with satellite-based observations of precipitation and crop irrigation, climate model predictions, and other information in order to identify the causes of regional trends in freshwater storage. In this visualization, blues indicate areas with more stored freshwater than the average and more interesting red to know areas with less. The science team classified the major trends observed by grace as driven by natural variability, human activity, or climate change. For instance, the steady decrease in freshwater storage in Greenland is caused by the melting of glaciers, which drain water into the oceans. In the western United States, a long drought reduced mountain snowpack and river flows, causing heavy reliance on aquifers for crop irrigation and severe depletion of freshwater resources. Precipitation measurements together with grace data show how natural variations in the weather and unsustainable rates of water use conspired to deplete groundwater in California. In southern Africa, the O'Fongo Delta region experienced a huge increase in stored fresh water during the period of the grace mission. The science team analyzed precipitation data for the area and found that it was caused by a pronounced increase in rainfall. Between 2004 and 2012, the region saw about 15% more annual rainfall than during the previous 25 years. The rain ended a regional drought and replenished water storage in the area. In northern Saudi Arabia, grace detected a dramatic decrease in freshwater storing aquifers. Images taken by NASA's Landsat program show a rapid increase in irrigated crop land supported by water pumped from those aquifers. Most of that water is non-renewable on human time scales, but in 2014, the Saudi government entered a domestic meat farming program and grace data suggests that aquifer levels may be stabilizing. In northwest China, grace revealed rapid decrease in freshwater storage without an obvious cause. Scientists knew that mountain glaciers were melting, but the bell water did not leave the region, so they looked for another explanation. As it turns out, much of the region's surface water is redirected to agricultural areas and the desert to the south, where the death rates leaving the region with a net loss of water. The original race satellites stuck operating in 2017, but NASA and the German Research Center for Geosciences are partnering to launch a new satellite pair. Grace Follow-On in late spring 2018 to continue providing data about freshwater times around the world. So pretty different satellite there than what we've been talking about. I'll briefly, I'm not going to get too deep into it, just kind of a brief summary of how it works, but essentially it's two satellites that are really close to each other in space. They're orbiting the Earth kind of right next to each other, and then the distance between them changes depending on the gravitational pull between the Earth, well between the satellites and the Earth, and so they have a radar ranging system that measures the distance between each of them, between the two satellites, and depending on if they're closer or further apart, they can relate that to the gravitational pull that's underneath them, caused by how much mass there is on the Earth right below them, and then they can relate that to things like mass storage and then go away and derive that kind of information that they were shown in that video. Okay, two examples that we're going to end on that I think are kind of fun, kind of cool. One being observing coral reefs with Landsat, and then the other being looking at salmon habitat and freshwater systems with LiDAR. So coral reefs just as an introduction, if you're not super familiar with them, they cover less than 0.01% of the total surface of the Earth, but are home to 5% of the global biota. So global biota just means all of the world's plant and animal species. They're home to 25% of all marine species, so even though they're kind of a small pocket of habitat in the oceans, they're home to a large breadth of biodiversity. And increased sea surface temperatures, among other causes like pollutants and things like that, have led to increased coral bleaching. And bleaching just makes reefs really susceptible to stress and disease, and increases their possibility of death. So coral bleaching doesn't actually kill coral reefs, but it makes them really, really stressed, makes them very susceptible to dying in lots of other diseases. Coral reefs can bleach and potentially recover. Some coral reefs can bleach and then are just really susceptible and potentially could die. So it's really important to monitor in the case of reefs here when they are at risk of bleaching. Now, Landsat has emerged as a really key tool for mapping and monitoring coral reefs. So there's a variety of reasons why Landsat is a really good use or really good data set to apply for mapping coral reefs. Landsat 8 in particular. And the key reasons are that, A, we have that fine, moderate spatial resolution with Landsat, that 30 meter pixel size, which is ideal for something like coral reefs, because coral reefs aren't particularly big. They're not particularly large targets, so we need something that has a pretty fine to moderate spatial resolution in order to go away and map and monitor them. We also have a nice long record through time with the Landsat series of satellites, so we can monitor how coral reefs have changed through times, how trends of bleaching have changed through time. It's a free data set, anyone can download it. But most importantly, in particular with Landsat 8, one of the newer satellites in the Landsat mission and Landsat 9, they have these newer spectral bands that are really, really useful for monitoring coral reefs and monitoring other features in the ocean. But they've been applied really, really nicely for monitoring coral reefs. The key is the key band that they have is Landsat 8 and Landsat 9 have an extra band in the visible blue part of the spectrum. So usually in Landsat there is a red, green and blue band. In Landsat 8 and Landsat 9 there's an additional blue band. So there's a red, green, blue band and then another blue band that's kind of closer to the ultraviolet portion of the spectrum. So it's a slightly smaller wavelength size than the original blue band. And it turns out that that band is really, really useful for oceanic purposes and for monitoring things like coral reefs. These two satellites in particular Landsat 8 and Landsat 9 have an extra level of sensitivity related to something called radiometric resolution, which we don't actually talk about in this course. But for exam purposes just knowing that Landsat 9 are really useful for monitoring coral reefs because they have an extra level of sensitivity is sufficient. They also have a better signal to noise ratio, which just means essentially that the quality of the data is really, really high. So for all those purposes you can go away and with something like Landsat 8 get a really nice clear image of coral reefs and be able to check and monitor if they are bleaching and when they are bleaching. So this is just an example of a bunch of not Landsat derived data in this case, much lower spatial resolution data on a global scale using GOES satellites, which are weather satellites, as well as a couple other satellites by designed by NOAA to go away and monitor on a global scale at a very, very tight, fine temporal scale where we need to be worrying about coral reefs bleaching. So just an example here kind of going away a little bit from the Landsat 8 Landsat 9 example, but being able to use Earth observation data at a global scale at a really fine temporal resolution to monitor potentially when coral reefs are at risk of bleaching. Okay, last example that I am going to talk about today is mapping salmon habitat with LIDAR. This is something that is particularly new and kind of portion of research that I'm decently familiar with because two close colleagues of mine are really, we're really into this research and are still very involved in this research. So first off, just salmon in general are keystone species, which you kind of aren't familiar with, you know, salmon ecology. They are typically a sign of greater ecosystem diversity. So generally when you have a healthy salmon population, you have a lot of really high quality ecosystem diversity. They're really important in energy cycles, so they are part of what cycles nitrogen and other important nutrients from marine systems into forests and terrestrial systems. They're also really important species for economies because we eat a lot of them and for sustenance for some communities as well. There are particularly vulnerable species for several reasons, lots of stressors and threats out there, just to name a couple of climate change, lots of habitat degradation because of industry and other things being built up in historically kind of untouched streams and freshwater systems, as well as pollutions. So just, you know, oil spills, things like that that are spilling into these salmon habitats. Now what's really cool about LIDAR, again, that we've already talked about is that we can get that 3D information from LIDAR. So we can get information about forest structure. We can also get information about surface topography. And using that, we can map habitat units of salmon, which is really important for understanding salmon behavior and abundance. So LIDAR allows for efficient data collection and mapping of things like riffles, pools, caskets and glides, which we generally denote as different habitat units of salmon. So a riffle is kind of something that looks a little bit like this, where you'll see lots of bumps in the stream of the water. A pool is kind of a very calm, very deep section of a stream. A cascade is where you'll see kind of a waterfall type feature, and then a glide is similar to a riffle, but typically a little bit deeper where the water is flowing a little bit faster. But what we can do now is using something like LIDAR, we can, in an automated fashion, go away and map out all of the different habitat units in a given stream. Now generally, historically, and I mean it's still done to this day again to validate this data, and because some people don't have access to LIDAR data. But generally speaking, historically, if we wanted to map habitat units of salmon in freshwater streams, we would have to go out there, and I've done it myself. I've gone out there, put on waiters, walk around the stream, and literally with, you know, a piece of paper and a pen, kind of map out by hand where each of these habitat units might be. I kind of draw like a depiction of the stream by hand and be like, okay, here, there's a riffle here, there's a pool here, there's a cascade. So obviously not very efficient, takes a lot of time, very strenuous, not super safe because you've got to be walking through a stream. But what we can do with LIDAR data is essentially collect that same kind of information. So LIDAR data can use that terrain and structural information to map salmon habitat units with a really, really high accuracy, up to 96%, so that accuracy would be related to that field data, that validation data. So if I went out and I mapped by hand where all of these salmon habitat units were, and then I did the same thing, but with LIDAR data, the LIDAR data would match it at about a 96% rate. So this is an example here of the different kinds of data we know we can derive from LIDAR. So normalized elevation here, that's essentially our digital elevation model, that's telling us what the terrain is like of the stream, where kind of the steep parts are, where the less steep parts are. We got a measurement here of roughness, so based off the variation of the terrain, we can identify areas that are really, really rough, versus areas that are really smooth. We can also get things like canopy height, understory cover, and in-stream wood. These are all things derived more from the structural information that we get from LIDAR. So canopy height is essentially how tall are the leaves, or how high above the ground are the leaves on the trees. Understory cover is how much cover is there in the understory. The understory is just in the forest, the area below the canopy. So if you're looking at a forest, the canopy is kind of like the top layer of all the branches and leaves, and their understory is kind of the bottom layer of all the smaller plants that's below the canopy, below the really big trees. And with LIDAR, we can get a sense of, okay, how much understory cover is there, how much vegetation is there on the ground, and what's the height of a canopy, how high up are the leaves in the trees. We can also even derive things like in-stream wood. We can actually detect and measure individual logs and pieces of wood that are in the streams. And if we combine all of that information, the topography information, with the structural information of the forest in that area, we can go away and predict with some modeling techniques, where there's pools, where there's riffles, where there's glides, and where there's cascades. And that's where this map on the right is showing here. So it's just showing for a couple of different streams. This one's called Elk Creek, this one's called Gray Creek, this one's called Headwater Creek. The ALS, which just stands for Airborne Laser Scanner, which means LIDAR, derived habitat units. So the prediction from the LIDAR of what kind of habitat unit we can see. And then comparing that to the field measurements. So people that went out there and measured out manually where each of those habitat units were. And you can see it's pretty similar. It's not too far off the LIDAR from the field measurements here. So it's about a 96% accuracy in this example. This is something that's really new. So I think the citation there is 2022. So something that's kind of newly being done. It's a really cool new application of LIDAR. And an example of the really, really high resolution three-dimensional information you can get from something like LIDAR. And applying it to a really interesting example of looking at salmon habitat units and identifying where they are in a stream. Okay, so in summary, we talked about first sea surface temperature. We talked about how we can use MODIS and the thermal infrared bands that are onboard MODIS to go away and passively measure sea surface temperature. We then talked about the microwave sensor that's onboard the trim satellite. So it uses microwave bands to go away and still passively measure the sea surface temperature. But it relates to the 1 mm depth of sea surface temperature as opposed to the top 10 microns like thermal infrared does. We then talked about a couple of new satellites that we haven't mentioned before that can be used to measure sea level. So we talked about topics and the Poseidon as well as the Jason series of satellites, all radar satellites. And then the Grace satellite, which measures gravitational pull to be able to create something like a geoid and then be able to derive information on sea level. And then lastly, we talked about habitat characteristics. We talked about using Landsat to monitor coral reefs. And then lastly, we talked about using salmon or using LiDAR to monitor and measure salmon habitat units. Now, again, you've seen this slide before. Nothing new. Why is it useful for us to use Earth observation remote sensing data to monitor all these different features, all these different metrics that are throughout the oceans and fresh water? It's nothing different from what we've already talked about with the biosphere and with the cryosphere. We get standardized data with something like MODIS. We are getting the exact same data set. There's no subjectivity to it. When we were thinking about the salmon habitat units, for example, and you're imagining me out there kind of manually measuring out and drawing where all the habitat units are, that can be super, super subjective. Different people might do that in very different ways. If we just fly LiDAR over the area and we say, okay, we're going to fly the exact same LiDAR instrument with the exact same altitude, exact same plane, exact same LiDAR instrument, the data collection is standardized. It's going to be the exact same. We can repeat those data collections and get a nice standardized piece of data, rather than getting different people to go out there in the field and try and subjectively measure and denote habitat units for salmon. We have efficient data collection. We know we can get really, really quick data collection all across the world. Really, really good coverage as well. And then depending on the data sets we're talking about, we can get fine to moderate spatial resolution data with something like Landsat, super useful for maybe monitoring sea surface temperature of much smaller lakes or maybe monitoring coral reefs. We can get potentially very fine temporal resolution data, which maybe would be really good for monitoring something like daily sea surface temperature. And we know that we can get different kinds of spectral information. Really good example of that that we talked about today is the measurement of modus using the thermal infrared bands versus the trimmed satellite using the microwave bands. So the different kinds of spectral information that we can get from each of these different satellites allows us to derive different kinds of information. So all really valuable data sets, really valuable examples. Hopefully you're seeing a pattern by now, as I mentioned yesterday. Same topics. So applied in an exam, these are the things that I would want you to bring up. Again, just making sure that you're relating them to the specific example that I'm giving, whether it might be relating to fresh water and oceans or the cryosphere or the biosphere, or what we'll talk about in the future like wildlife or the human footprint. Okay. Got some example questions to go over. So I'll give, like I always do a couple minutes for you to go over those. Evan and Leanna will be here in a couple minutes. So we will go over the answers to these in maybe three, four, five minutes. And then I will give it to Evan to give his reminders as usual. And then I will let Leanna give any tips that she wants to give. And you're welcome to ask her about the assignment that's due this week, if you would like. If you want to come down and chat before I go over these and ask questions or anything, you're welcome to. If you want to head out, you're also welcome to do that. And yeah, that's pretty much it. Yeah. Yeah. I think they, I definitely seen some imagery and some articles where they have and they can. I'm not sure of which satellites those are off the top of my head. Happy to look into it for you and get back to you. But definitely something that I'm sure, because the, I've definitely seen some images in the news and stuff of those big patches of garbage in the oceans. I think they're pretty massive. I don't know off the top of my head, to be honest. I haven't looked into it very, very much before. I'm happy to. I'll check it out for you. I'll let you know. Yeah. Okay guys, we can go over these answers and Evan and Leanna will be here in one second. So how was sea surface temperature measured historically? Well, how do we measure sea surface temperature before satellites? Still, still use it now, but. Yeah. Boys. Yeah. Exactly. And is that still used today? Kind of just gave the answer. Yes, no. Yes. Why is it still used today? What do we still need boys for? Boobies, boys, depending on where you're from. Yeah. Exactly. Yeah. To validate our satellite measurements, to make sure that we have a level of accuracy with our satellite measurements, we can check how true our satellite measurements are. So for validating satellite data, it's kind of the key term you'd want to use there. Great. Awesome. So what is one advantage of using satellite data to monitor sea surface temperature when compared to historical methods? We just talked about. So boys, what's one method or one advantage, one reason using satellite data to monitor sea surface temperature specifically is advantageous when compared to say using boys. Yeah. They can cover a lot more area. Absolutely. Yep. So we have a ton more area. One other kind of really key point that we mentioned is the difference between not just the coverage as a whole, the amount of area it covers, but the type of measurement it is. With boys, we have point measurements. So you're only measuring one single point with satellites. We're measuring areas. So much larger regions of measurement. Pretty similar to what you mentioned, but a little bit of a distinction there. Both would be totally acceptable answers. What was the purpose of the Topex Poseidon and Jason series of satellites? Why were they launched? What did they do? Primarily. Yep. Measure sea level rise. Measure sea level rise. Exactly. Measure sea level as a whole. Pretty much. Yep. What is a newer or useful satellite that's really good for coral reef mapping? Yeah. Yeah. Landsat. Which Landsat? Specifically. Landsat 8 and 9 in particular. Yeah. Exactly. And then lastly, really straightforward, talked about it for like 10-15 minutes. What is an example of freshwater habitat mapping using LIDAR? Yeah. Like mapping like seven? Yeah. Mapping. Not the salmon themselves. Yeah. Kind of sort of. You want to be careful with that. I've had, you know, be careful with your terminology around that. So the LIDAR that we talked about is measuring and kind of mapping out habitat units of the salmon. So you're not quantifying the habitat that the salmon is in. It's not necessarily saying this is where the salmon are and this is where they aren't. It's kind of helping to predict where they might be, but that's done by mapping out their habitat and quantifying their habitat. Does that make sense? No, actually counting any salmon. No. That's an important distinction that I think a lot of people get mixed up. Yeah. Yeah. And if I can count salmon with LIDAR, PhD done. That would be amazing. It can't do that. Anyway, sorry. You could jump in. But yeah. So you're quantifying the habitat essentially. I had someone recently also kind of do make a similar or just not make that distinction as well when I was talking to them or when I was teaching about how you can use satellites to monitor and quantify bear habitats where they said, oh, we can use Landsat to see where bears are. And that's not in a way it gets there because you can use Landsat to quantify habitat to then be able to predict where they might be, but to know where they actually are, you need some sort of actual movement data. So in the case of salmon, to know kind of where the salmon actually are, maybe you put tags on them or something like that so you can actually see where they literally are. Yeah. That makes sense. Yeah. Okay. I will give it to Evan. Quick couple of announcements. Do you have any. Science? Okay. There's also a bathometric ladder and you can see alligators. Which one? Bathometric ladder. Sorry, which one? That's today, right? Bathometric ladder. Yeah. Nick, those refuses to buy the batch metric ladder so we can't do that. All right, pretty short stuff for me today again as always. This week, assignment four is due on Thursday. I was going to talk about that in a bit. Next week, assignment five will be due on Thursday. Sam Rubinger is going to come in. Next week, we can talk about that. Next slide. Last office hours left for assignment four, which is with Leanna, is on Thursday, they it's due at two to three p.m. Other than that, last thing I want to remind you about is to check over your mid-term and email me if you have any questions, comments or concerns. If you have it on your laptop, you can also come talk to me now. If you do email me, please include the question number as you're concerned about and some way for me to identify you in Canvas. So your name, that's in Canvas or your student number. Also, today is the last day that you should be doing this. After this time, it gets a bit more iffy on whether or not we're going to do the grade changes because we told you there's a two week. We're not doing the grade changes after today. Some of your emails come say hello to me. That's it for me. Leanna's going to talk about the assignment. I have no tips. You guys are doing very well. But if you have questions, I will be here. Okay. Awesome. Thanks, guys. How do I stop this? All right, hi guys. Welcome. Getting close to the end of the semester now. So welcome to the final grind. Basically, things aren't too stressful for you guys. If they are, then you're getting close to the end. So a little bit of a light at the end of the tunnel. For this week, you should be working on assignment five. So let's do Thursday. Then it's a quick turnaround for assignment six, which is due next week on Thursday as well. Sam, who is your TA for assignment five and assignment six, will be coming at the end of class today to talk a little bit about assignment five. And just be available if you have any questions about assignment five. And then he will also post assignment six. It's not published yet. So you can't see it on canvas yet. But he will do that tonight at some point. So he's just making a little bit of an intro video to go along with assignment six. So he'll post that with the submission, with the assignment tonight at some point. So you can get started on it early if you'd like. And then you just have assignment seven after that. The next blog post, I believe, is blog post five. Is that correct? Does anyone know? Yeah. So the next blog post is blog post five, which will be due in a couple of weeks. We're going to talk about that blog post in class. And I'll give you some more details and context for it in class. And then you can go away and do it after that. So it will be published, aka, you'll be able to view it as soon as we talk about it in class, which will be coming up in a couple of weeks. I'll show you the schedule for the next couple of weeks and let you know when that will be. So assignment five this week, assignment six next week, assignment five office hours are this week. So the four to five set office hours will have just happened for Sam. And then he's got two Wednesday sets of office hours, a Thursday set of office hours, all for assignment five. So that is office hours and assignments for this week. Today we are talking about monitoring wildlife with Earth observation remote sensing, which is super fun for me because that is kind of what I am at least the closest to an expert in. But quickly before we get into that, just a schedule for the rest of the semester. Today we're going to talk about wildlife. Tomorrow we're going to talk about the human footprint. Next week, Monday, Tuesday, we'll talk about environmental change and then the future, which will focus mostly on drones. And then April 3, which will be the week after that, we'll talk about Canada from space. We'll talk about eco zones. And that's the lecture that I'll kind of introduce the blog post, blog post five that you'll be working on from that lecture. So it won't be posted until that day. So April 3 at 5 PM on the Monday will come in. I'll give a lecture about Canada from space. And simultaneously blog post five will be published. It's a quick turnaround. So it'll be due that Thursday. So you only have about four or five days to do it. But it won't be posted till we talk about it in lecture that day. And then following day, Tuesday, April 4 is our final exam review. That is our last day of class. So the week after that, the Monday is a holiday. Technically, the Tuesday is still class time. We're not going to have class on the Tuesday. So take it. I should probably say, take it to study for the final exam or do whatever you would like with it. Take a break. Get outside. So that's our schedule for the end of the semester. Any questions about that? OK, sweet. OK. So today we are going to talk about wildlife and how we can monitor wildlife with different Earth observation technologies. So we're going to talk about what wildlife is, why it's important to monitor it, how we've monitored it, historically. Again, all very similar format to our last couple of lectures. We'll talk about some remote sensing technologies used to monitor wildlife, specifically how we can track animal movement, how we can quantify animal habitat, and then how we can combine animal movement data and animal habitat data derived from satellites to answer questions related to conservation and wildlife management. So we'll talk about what kinds of questions we can ask, what kinds of questions we can answer, and then we will focus on a grizzly bear example, because I study grizzly bears a lot, and grizzly bears are just fun and cool, I think. So brief introduction, the term wildlife just refers to any undimesticated animal species, so any animal that lives independently of people. So dogs, cats, household pets, not considered under the umbrella of wildlife, but any undimesticated animal typically includes mammals, birds, fishes, amphibians, reptiles. We have a really interesting, fun assemblage of wildlife here in Canada and here in British Columbia. So we have beavers, bears, caribou, eagles, links, orcas, a really, really interesting variety of wildlife to monitor, to conserve, to manage for, and make sure that they have enough sustainable habitat. So there's lots of reasons to go away and monitor wildlife. One of them is ethical. There's an intrinsic value and need for a lot of people to preserve wildlife species for future generations to be able to view and enjoy. So there's this intrinsic value in life in other wildlife species. There's political reasons that we have to conserve and monitor wildlife, oftentimes species that have low population demographics or are threatened or endangered are listed on politically in certain countries and certain jurisdictions. We in the province of BC have a list of endangered and threatened species. In the country of Canada, we have a list of endangered and threatened species. And there's also worldwide lists as well. A lot of those lists come with political thresholds and steps that have to be taken by people working in those industries in order to conserve habitat and conserve those species. So that's kind of the political aspect of it. Sometimes we have to because according to the law, if certain species are under certain lists, then we have to do a certain amount of work to conserve their habitat. Culturally, some local communities have identities that are intertwined in different wildlife species or may hunt wildlife species for sustenance. And then lastly, there's oftentimes an ecological reason to monitor wildlife because different species can serve very key purposes in ecosystem functions. Beavers, for example, is a really obvious one like that. They will build these big dams and rivers and lakes kind of alter the whole function of an ecosystem. And without that, with that, the ecosystem can function very, very, very differently. So we'll talk about these kind of as an example later when we talk about grizzly bears as a specific example. But lots of different reasons to monitor wildlife. Now, the actual practice of monitoring wildlife, not just conserving them but just going out and collecting data to learn about them, allows us to track population trends and identify threatened and endangered species. So by us going out and monitoring species, saying, OK, how many are there? Are they healthy populations? Are they not? We can monitor their trends through time and identify if there's threatened or endangered species. The kind of semi-recent addition of Earth observation remote sensing data allows us to, again, similar pattern from what we've talked about in other lectures, allows us to efficiently collect data on animal movement and efficiently quantify animal habitat. And we do that to ultimately try to make inferences on why species may be threatened. If we see that a particular species is all of a sudden declining, then we can collect very efficiently animal movement data so we know where they are, and then try to relate that to remote sensing derived animal habitat data to understand why they're there. And by doing that, we can ultimately try to figure out why their populations are maybe declining. So ultimately, in the kind of biggest picture, at the end of that process, we can go away and make some sort of management decisions, change something about our policies, about our laws, about the way that we operate in order to hopefully better conserve that species and provide a better habitat for them so that they could continue to thrive. OK, so there's two key aspects to this that we're going to talk about today. That the first being tracking animal movement, the second being quantifying animal habitat. So tracking animal movement, we're going to talk about the use of GPS collars and GPS tags. So GPS collars are just collars that are put on a species, on an animal. Tags are a very, very similar thing, but they are often used on birds. And we'll also talk about camera traps lastly, which are technically a remote sensing device, but don't really have a space element to them. Then we'll talk about quantifying animal habitat. So we'll first talk about how do we track animals, how do we know where they are, and then we'll talk about quantifying animal habitat. So trying to figure out if animals are in forests or are in wetlands or are in urban areas, trying to figure out where they are by describing their habitat. And we'll talk about the data that we often used to do this, focusing mostly on land status as a specific example. So historically, before we add GPS collars, before we had Landsat data, how would we track animal movement? Scientists would generally go into the field, place themselves in some safe position, or use an aircraft maybe, and just quantify animal movement and behavior for hours, days, or weeks. So this would look literally like me going out and to say this field here, full of bison or buffalo, whatever I'm studying in this case, going out, standing at a distance that's maybe not too close to them, but close enough so that I can see them and try and see what they're doing. And then just having a notepad piece of paper and just writing down, okay, that one over there is, yeah, it is sleeping, yeah, that one over there, it is eating grass, yeah, okay, that one over there, it's doing something else, looks like maybe it's taking a poop, I don't know, it's hanging out. But that's how it would look like, by hand, just drawing down what each of these individual animals were maybe doing. Now obviously that's not very efficient, highly susceptible to human error, the way that I classify or maybe document how a particular animal is behaving could be very, very different from how someone else maybe classifies how that animal is behaving. I could sit there and say, okay, it looks like that animal is maybe being quite aggressive right now, and then someone else might sit there and look at the same animal and say, yeah, that animal is just relaxing, they don't really care about anything. So highly, highly susceptible to human error, lots of different people can quantify animal movement manually very, very, very differently. However, these field expeditions were still important, they are still important today, because they provided the first data sets on animal movement and animal behavior. We only have, you know, similar to most remote sensing, satellite derived data, we only have animal movement data from, say, collars and camera traps and tags, maybe back to the 90s and 80s when those technologies started to be developed and implemented a lot in practice. And these field surveys also allowed a quantification of behavior, something that's really hard to do today with collars and with camera traps. So you could say, by looking at a species, okay, is it sleeping, is it moving, is it being vigilant, how scared does it look of potentially being eaten or attacked by another predator? Is it feeding? We have a really tough time nowadays, getting that kind of information from, say, a collar. A GPS collar tells us exactly where an animal is, maybe every 30 minutes, but it doesn't tell us whether that animal is sleeping or moving or how vigilant it's being or whether it's eating. We can sometimes try to infer that kind of information based off of its movement patterns, but again, it's not very reliable. So these kind of historical field surveys still hold a lot of value and sometimes are still used to this day for particular applications. Now in terms of using kind of more modern remote sensing, earth observation technology to track animals, there's two key approaches, a land-grangian approach and a eularyian approach. So a land-grangian approach uses an animal-borne tracking device, a eularyian approach uses a predetermined frame of reference. So a land-grangian approach would include a GPS collar, so something like this, or a GPS tag, where the animal is essentially wearing the tracking device. So there's the collar on board our animal here. The eularyian approach, the predetermined frame of reference, might include visual surveys, so just like those historical surveys that I just talked about, a predetermined frame of reference just means you pick a particular viewpoint, you're always looking at that viewpoint. So all you can really say is if there is an animal in front of you or not. So a visual survey is a really good example of that, you stand at a particular point, you look out of the landscape, and you're just looking at that one particular area. All you can see is the animals that are within your viewpoint. You can't see any others. So that's a predetermined frame of reference. Camera traps also use a predetermined frame of reference. So these are often remotely triggered cameras that are just mounted on trees, or something else that's maybe out in the forest or on the landscape, and they can get really cool photos like this, generally at night and during the day. But again, a camera is just mounted on a tree, looking at a particular frame of reference, and just telling you whether or not there's an animal in front of that particular camera. Whereas a collar, a land-grangian approach, is mounted on the animal, so it tells you exactly where that animal is, no matter where it is. So a land-grangian approach will give you a data set that looks like this. If this circle here is an animal, then you can track at a very fine scale exactly where that animal is, all through space. In this example, in this eularyan approach, maybe using a camera trap, if these squares are cameras mounted on trees, and they're all looking downwards, all looking in this direction here, then essentially this camera, will say, okay, yeah, there's an animal right here, I see an animal right in front of me, this camera will say there's no animal, this camera will say there's no animal. So the eularyan approach here has a predetermined frame of reference, it's always looking at the exact same position, or exact same area, land-grangian approach, can track the animal no matter where it is. There's pros and cons to each of these approaches, which we'll talk about in a moment. So with GPS collars, we put GPS collars generally on a wide variety of species, I've mostly focused here on Canadian and British Columbia species that are important, and that are of value in terms of wildlife management, wolves, moose, bears, caribou, deer, there's a wide wide wide variety of species that we will put GPS collars on. So a GPS collar might look something like this, this is called a Telvit GPS collar, this one in particular is used on grizzly bears, and they have an accuracy of about five to 10 meters depending on the terrain, so they're accuracy similar to any GPS device whether it's your phone or some fancy schmancy GPS device, it's gonna depend on potential obstructions, potential errors that we've talked about a little bit in this course. So if you're in a deep, deep valley, or if a bear in this case, is in a deep, deep valley, surrounded by a really, really dense forest, it's gonna have a much lower accuracy. If it's say at a really high elevation, in an alpine meadow on the side of a mountain where it's really clear, it's gonna have probably quite a good accuracy. So the accuracy of the collar kind of depends generally on the terrain as well as some other errors that could potentially influence it. Today we can usually customize how often we get locations from a collar, so for example, we might get a location from a collar mounted on a bear once each day, once each hour, maybe once every 30 minutes, the finest you can see, generally is most often at a 30 minute scale, if you really, really wanted to, you could get locations every five or 10 minutes. Now, there's a bit of a trade off between how often you get locations and how long a collar might last. If you get locations, say, every five minutes, every 20 minutes, every 30 minutes, the collar might only last a couple years because the battery will die. If you're getting locations maybe only once every hour or a couple times a day, then the collar might last years and years and years, maybe even a decade or two, so it'll last a lot longer because the battery will last a lot longer. Ultimately, the collars are removed so they typically have a rot off feature, so that is on this one kind of, this portion of the collar here, and all that means is, after a certain amount of time, after a couple years, that portion of the collar will literally rot and just fall right off the animal. Oftentimes, there's sometimes an electronic mechanism instead, where after a specified amount of time or after the battery on the collar dies, it just pops off, falls on the ground. Tags are very, very similar to collars, they operate in the same way, like any GNSS GPS system, but they're generally just a lighter weight version, so they can be attached via rings around bodies or necks or sometimes the legs, so there's one that's attached to a leg, and then generally, they work like this where there's kind of a small string that goes around the neck of the bird, and then it will often just sit on the back of the bird like that. But same as the collar, it uses GPS, GNSS technology, to just figure out exactly where the bird is, you can get transmissions or locations every couple minutes if you really want, but generally, once every hour, once every two hours, maybe once or twice a day. Okay, so a couple advantages and disadvantages of collars, collars and tags generally give you really, really fine scale level movement data, so you can get potentially a location of an individual animal every 30 minutes. It's remote data collection, which means that if you are a scientist and you have collars out on a bunch of species, then you'll just be sitting at your computer, and every couple of days or every day, just remotely, the data will transmit from the satellites down to your computer. So you don't actually have to go out and retrieve the data. Disadvantages include that GPS collars only provide individual level movement, which just means that if I put a collar on an animal, I'm tracking that single animal, not tracking a population of animals unless I happen to collar an entire population, which is generally not feasible. So typically, you might collar in a single project, say with bears, maybe if you're lucky, three, four, five, six bears, so you're taking a sample of the actual population. Over a given area in BC, maybe there's a couple hundred bears, and you get to collar maybe three, four, five, so you have to hope that the ones that you call are representative of the behavior of the whole population, which they may or may not be. Yeah. It's not a curiosity, how do you put a collar on there? So generally, they are tranquilized from helicopters. So someone flies around in a helicopter, they tranquilize the bear, they go down, find the bear, they'll do a bunch of measurements like the size of their teeth, the size of their neck, the size of their paws, things like that. Then they'll put a collar on the bear, and then after about 45, 60, 90 minutes, depending on how strong the anesthetic was, the bear will wake up, walk away. Because of that, because of that whole process, they're also really expensive, callers in general. You have to pay someone to fly a helicopter, you have to pay someone who's legally allowed to tranquilize a bear, you have to pay someone who's legally allowed to handle bears, there's lots and lots of laws and regulations about that in Canada, so it's not a very easy process, they're very expensive. The collar itself is expensive, the process of doing the coloring is expensive, but also they're frequently considered even more expensive because of how often they fail. An individual GPS collar is maybe anywhere from two to three to $6,000, and if it fails, if it just happens to stop working, then all of a sudden, all the money and resources you've put into getting a pilot, into tranquilizing a bear, into putting a collar on it, et cetera, et cetera, all that money's gone to waste if your collar just fails, which happens more than you would think it would. And then lastly, another disadvantage is that it is an invasive process, you do have to get right up close and personal with a bear, generally the bear's asleep, but regardless, it is still an invasive process. So this is an example of a data set you might see from a GPS collar project, this example is looking at Wolfpax in the Midwest, believe it's in Minnesota, and again, you can see here that really, really fine scale level of movement data. You're getting locations here multiple times a day, and this is an example where they were really clearly able to map out different Wolfpax. So wolves are very territorial, and you can see here this pack of wolves stayed in their territory, this pack of wolves stayed in their territory. So this project, which was called the Voyager's Wolf Project from Minnesota, they were able to really, really accurately map out the different territorial ranges of each of these Wolfpax. But that's just an example of how a GPS collar data set might look like. You get essentially a point in space that has a timed stamp on it, just related to whenever that location was collected or measured. Okay, quick video here showing an example of the use of collars but outside Canada, outside of BC in Africa. We can look at this on a page around this event, the migration of the world of east. The alliance of the Voyager's games, the Massasana National Park. These are the movements of 12 world of east, representing the 30,000 world of east in the Lord's land. World Middle Kingdom of New York, and the Washington political church.Insp spa, May our Okay. The second kind of animal movement data collection we're going to talk about is from a tool called a camera trap. So a camera trap will often look just like this. There is a infrared sensor here, a digital camera, a flash array here, and then it's often just in this weatherproof case. And how it works essentially is a camera trap will detect something that is moving and warm in front of the camera frame. So something in this case, you know, they're showing a wolf in this image here. If something that is warm, which would be detected by the infrared sensor here and is moving in front of the frame, then it'll snap a picture of it. So this wolf is outside of the frame of this camera. It's not going to take a picture. If it starts moving into the frame, it will take a picture. If it sits in the frame and it's completely motionless, it won't take any pictures. Then if it moves again, it'll take another picture. So this is kind of, you know, the infrared sensor image. But generally you just get a normal RGB kind of plain digital image looking something like this of whatever point of reference the camera trap has, whatever area it's looking at. So it's remotely triggered by moving warm objects. In this case, this camera trap here is mounted on a tree and it's just looking at boom, this general area here. So anything that's warm and moving within that zone will trigger the camera and it'll take a picture. So couple of advantages and disadvantages of a camera trap versus a GPS caller. One nice thing about it is it's non-invasive. So you don't have to get up and close and personal to a bear or any other wildlife species. You can just go out in the field, set up the camera on a tree or some other kind of post-type thing. It just sits there and just takes pictures. It's cost effective. It's not super expensive. A camera trap is maybe three to four to five to six hundred dollars per camera. Maybe even cheaper than that. Sometimes you can get as low as a hundred and eight to two hundred dollars. While a caller might be upwards of six thousand dollars per caller. Plus all the resources needed to get a pilot to get someone to tranquilize an animal, et cetera. So much more cost effective. You do get a population level data sample, which means that rather than with callers where you put a caller on one individual animal, then you're tracking that one individual animal. With camera traps, you're effectively measuring or sampling the entire population. No matter which individual animal it is, if the animal moves in front of the camera, it'll take a picture of that animal. So in that sense, you get more of a population level data estimate. It's multi-species. So no matter what the species is, that's in front of it. It'll take a picture that includes birds as well. So it'll take pictures of birds, squirrels, hares, wolves, deer, bears, people. It's a really nice way to also get an estimate of the human use of an area because they'll also always take images of people. And sometimes from camera traps, you can derive some behavior information. Sometimes from a photo or a sequence of photos, you can say, okay, that animal is feeding, or that animal is vigilant because it looks like it's moving really, really fast, or that animal's, you know, maybe you can see it's sleeping or something like that. Either way, you can't get that kind of level of behavior information ever really with GPS callers. Unless you're trying to infer based off how frequently something is moving around, whether it's resting or feeding or whatever it's doing. But it's possible to get sometimes behavior information with camera traps. Now, this advantage is, so you get a much coarser level of movement data. Say you put one caller on a bear for a summer, and you are collecting a location of that bear at a pretty coarse level, maybe just one location of a bear a day. You're still guaranteed for say three months, 90 locations of that individual bear. If you put, on the other hand, say 50 or 60 cameras out over an area that's 500 kilometers squared, you may be, if you're lucky, we'll get 200 or 300 detections of all of the bears in the entire area. So one caller potentially gives you hundreds and hundreds and hundreds and hundreds, maybe thousands of locations of an individual bear, whereas a camera trap might only give you a couple locations, maybe a few hundred if you're lucky, of all the bears that are living there. So much coarser level of information. Also require, require manual data retrieval. You have to go out into the field to the camera traps, to get the memory cards and bring them back to your computer. It's not like with satellites, where the information is just sent remotely to a satellite, then downloaded to your computer. Camera traps also have the potential to be affected by weather. So sometimes in the winters, especially in a place like Canada, camera traps that are mounted on trees are just fully covered by snow, so you can't get any data in the winter, or sometimes in tropical areas where flooding is really frequent, then the camera traps are just fully covered by water. They'll generally still be preserved once the flooding goes down, it'll still take photos, not always, but most of the time they're pretty weatherproof like that, but you're not going to be collecting data when the whole area is covered with water. Can't take any images of any species that are walking by when it's flooded like that. Other kind of plus fun thing about camera trap data, you do get lots of really fun images, so this is from an area in southwest BC, about twice as far north from Vancouver as Whistler is, so if you just drive straight north to Whistler, then you drive another four hours or so, six hours, four hours this, north from Whistler to an area called the South Chocote Mountains, that's where these images are from, so these are from the South Chocote Mountains Provincial Park. We got some really, really amazing imagery here of this very curious grizzly bear, a couple of grizzly bears that were fighting, and then this one is this hair sprinting away from this weasel or other kind of weasel-like species, so you can see these are taken right after one another, 31 seconds, 39 seconds, this guy probably didn't make it, not gonna lie, but yeah, some cool imagery, fun stuff to look at, yeah. Yeah, totally, yeah, so usually it's associated with some sort of sampling design, typically it's a systematic design where if you have an area of interest, then you'll systematically place one camera at a specified distance from one another, so that they're kind of equally spread out, sometimes you might just randomly place them, sometimes you might apply a stratified random sample where you say, okay, I know there's these three kinds of habitats that exist in this area of interest, so I'll put 10 camera traps randomly across each of the three habitat types that exist, so it really depends, there's lots of different ways that you can do it, but it's definitely true that you might put up a camera somewhere and you might not get images of anything, which is, you know, one of the downsides of using a camera trap, if you put a collar on something, you're gonna get location data no matter what, if you don't, if you put up only, say, 10 cameras, you might not get much, generally you typically in a camera trap project want to put up at least 50 or 60 cameras, but the more is always the better, most projects aim for 70, 80, maybe even over 100 cameras in say an area, ranging from anywhere from 300, 400 kilometers squared to a thousand plus kilometers squared, yeah, does that help answer, yeah, do you have a question? For cold-blooded animals? Like, do you have an example? Like, like, snakes? Like, yeah, something that wouldn't be warm, that wouldn't kind of be triggered by the, so, generally, I mean, in the case of Canada and BC, we don't have a lot of species like that, at least not ones that were particularly interested in studying, so we don't have that issue too much. I'm honestly not exactly sure, off the top of my head, my gut would say that just the motion trigger itself would be enough to trigger the camera to take a photo, so even if it's not super warm, just the motion of it in front of the camera would probably allow it to still snap a photo, we get a lot of false triggers sometimes with camera traps because we'll set it up in an area that's really windy, and even just the plants that are swaying across the image will trigger it to take tons of photos, so it probably wouldn't be an issue as my guess, yeah, but yeah, good question, haven't thought about that before. Any other questions, yeah? Take, take like videos? Yeah, some of them can take, well, yeah, so the, the story answer is yes, some of them can take videos, generally, there has to be a quite a specific niche reason to do that, because it's hard to know when exactly to record a video for. You could in theory always be recording a video, but the SD card, the memory card is going to fill up really, really quickly if you're constantly recording video, so typically, you know, if you wanted to get, say, a nice video of a species interacting or doing something in front of the camera, you'd have to know that the species was going to be there, so maybe you'd be kind of hidden, you know, in camouflage, like waiting for a species to come, and then once you saw it then like triggering the video to start. The kind of more common thing that you see that's not really a video is you'll see image sequences just stitch together to kind of create a video, so if an animal is spending a lot of time moving right in front of the camera, you might get a photo every second or every two seconds, so if you stitch that all together, it kind of looks close to a video, but, but not quite, and then the last thing that you'll often get sometimes is the ability to get time-lapse videos, so a lot of these cameras, even though they're motion triggered, they'll take a photo every single day at the exact same time, say at noon, and then you can stitch all those together and get like a time-lapse of that whole area, yeah, does that kind of answer your question? Any other questions? Cool, okay, so up till now we talked about just how we collect animal movement data, specifically with collars, specifically with camera traps. Now how do we actually quantify an animal's habitat? We need to relate where they are, where the collars, or where the camera traps tell us that animals are, to information about the environment and about their habitat, to ultimately try to answer questions like, okay, we know an animal is here, but what, based off that, what kind of habitat do they actually prefer? Where are they most commonly located? And what do they avoid? What do they not really like to spend time around? And that ultimately allows us for management to make decisions, like for example, well we know that this particular species really likes this area, and that particular area has a lot of hiking going on in the summer, but that has the potential to displace those wildlife, to displace those animals, so let's maybe close that area down for hiking for people in the summer because we know it's an area that this particular species that we're concerned about really likes. So we can try and derive information to allow us to make these management decisions by asking these questions like, what kinds of habitat do animals prefer, and what kinds do they avoid? Now ultimately today, we do that mostly with satellites, but we always talk about in this course what we did before satellites. So historically we quantified habitat manually, you went out into the field, you went out to a site that you knew an animal might spend a lot of time in, and you'd look around and try and quantify the habitat just manually. So you might, you know, look at your GPS caller data, for example, and say, okay, it looks like this animal is spending a ton of time around this particular area. Let's go survey that area, let's walk around, let's do some hiking, let's maybe fly a helicopter or a plane over it, and try and take some notes down about it. Is there lots of coniferous trees, is there lots of deciduous trees, is there lots of open space, are there lots of water sources, try and quantify some data related to why that species may or may not like it there. With camera traps, you know, very, very similar, but a lot kind of easier to pick an area that you might want to go survey. You have all of your cameras maybe set up all across the study area, you're getting a ton of photos of one particular species at a couple of cameras or at one particular camera. So you can go right out to exactly where that camera is located and kind of just survey, look around, get a sense for what the terrain's like, what's the slope, what's the aspect, how many plants are there, what's the vegetation cover like, what kind of plants are there, are there a lot of food sources, are there roots and berries and plants that a particular animal might really like to eat there, but you do that all manually just by going out there and just jotting down on a notebook what you see. Sometimes you might do it like I mentioned from a plane or from a helicopter and you'd literally have just someone sitting in the plane or helicopter with a, you know, piece of paper where they're just manually mapping out all the different types of habitats that they can see on the ground. Again, subjective, not very efficient. So let's, we're going to use then to kind of wrap our heads around these kind of questions we want to answer and our ability to combine animal movement data and satellite derived habitat data. We're going to look at an example because I think it's fun. Hopefully you do too. We'll see about grizzly bears. So let's say that you have GPS caller data. So in this case, this is showing a bunch of GPS caller data from southeast BC and kind of northwest ish USA. So all of these points here, all of these individual dots you can see on this map represent a discrete location from an individual bear. So the different colors here represent different individual grizzly bears. And each dot represents a location where that GPS caller found that bear to be. So let's say you have 18,000 GPS telemetry locations on 13 grizzly bears. What do you actually do with this data now? You have all these points, you know where these bears are located in space. But how do you know what habitats they like, what they don't like, how can you make some management decisions based off this data? That's where this satellite habitat characterization comes in. So we still require field work generally to validate satellite data. So we'll still go out to much fewer sites, but still a couple of sites to ensure that what we're looking at from our satellite data is true, that it's actually accurate. But overall, we can with satellite data efficiently get very informative information about habitat over pretty large areas. This I'm showing an example of is from habitat characterization of rhinos. So not a Canada BC example. But you can see here for the study area, we have information about land cover. So maybe that was derived from some passive remote sensing system like Landsat or like MODIS. We have aspect here. We can derive that from a digital elevation model, which maybe was collected with a LIDAR instrument or with a radar instrument. We have information about slope here, again derived from a digital elevation model potentially from LIDAR or radar. We have information about precipitation here. Maybe that was derived from the thermal microwave imager on the trim satellite that we talked about when we talked about oceans and fresh water. We have forest cover here. How much forest is covering a particular area? Maybe that was derived from Landsat or MODIS. So we have all of these different kinds of satellite data that we've talked about throughout this course that now we can derive real information about in the form of habitat characteristics to try to understand, okay, we know where an animal is located. Is it located in areas that have high forest cover or low forest cover? Is it located close to human settlements or far away from human settlements? Are they found to be very close to roads or do they like to avoid roads? Do they never really come anywhere close to roads? We can get all of that from satellite data. So we've talked about in this course, we've looked at this map a couple times. This is a Landsat-derived 30 meter spatial resolution land cover map all across Canada. So we can take information from GPS collars about bears, about wolves, about some species of interest, look across this land cover map that's out of 30 meter spatial resolution covering pretty much all of Canada and say, okay, well, do these species seem to prefer shrubbed areas or wetland areas or coniferous areas or broadleaf areas? Do they like to be located close to water? How do they feel about snow and rock? So all of this kind of information we can get from combining that animal movement data and that satellite derived, in this case land cover or habitat data. We've also briefly talked about the ability of Landsat to quantify disturbances. So they tell us where forest fires are occurring, where forest harvest is occurring, where areas are being converted to agricultural land. Again, similarly, we can then look at our animal movement data and say, okay, are these species ever located close to areas that are burned or do they always stay in areas that have really dense, healthy forests? Are they ever located close to agricultural areas? These are the kinds of questions we can try to answer by combining these different data sets. So we're going to look at specifically, as I kind of mentioned already, a grizzly bear example. So grizzly bears are often a focal point of wildlife conservation, particularly in western North America. For a variety of reasons, ethical reasons, their intrinsic value, political reasons, for a period of time they were listed as a threatened or endangered species in both Alberta, British Columbia and Canada. They are still kind of listed as endangered species in the states. They also have a lot of cultural value to a lot of local communities. And oftentimes they are considered an ecological keystone species or an umbrella species because they can really influence how an ecosystem functions. And there's such a wide ranging species, they'll be found in so many different areas that oftentimes if you are conserving or preserving grizzly bear habitat, you're simultaneously just conserving and preserving habitat for a bunch of different wildlife species because grizzly bears are found in such a wide variety of areas. So let's say we have GPS caller data from some grizzly bears, we want to take some Landsat-derived habitat information about land cover, about disturbances, to try to answer some questions. So we know when and where fires have occurred, we know when and where logging has occurred, and we know when and where grizzly bears are located. So let's look at this example here, this is an example from Carni et al. 2019, so it's a publication that's pretty recent a couple years ago. And they were interested in looking at how grizzly bears in west central Alberta in this case. So this is just a map of Alberta here, this is the study area that they looked at right here, kind of outlined in red. They were interested in how bears might be responding to logging and other disturbances that were going on in west central Alberta. So we have here our top map showing location density. This is derived from our grizzly bear GPS caller data. So the kind of yellow areas represent high density areas where lots and lots and lots of bears were found to be according to the GPS caller data. And we have these kind of bluer areas around here, these areas represent low density. So there were never really many grizzly bears found to be in this area, in these kind of blue areas here. So we know where lots and lots of grizzly bears were, and we know that they were frequently located in kind of these, you know, yellowish areas here and here. And then we also know at a 30 meter spatial resolution where and when logging occurred. So that's the green. You can see there was quite a lot of logging in this area here. We know where forest fires occurred. So that was the red here. And then we also have non-stand replacing disturbances, which is just a general disturbance term for when a forest isn't fully cut down. There's never a new forest stand regrowing. That's called a non-stand replacing disturbance. No need to worry about that too much. What they were most interested in is how was this logging and maybe these fires as well affecting where grizzly bears were being located, how they were behaving. And what they found in a lot of this research in West Central Alberta was that bears were often very commonly found in or very near to forestry cup blocks. When I say cup blocks, it's a forestry term. I'm referring to clear cuts. So clear cuts are generally the most common form of harvesting forest or logging in most of Western North America. So when I say cup blocks, I just mean clear cuts. But bears were often found in cup blocks more than other habitat that was available to them. So more than forested areas. That was only at certain times of the year, but at those certain times of the year, particularly in summer and fall oftentimes as well, bears were in this case 1.6 times more likely to be found in cup blocks than any other habitat. And this at the time, so this study here is from 2004, is looking at using very similar data and very similar methods to what this map is showing here. And they both research, both researchers had very, very similar outcomes of their research. Both of them essentially found that grizzly bears were really frequently located in these forestry cup blocks. And at first, they were kind of perplexed by that. Didn't really make a lot of sense. Why would bears like clear cuts? Why would they be located in clear cuts? The key reason, the key driver that they found that bears were located in clear cuts or in cup blocks was because of food availability. So grizzly bears are species that are very, very strongly driven by food. So where there's available food is generally where you'll find bears. Bears, especially in West Central Alberta, really, really like berries. And what they found was there's a lot of food, a lot of berries located in cup blocks that are regrowing. So particularly clear cuts or cup blocks that were harvested within the past seven to 30 years. So in the kind of stage of regrowth where they've had enough time to start regrowing vegetation in the cup block, but there hasn't been so much time that trees have fully grown and kind of enclosed the canopy. So there's kind of cup blocks that look like this. They're very bushy. And because of that, they have lots of these kind of berries that bears really, really like to eat. Contrast that to maybe say a second growth forest. So forests that are maybe 40, 50, 60, 70 years old. And you get something that looks more like this. The canopy is kind of enclosed over top. You don't get a lot of vegetation growing on the ground. So generally you get less food availability for bears. So what about other disturbances then? Why was it particularly cup blocks? Why would, you know, cup blocks be the habitat of choice for bears? Sure they would like to get food, but there's got to be other habitat types that are available for them. And for a long time, that was generally areas burned. So areas that were subject to wildfires. Forest fires would burn areas, parcels of forest, and then those areas would kind of regrow. Lots of berries and vegetation would regrow in those burned forests and give lots of food availability to bears. Historically, in the past half decade or so, humans have gotten really, really good at suppressing fires, so at fighting wildfires. So the result has meant that there's kind of this overall lack of natural forest openings for grizzly bears. Grizzly bears in general really, really like open areas. They like sub-alpine meadows. They like cup blocks. They like burned forests. They like areas where there's lots of openings in the forest. So where there's not really, really dense forest, because that's where they find berries. That's where they find a lot of the food, a lot of the vegetation that they like to eat. Historically, especially in Western North America and BC and Alberta, we have spent a lot of time and money and effort fighting fires, because we need to protect human communities and things like that. Because of that, there is proportionally less natural forest openings available for grizzly bears to try and go find food than there may have been prior to kind of excessive management of forests like we see today, with lots of fires being fought and with lots of forest harvesting going on. So what these authors ultimately were able to conclude was that it appeared though, as though bears were adapting to utilize cup blocks as kind of a surrogate or a replacement for natural forest openings like burned forests. Less of those habitats are available now and more of these cup blocks are becoming available because there's a lot of forest harvesting going on. There's a lot of logging. There's a lot of active forest management. So because that's what's available and because there's a lot of food located there, bears are hypothesized to be using those habitats much more than adjacent habitats. So again, if we go back to this, we can kind of see that. We have our study area again here in red. We got our location density of bears kind of very high right here. We see a ton of harvest there, lots of forest harvest again kind of right in here, which is right kind of in this area here, lots of forest harvest in there as well. We got a little bit of wildfires going on here and here and we do see some bears out in that way, but generally speaking, a lot of the areas that they were finding bears in was where there was a lot of forest harvest going on. And again, we are able to derive all of that information using the combination of that GPS caller data with this derived land cover and disturbance information from the 30 meter spatial resolution Landsat data. Now that's really important and really nice for a species like grizzly bears when we're trying to manage them and we're trying to conserve their habitat because from that information, we can make really valuable management decisions. We can say for example, in the forestry industry, let's try to harvest forest where there is a lack of natural forest openings. We've identified an area where maybe there hasn't been a wildfire in a really long time. All there is is a bunch of really dense second growth forest. Maybe it would be useful for us to harvest some forest in that area to produce a forest opening. Antiprogenic in this case, not natural, but nevertheless a forest opening where bears could maybe go find some more food. Maybe also we need to protect disturbed areas better. Maybe we can identify cup blocks or clear cuts that bears might really, really like and say, okay, we're not going to really allow much access for humans in those areas because we want to leave them aside as kind of a bear habitat. Now the one thing I'll say before I summarize this is don't take this too out of context in the sense that please don't go tell anyone that Chris told you today that cutting down trees is good for bears. That's not exactly what I mean. The context of all this is that today, nowadays, a lot of forest, especially in western North America, is managed for timber production and there's a lot of wildfire fighting that goes on. So given that context, that's where these findings kind of come from. In reality, if you had a completely natural forest that had no kind of, you know, at least no real human interaction in the sense of the kind of more western and modern type of management that we do today, then this wouldn't really necessarily be the case. You know, there's a good chance that cup blocks would not be a preferable area for bears. But because we have this historical fire suppression that we've seen and this lack of natural forest openings, then we've come up with this hypothesis and this phenomenon where we see bears potentially using cup blocks and clear cuts as a surrogate for naturally open areas in a forest. Does that kind of make sense? Okay. Don't tell anyone, Chris told you that cutting down trees is good for bears. It's not true. It's not a mean. Okay. So just in summary, we talked about tracking animal movement in a couple different ways with GPS callers with GPS tags and with camera traps. We talked about how we can quantify animal habitat with satellite data looking at land cover information. So we looked at that big map of all of Canada that has at a 30 meter spatial resolution information about where there's coniferous forests and where there is deciduous forest and where there's wetlands and where there's herbs and shrubs and things like that. And then we can also get that disturbance information. We can get that again, Landsat derived disturbance information telling us where there's logging that's occurred, where there's been forest fires, etc. I mean, combine all that data to go away and try and answer questions to help us inform our management decisions. So a couple of review questions for you guys here, a couple of practice questions. I'll leave you to try to answer them yourself and then I will go over them with you in a couple of minutes and then I'm going to give it to Sam. I think he's got maybe one slide to show to give you a couple tips for the assignment that you're working on this week. If you have questions for him after that, you're welcome to come down and chat with him. So if you're going to head out now, please do so and do so swiftly. And then we will go over these in a couple of minutes, then I'll give it to Sam. See, thanks guys. Hey, how's it going? This, you can just like hold it as a mic. I just like do a voice recording. I'm not sure he's going to host it. Okay guys, let's do a quick crowd source here and then I will hand it over to Sam. So from what we talked about today, what are some advantages of using camera traps to collect animal movement data? Yes, we talked about a couple, let's do maybe like two. Yeah? Yeah? Exactly, cost effective provides population level data. What about some advantages of using a caller, a GPS caller to collect animal movement data? Yep. Totally. Fine scale movement data and remote data collection. You don't actually have to go out into the field to collect the data. And you get really, really fine scale level movement data, maybe up to every 30 minutes if you really like. Okay. So bit of a longer question here or longer answer to this next question. Describe how you might use GPS caller data and Landsat derived data or information to determine what habitat an animal prefers or avoids for that case. Feel free to use the grizzly bear example to help explain. You don't have to. I feel like it's probably the easiest because we just kind of went over it in detail. But maybe in a couple sentences can someone kind of explain to me just kind of theoretically how that works, how you can combine GPS caller data and Landsat data to try and understand what kind of habitat an animal prefers. Yeah. Well, like GPS data will tell you where the animal is and the land that data will tell you what kind of environment that area is. So you could say that, for example, the grizzly bear prefers this area because this area has environmental factors that help the grizzly bear thrive because it recently burned down and has buried. Totally. Yeah. And like specifically, like what we talked about with the Landsat, specifically what kind, what are some examples of the habitat information that we got from Landsat? You mentioned one of them already just now. Right. Just like, I forget what the term board is like a disturbance. Is that what you're thinking of? Yeah. Yeah. So disturbances get things like disturbances. So, so just to read it, read it, reiterate his answer there. With the caller data, we can determine where animals are located. Then with the Landsat data, we can quantify the habitat. We can say, okay, this is where coniferous forces. This is where deciduous forces. This is where wetland is. Then we can also quantify disturbances, how the land is changing. We can say, okay, this is where logging is occurring. This is where wildfires are occurring, etc. We overlay those two informations and we have okay. We know where our animals are. We know where the habitats are located. Our animals close to these kinds of forests. Are they close to wetlands? Are they close to herbed areas? Are they close to these disturbances? Are they close to harvested areas, logged areas? Are they close to wildfires? Okay. Great. Great answer. Okay. Lastly, specifically with the examples I gave about grizzly bears, what insights have we gained about grizzly bear habitat preference using caller data and Landsat data in west central Alberta? What was kind of the final outcome of the research that I was talking about just there? What were they able to kind of determine? Yeah. Yeah, exactly. Specifically, they prefer open areas such as cup blocks. Specifically, they determined they found out that there's potentially this phenomenon where grizzly bears are substituting cup blocks, kind of an anthropogenic or human created open forest for natural open forests like burned areas or areas that may have disturbed in other ways such as avalanches and things like that. So they were able to determine that cup blocks or clear cuts were being used kind of as a surrogate or replacement for some of these natural forest openings. Great. Awesome. Okay. I am going to give it to Sam here. He had a couple of slides he just wants to go over and then you're welcome to come ask him if you have any questions after that. So I got the one image, the second image and then both of side by side here. Okay. Cool. Before we talk about that, we have some updates about assignment five on Canvas. There was a mistake. Question 17 refers back to questions 15 and 16. On the version that went up, it said questions 16 and 17. So that should be 15 and 16. I've changed it now. But I think that those of you who started the quiz already are not going to see the change. But just know that if your version says 16 and 17, the 16 should be 15, the 17 should be 16. I've made no changes except swapping those numbers. If you're confused, you can message me. And you know, if you get it wrong before you're confused, we can work it out. So don't stress over that. It's my mistake. Also, if you're wondering exactly what that difference is, what the mistake is that Sam fixed, he just sent an announcement about it on Canvas. So it's right on Canvas. He very clearly just lined out this number. Actually means this number. And then one more thing I did have a few questions about question 18. It's just where it falls in the assignment. It's unclear if it's referring to the pixel numbers from the previous few questions or the image. It's referring to this image, the image that you're given in the document. So that's what question 18 is about. Yeah, again, if you're confused, you can reach out. But it's not to trick you. It's just how the document is laid out. And then, yeah, I've gotten a few questions about like, oh, it says that my image should be purple and green. No, that's great. It says that my image should be purple and green, but it looks more red and blue to me or something like that. And these are real images that have changed in real time. So it's not always going to look like it looked when we wrote the lab, because a new lens that image has come out. This lab, that question was written in the fall, maybe. So just as an example, here are two images of exactly the same place, Mount St. Helens, which you'll be talking about. One is from October. One is from a few weeks ago. And with that composite that it asks you to use that exploits the shortwave infrared and the thermal bands, you're going to see some big changes in color because there's snow in the winter. There wasn't in October. And then there's a lot of changes that have to do with water, that have to do with temperature, that have to do with snowpack and with snow melt. And those longer waves of lens out are sensitive to those changes. Really sensitive. So if it doesn't look quite as it does in the image in the text, don't sweat, because it's a new image. It's dynamic. And then the other thing is these images are stretched. So just keep that in mind. As you zoom in and out, there's going to be some changes. And that's just the way that the website is displaying those images. It's not that the numbers have changed. It's just has a built-in stretch that optimizes the image to make it very clear what the differences are based on what's in that image. So don't worry too much about the details of that. Just big picture, look at differences, interpret the image. This lab is all about interpreting the image and interpreting differences among images. And again, reach out if you're confused. But none of this is a trick question to trip you up because, oh, it was purple and you said green. There are no questions like that. Okay. Awesome. Thanks, guys. Feel free to come down if you got any questions for Sam W. Simon. All right. Hi, everyone. Welcome to class today. We have just a couple of other weeks of lectures to get through. So we got human footprint today and then we got next week environmental change and one other lecture. I'm forgetting for some reason. But we got two lectures next week still going over class content. In the following week, we have a Canada from space lecture, which will be a very short lecture related to one of your last blog posts. So blog post five won't be available until that lecture, which will be our second last day of class. So Monday, two weeks from now, essentially. And in class, we're going to go over a couple of things that you'll then use to go and do blog post five. So we'll go over that when we get to it. And then in terms of class today, we'll do the human footprint lecture. End of class, Sam is Sam had an appointment, so he's not going to be here at the end of class. But if you have questions about assignment five, Evan will be here and I'll be here. So we'll leave some time at the end of class if you need to ask us any questions about that. And then also at the end of class today, after I go over this lecture, Evan's going to be here and talk about the final exam as well. So he'll talk about final exam format, practice questions that will post timing, etc. So if you have any questions about that, you are welcome to ask. And he, that also sound good. And good to go. Sweet. Okay. So today we are talking about observing the human footprint from space, again, similar formats, what we've done are past four or five or so lectures, talk about what the human footprint is, why it's important to monitor it, how we've monitored it historically, what are some of the remote sensing technologies that we use today to monitor it, and then use some specific applications and examples to kind of further explain and dive deeper into monitoring the human footprint, particularly towards the end of the class when we're talking about specific applications and examples associated with a particular Earth observation remote sensing data set, we are going to focus on nightlights. So data of the Earth Earth observation satellite imagery that is collected at night, and is related to how much essentially light pollution there is across the surface of the Earth. It's kind of a fun, cool topic to add on. So we'll get to that in a moment. First off, what is the human footprint? I'm sure it's a term that probably you have heard tossed around a little bit, and maybe some readings, maybe just chatting with people, conversations you might have had. There's lots of different ways that we can describe what is termed as the human footprint. So sometimes it's considered just the geographic extent of land that's under human use. So how large of an area of the surface of the Earth has some influence by humans. Sometimes it's just considered a measure of how much we are using the Earth's natural resources. Sometimes and generally more so, it's just some sort of metric that allows us to calculate human pressure on the Earth and on the Earth's resources. But that's kind of three very different definitions. So let's keep that in mind. We'll get back to that in a moment. But not a particularly clear definition of exactly what the human footprint is. And that's part of the reason that Earth observation data is really valuable for monitoring and measuring something like the human footprint. And we'll loop back to that in a second. So there's lots of different ways that we can try and quantify or measure the human footprint. So it can be sometimes assessed by just the size of a population. How many people do we have on the Earth? It can be assessed by the amount of human settlements. How many cities and towns and villages do we have all across the Earth? Maybe we can assess it using the degree of resource extraction. How much forest have we cut down all across the Earth? How much oil or gas have we mined or extracted? We can also maybe look at other metrics like the number of products people consume. Or the number of cars being driven and how much they're being driven and how often they're being driven. So there's lots and lots and lots and lots of different ways that we can try to measure how much pressure we as humans are putting on the Earth. There's lots of different ways to do it. And it's important to monitor. It's important to measure because it essentially allows us to assess our impact on the world. Both environmentally, economically, socially. When we think of the human footprint, usually we think of our environmental impact. And ultimately that allows us to make decisions on resource management, on land use planning, on urban planning. How can we plan our cities, our settlements, our towns to ensure that we don't have as much of a human footprint as maybe we could if we planned it in a different way or a different manner. So this allows us to create conservation focuses. It allows us to monitor how we're performing and essentially monitor what our impact on the Earth is. Now, one of the common ways that we might describe the human footprint, one of the ways that I mentioned earlier, is by trying to quantify how much urban areas there are all across the surface of the Earth. So I wanted to start by looking at this and I'm going to try and get you guys to help me out here. If we want to look at some of these images and consider from a satellite's perspective how we could classify these images as either urban or not urban and then furthermore as either contributing to the human footprint or not contributing to the human footprint. So let's start kind of in the bottom left here. This one's pretty easy. Would you call this this one in the bottom left here? Would you call that area an urban area? Nods. Again, lots of nods. Yeah, probably. Okay. What about this one just to the right here? Would you consider that an urban area? Some nods. Semi. Yeah, maybe, maybe not kind of suburban. A little bit harder to tell on that one. What about say this area as a whole here? Would you consider that an urban area? No, she's some head shaking. Some head's nodding. We do have some really high density housing going on here. So depending on what we consider to be an urban area, maybe at bare minimum this could be an urban area. Maybe this one not so much because there aren't as many buildings there. Okay. Fair enough. What about this one in the top right here? Would that an urban area? Yeah. Some nods. Maybe so so. And in the bottom right here? Urban area? No. Probably not. Okay. Fair. So now what if we considered each of those images from the perspective of a human footprint? So for example, we started in the bottom left here. We said, yep, that's definitely an urban area. Is that area contributing to the human footprint? Yes. Obviously it's a super, super urbanized area. It's definitely contributing lots to our human footprint. What about this kind of suburban area here? Yes, contributing to our human footprint, but a little bit less clear whether that's urban or not urban. What about this one up here? Would you say it's contributing to the human footprint? Maybe a couple of nods sort of. What about this, just this particular area? We said that maybe this area isn't urban. Would that still be an area that you'd consider as contributing to the human footprint? Yeah. I have some nods. You could argue it is for sure. It's still a vegetated area, but it's farmed. It's under a use by humans. And then what about let's end on this one right here. We said that that one's definitely not an urban area. Would you say that this area is contributing to a human footprint? A couple of nods. Yeah. You could again argue it is. Maybe not so much in kind of this forested, kind of preserved area in the back here, but the farmland's sure and the areas where there's houses and and other buildings related to the farm, sure probably. The point is from looking at all of this. It's hard to necessarily classify an area as urban or not urban. And subsequently it's also really hard to classify an area as contributing to the human footprint or not contributing to the human footprint. And just because you classify something as urban or as not urban does not necessarily mean that it is or is not contributing to the human footprint. So all of this makes trying to measure and monitor the human footprint a little bit more complicated. So the definition of the human footprint is really hard to quantify and to define what is an urban area. Is it just an area with lots of buildings? Is it a residential area? Is it you know, at what point transitioning from say an urban to a rural area? Do we draw that line? It's hard to say when you're just looking at images like this, this, and this. Sure, this is definitely urban, but this one maybe is urban. Probably still this one definitely maybe urban over here. Probably not over here, but you know, there's still lots of humans in this area as a whole. So maybe you could consider it urban still. The point is it's just really hard to classify and identify. The really nice part about earth observation remote sensing, it is that it allows us to standardize those measurements. It allows us to look at an area on a pixel by pixel basis. And according to some metric that we define, typically related to the spectral properties of the reflectance of the surface of that area, we can define areas discreetly as urban or not urban as contributing to the human footprint or not contributing to the human footprint. That doesn't necessarily mean that however we are defining an urban area or defining a area that contributes to the human footprint is appropriate. We still have to deal with this issue of not really having a clear definition of what an urban area is or what the human footprint is, but it allows us at least to define a methodology that we might use to identify urban areas to try and quantify the human footprint and then repeat that process over different areas across the surface of the earth. So you're using the same standardized metric and measurement to try and derive a measurement of urban areas of the human footprint. Now there's lots of other reasons why the human footprint is particularly difficult to quantify and measure. If I ask you on an exam, I'll probably focus on it not having a very clear definition. So if you see me ask you something on the exam along the lines of why is the standardized nature of earth observation remote sensing particularly useful for quantifying the human footprint. Well there are these other reasons like the human footprint is different at multiple scales, it's constantly changing, it's hard to find detailed data with global coverage, but the most important one, the most valuable reason there is for earth observation, monitoring and measuring the human footprint is this last one here. That is the human footprint has a really uncertain definition, there's no clear and universally accepted definition and so earth observation remote sensing data at least allows us to measure it in some standardized way that we can repeat all across different parts of the world. Now similar to all of our kind of most recent lectures before we had earth observation data, before we had satellite data or at least space based earth observation data, we often quantified say urbanization which was a common metric to relate to the human footprint with aerial imagery and aerial imagery of urban areas and expanding urban areas. So aerial imagery provides the longest available record of landscape change that we have available to us typically going back about 100 years or so and it's really useful for comparing to more recent satellite imagery because it gives you a little bit of the history of an area prior to say the 70s and 80s when we know the Landsat mission and other earth observation satellite missions started being launched. So aerial photos were the first form of remote sensing, remote sensing just being any kind of measurement where you have a sensor that is measuring the properties of something that are not in the immediate vicinity of that sensor. So a camera in that case a satellite is measuring things that are kind of maybe kilometers or meters away. So that's just what remote sensing is as a whole. Aerial photography was the first form of remote sensing. So during World War I, aerial photography was used for reconnaissance missions and then after the war Canada had a large surplus of planes and camera equipment given to them by Britain for military purposes. So they started using aerial photography for more civilian applications. They started flying planes across the country, across different landscapes to look for forest fire detection, to look and monitor fisheries, to try and map urban areas and start trying to quantify things like urbanization. So typically historical aerial imagery is panchromatic which just means it's black and white. It generally just has one broad spectral band. So if we think back to our spectral resolutions lecture, aerial imagery generally is panchromatic, just has one broad band usually spanning the entire width of the visible part of the electromagnetic spectrum. It's generally quite a high spatial resolution because the plane was probably not flying at an altitude as high as say satellites are orbiting at. So the plane is flying a lot lower than satellites because of that we generally get a higher spatial resolution say around one meter. We can get vertical or oblique imagery from this historical aerial imagery. I'll show some examples of the two of those in a moment. Vertical just means looking straight down, oblique means kind of looking at an angle to the side. Now downsides, individual photos from aerial imagery had quite limited spatial coverage which meant they needed to be mosaic. So generally you get a bunch of images overlapping a particular area. You'd have them on prints, pieces of paper essentially and you'd have to toss them up on the wall and kind of pin them all together like this to make a mosaic of a whole region. And the spatial and temporal coverage of aerial imagery projects generally depended on the needs of the project. So if you were looking at forest fires or maybe deforestation or forestry you might have a spatial and temporal coverage specific to those project needs. So specific to monitoring deforestation in the 30s or in the 40s for a particular area. So this is just an example of some historical vertical imagery. So some imagery that's just looking straight down, taken from a plane compared to some newer high resolution satellite imagery on the right here. So these are two aerial images. And again, this is just an example of being able to look back through time compared to newer satellite imagery and make out differences in the landscape. In this case in urban area you can see kind of the big thing that was changed here. There used to be a big kind of train station here and that was changed into kind of more of a mall and fitness center here. You can see also the difference there in the density of cars, the amount of cars being driven on the road. You can see lots more kind of little specks of cars here than you can see in the older image here from the 20s. But just an example of some vertical imagery looking at the change of an urban area through time. This is another example looking at kind of less large scale. So zooming out a little bit looking at kind of the urban rural transformation of a particular area. So less of a transition for maybe an industrial kind of place to more of a residential kind of modern place. And something here where we have the edge of residential areas then farmland slowly being converted. In this case over almost 100 years to more suburban residential areas. Now those two examples kind of focus more on residential areas, urban areas per se. We can also look at historical imagery from aerial imagery and look at landscape changes. So natural environment changes. In this case we can see here there was a reservoir that was created after a dam was placed and kind of flooded this whole area. So this is what that landscape looks like prior to that dam being put in prior to the creation of that reservoir which you can now see here. This one's from Alberta. Okay and then the last one that I'll show an example of an oblique image. So being able to kind of look to the side a bit. Which is really nice sometimes because you can get different features and different characteristics of the imagery that you might get if you're looking straight down. In particular for urban areas you get a really good sense of the skyline. So this a sense of how many buildings there were and how tall those buildings were. So you can see here in comparison we got our Stanley Park on the left here in Vancouver, downtown, lots of skyscrapers, lots of big buildings and then a comparison much older from the 1920s. Much less buildings, no bridge built yet over here. A lot of change that you can observe here. And again this is just an example of taking some historical aerial imagery being able to in this case compare it to some more modern aerial imagery. Now we kind of know from talking about it a little bit in class that you can take these true and false color composites that you guys have worked with a couple times in the assignments and you can look at images of the same place through time to track changes in those areas. One of the things that we can look at now is urbanization. But again we know from talking about the Landsat mission that that only allows us to go back as far as the 70s and 80s. So prior to that if we wanted to try and monitor something like urbanization to consider how the human footprint might be changing we'd have to incorporate some sort of historical aerial imagery. But 70s up until the present we can mostly use satellite imagery and we can look at images through time to try and quantify and depict the human footprint by in this case looking at things like urbanization. However with traditional kind of classic satellite imagery we'll generally get of an urban area something that looks like this. And you can see here in this urban area you know this is definitely an urban area this is in Warsaw there's lots of different features lots of different characteristics of the surface of the earth for this particular urban area. You can see we have you know water features we got a river there we got some parks so we got some green areas here. Even the buildings themselves we have here a stadium that's really really bright white. We have another kind of neighborhood over here where we see colors more along the lines of kind of greys. We then have an area up here a historical portion of the town where there's lots of red buildings and actually even kind of bright green buildings that you can barely see there and there. What this is an example of is the large amount of diversity of color and thus of spectral properties that you might see across an urban area. That makes it really really hard to classify an area as urban when you're looking at imagery of a place during the day using this kind of traditional classic looking imagery. It's hard to tell a computer algorithm or even to just make up with your eyes. Well okay this is urban but it's red and green and this is urban but it's kind of grayish and this is urban but it's really bright white. It's hard to be able to try and quantify say the human footprint by looking at something like urbanization with an image like this because we have so many different characteristics, so many different colors and spectral properties all across this particular area. So that is where this data set called a nightlights data set comes in. That's kind of what we're talking about for most of the rest of the class today. So I got a quick kind of intro video about nightlights data sets before I kind of launch off on it. In daylight our big blue model is all land, ocean, sea, and wetlands. But the night is awake. Same from studies our planet comes alive with light. This new view of the Earth's life is a positive layer of carbon monotone or the Sunni, near-p2d satellite. A board design model, a new design instrument called the Earth is able to collect what time it is serving because it remarkably detailed being of the Earth's life. In some places the city lights resemble solitary shards of the night star. In other places, events cluster together. The satellite can even distinguish likely this approach that line with which non-river and the massive plane from gas waste produces a line product of the Earth's life. The satellite passes the forward darkness of the Himalayas. It shows how human settlement is bound on natural borders. Even political borders are starkly visible in this view of North and South Korea and in a line of fishing boats that got to the other city. But not all light is electric. Blowing just as bright, late fly fires burned across Australia. This new view of the Earth at night offers a unique perspective for exploring the many places in which we live and seeing the impact of human populations around the world. No matter how light or how light, they're large shots. Okay, so we're going to talk about a couple of night lights data sets. The first one or the first satellite mission being the operational line scan system or OLS onboard the Defense Meteorological Satellite Program. It's often just called the DMSP OLS sensor. This sensor was originally designed to detect clouds at night and aid mostly in meteorological interpretation. By chance, scientists also noticed that it detected city lights really well in gas flares and fires and essentially all of these emissions of brightness of light at night. So they actually weren't planning on launching this satellite in order to monitor and detect lights emitted by humans or fires or whatever it might be. But they found that once they launched it, it was really, really good at monitoring and detecting those things. And they quickly realized that this was a really, really good metric for looking at something like the human footprint. It's hard to look at buildings and their spectral signatures because they could be so different and classify them all as urban areas or consider them as having a potential level of human footprint. What's really nice about night lights data sets is lights or lights. Lights will always have some sort of emission associated with them and generally, even if they're different colors, they're going to be picked up by these sensors. That's partly because these sensors are just measuring the emission of wavelengths all across the visible portion of the spectrum. So by measuring all across the visible portion of the spectrum, any kind of light that is being emitted across any portion of the surface of the earth is going to be picked up by these sensors. So this one, the first one, the DMSP OLS sensor operated from 92 till 2013 and produced the first data sets of night lights that we got from space. It measured radiation from 500 to 900 nanometers. So expanding that whole range of the visible portion of the spectrum as well as a little bit into the near infrared portion of the spectrum. Now the DMSP OLS sensor, even though they kind of stumbled upon it as a really good tool for monitoring and measuring something like the human footprint, it had its limitation. So because it was not designed for measuring and monitoring night lights, it was designed for measuring and monitoring clouds and for meteorological purposes had quite a coarse spatial resolution. So only about 2.7 kilometers. And it had a pretty low level of sensitivity, which meant that you generally got saturated brightness pixels close to cities and didn't get very good detection of areas that had very, very faint lights. So you can kind of see that in this example here. So this is kind of a new updated sensor used for monitoring night lights called the VIR sensor, which we'll talk about in a moment. But if you compare that to the older DMSP OLS sensor, you'll see very, very bright areas, kind of like cities, which you can see here and here, get fully saturated in this imagery here, which just means the maximum amount of brightness is measured all across those pixels. You also don't get nearly as much detail in this newer sensor image in kind of areas that don't have very bright lights. So this kind of area out here, you don't really get a lot of detail. You can kind of see some faint patterns. But you don't really get a sense of exactly where these faint sources of light are coming from. It also had a limited spectral resolution. So again, it only looked at that one band ranging from 500 to 900 nanometers. And then it also had no inflight calibration. All that means is that often satellites because of different atmospheric compositions at different times of the day over different portions of the surface of the earth, they can calibrate themselves during flight while they're orbiting to maximize or optimize the data that is being collected. And the DMSP OLS was an older sensor. It didn't have that capability. So then came the visible infrared imaging radiometer suite, also just called VIRS, which is a sensor on board, a NASA NOAA SWOMI National Polar Orbiting Satellite. It's often just called the SWOMI MPP VIRS mission. VIRS is the sensor. SWOMI MPP is the satellite. And it has provided global daily measurements of nightlights since 2011. So this satellite was launched specifically for the purpose of measuring and monitoring nightlights. And one of the big reasons for that was so that we could in a standardized and efficient way be able to track urbanization and try and quantify the human footprint. So it included a higher spatial resolution. So a smaller pixel size of about 375 to 750 meters. It has a daily temporal resolution so it gets more complete global coverage than the DMSP OLS sensor did. It has improved sensitivity so you don't get nearly as much saturation in the really bright areas. And you still get good detection of kind of fainting bright areas out in more of the rural areas here. It has more spectral resolution so it measures more bands and it has that in flight calibration ability. So overall it's a better sensor and allows us to get these really nice detailed images of the night's at light all across the earth on a global scale every single day. So this is a video specifically about the Viers sensor. Okay. So we generally don't look at the nightlights data sets and just say okay bright areas are associated with regions that have high human footprints. We generally try and associate the nightlights to some sort of metric that is related to the human footprint potentially. So we'll often correlate bright nightlights with things like population with energy consumption, unique activities such as GDP, urban extent, gas flaring volume, CO2 emissions, all of these kind of things that generally are associated with an increased pressure from humans on the environment have been found to be strongly correlated with nightlights. So it gives us that ability to maybe kind of summarize all of these different sorts of metrics by just looking at something like nightlights. Now we can do that in space so we can look across an area and say okay this areas are dark, these areas are brighter and that gives us a sense of where there is a higher human footprint where there's a smaller human footprint. We can also look at that through time so we can see how for a particular area how nightlights are changing and relate that to how urbanization is changing, how energy consumption is changing. We can also relate it to kind of more nuanced things like culture and population. So I have a quick video kind of with a cool example about that. Right, see from the ground how would he like to give us cheer and help us celebrate a season. But from space over time they can tell us something about both culture and energy usage. Three years ago a new satellite called Swami NDP began to give us the growing new use of the earth by day and by night. But as time as it is to see earth all the globe this is still just one composite image averaging observations over a period of months. Getting a big picture from space is always healthy but we can learn even more seeing that picture change over time. A NASA lead team of researchers has been pouring over the data from Swami NDP and compensating for factors like the reflection of the moon and the effect of terrain, clouds and aerosols to produce a scientifically valuable product. Measuring earth's lines on a daily basis. The first thing they noticed in the data was the effect of holidays. When we started looking at the data at night over the United States we were expecting to see a lot of stability in the night at night. And we were really surprised to see the vibrant, increasing activity under the holidays. In particular around areas in suburbs you have single family homes with a lot of yards based putting less. Compared to six weeks between next get-in in new years to the rest of the year the team knows large areas where nightlights were 20, 30, even 50 percent brighter over the holidays has shown here in shades of green. And so what we're seeing is this shift in location and in activity where people are staying in their homes and they're celebrating or they're traveling to the rural areas in their suburb and they're turning on the lights. And whereas in the urban centers people are turning off the lights because they're going off for the holidays. Researchers actually first saw changes in holiday lighting patterns during Ramadan in the Middle East. Well in contrast to the western holidays of Christmas and New Year's during Ramadan you don't see a shift in activity in location. What you're seeing is the communities are staying where they are and they're shifting their activities to the night. Another thing that the measurements are telling us is that they're capturing cultural differences even within a single community like the Muslim community during the month of Ramadan. We're seeing patterns in behavior that vary significantly from country to country. Smaller increasing nightlights in Turkey, large increasing nightlights in Saudi Arabia. I'm no use in nighttime lights right now because it's not a predominantly Muslim country. So it's a lot of diversity seen within the day. Because the nightlight data has such good resolution researchers from Yale University have even been looking at neighborhoods within cities and correlating that data with known political and socio-economic factors. But from a NASA perspective what is important to concentrate on Earth's lights from space? A lot of our capability focuses on using satellites to get a little picture of where we are with respect to the environment and with respect to the day they process that drives the area of the system. A big component of that is human activity and how they're driving the different processes that impact your carbon emissions and the healing patterns across cities. And so by looking at the lights we can see changes in human behavior throughout the seasons, throughout the pace. And we can use that information to then understand what are the norms that are driving the decision behind energy. If you look at climate change research right now, we know that more than 7% of emissions are happening in cities. And so NASA is putting a lot of emphasis on understanding cities and understanding the dynamics of how cities are interacting with climate and our systems all. Okay, so just to kind of summarize a bit of a comparison between the use of say aerial imagery and of satellite data for monitoring the urbanization patterns for monitoring and trying to measure and quantify the human footprint. With aerial imagery we generally get quite a high spatial resolution. We get a pretty small pixel size so we get lots of detail across an area. Makes it a little bit easier to try and quantify and pick out really small changes in areas that may affect the human footprint differently. So maybe a much easier to say, I don't know, pick out a park, a really small park from an urban area and say, well, maybe this doesn't necessarily contribute to something like the human footprint. Aerial imagery makes that a bit easier because you have a high spatial resolution. You get more detail in the imagery. With aerial imagery though, you also have to use manual interpretation. You have to mosaic all of your images, essentially pin them up where they overlap to try and put together an image or a color. An image or a composite of the whole area you're looking at and you have to manually classify it. You have to manually looking at it with human eyes and draw essentially boundaries around areas that maybe are contributing to the human footprint, maybe not contributing to the human footprint areas that are very urban areas that are less urban, etc. It's not a very efficient way to collect data to try and monitor and measure the human footprint. You have to go out and you have to fly planes. That costs a lot of money. You potentially don't get a very good temporal resolution. Oftentimes with aerial imagery, you might have an area that has been flown over maybe once a year if you're lucky. So you're definitely not going to get that potential for a daily temporal resolution like you might with, say, the veers sensor. But you do get that historical data. So you do get data often dating back to the 20s, 30s, 40s. A time period that we can't really quantify, say, Landsat, MODIS, or the veers or DMSP, OLS sensors. Now in terms of the satellites, same kinds of concepts and topics we've been talking about for our other applications when we've been talking about the biosphere, the cryosphere, oceans and water, particularly for the human footprint, was really, really key and nice about satellite data is it's standardized. We can use a consistent definition of the human footprint because it's not subjective. We can say, okay, our human footprint is going to be associated with this level of nightlights emission. And we can use that same threshold all throughout the world, all across the surface of the earth. So it allows us in a certain way to try and standardize the definition of what the human footprint is or what urbanization might be. It allows us that efficient data collection and process of the analysis. We can often automate it to classify where areas are urban and where they're not. We get really good coverage. We generally get, you know, from the veers sensor, global coverage, images of the whole earth every single day. And we have the potential for different spatial resolutions, different temporal resolutions and different spectral resolutions, different bands that we might be looking at. Okay, bit of a shorter lecture today. Ripped through that. Got a couple of practice questions here. So why is the nightlights data advantageous when trying to map urban areas compared to daytime satellites? What are some examples of metrics that nightlights data from space can be related to? Why is the standardized nature of satellite data particularly helpful for measuring the human footprint? Can definitely expect a question like that on the final exam. And lastly, just describe a situation where satellite data alone may not be applicable for measuring change in the human footprint. And you may have to incorporate the use of aerial imagery. So I'll give you about five or so minutes to try and draft up some answers to those. Then we will kind of crowdsource the answers to those quickly. And then we'll get into talking about the final exam. Lastly, and then I will leave some time if you guys want to ask me or Evan about assignment five. Sound good? Okay, sweet. If you don't want to stick around for going over these questions or for talking about the final exam, you don't have to. You are welcome to head out. If you want to, please, you're welcome to stay. Encourage to stay. Yeah, I'll give you guys a couple minutes. Then we'll go over these in a second. Hey, I actually have a question. Yeah, sure. I don't think I have a thing about it, but. Okay, guys, let's try and work through these guys. So first one, why is nightlights data advantageous compared to daytime satellite imagery when trying to map urban areas? Yeah. Great, great answer. Yeah, nighttime data has a lower diversity of spectral properties, which makes it much easier to classify than standard daytime imagery. Daytime imagery of urban areas potentially could have a wide variety of different colors, different spectral properties, making it a lot harder to identify and classify. Awesome. What are some examples of metrics that nightlights data from space can be related to? So we know loosely we can say, okay, well, it's kind of related to the human footprint, but what were some specific examples of metrics that can be used to describe the human footprint that nightlights data sets can be related to? Yeah. Yeah, socioeconomic factors, GDP emissions, CO2 missions, gas flares, those kinds of things. Yeah, totally. Why is the standardized nature of satellite data particularly helpful for measuring the human footprint? Hopefully I drilled that. Yeah. Exactly. Yeah. The human footprint is a really hard thing to define. There's no universally accepted definition for the human footprint. So the standardized nature of Earth observation data makes it really, really nice for monitoring and measuring and detecting the human footprint because we can use a standardized, repeatable metric to try and quantify and measure the human footprint. Totally. Okay. Last thing, describe a situation where satellite data alone may not be applicable for measuring change in the human footprint, and you may have to incorporate the use of aerial imagery. So what kind of analysis might you be looking at in relation to the human footprint that may require you to incorporate some aerial imagery? Yeah. Historical analysis, in terms of money, a way of 19. Yeah, historical analysis, absolutely. If you're looking anywhere before about the 1970s for changes or properties, measurements of urbanization of the human footprint, you're going to have to incorporate some aerial imagery. We don't really have any satellite imagery dating back prior to the 70s and the 80s. Okay. Awesome. I'm just going to switch out my slides here and just pop in my final exam slides. And perfect timing. Look who it is. Okay. This guy? It should be. Yeah. That was great timing. That was perfect. Okay. Please give Evan your attention while he talks about final exam. I may obnoxiously chime in here and there. But if you have any questions, this is a really good time to ask us about the final. Yeah. So cons 27 final. This is you guys. You're pondering your observation in case you need the meme explained. It didn't click. It's not clicking. It's not clicking. We're stuck on the meme. I'll just hit a button. Oh, sorry. The laser pointer doesn't rely on that. Yeah, I didn't. Yeah. So cons 27 final. Brief information. Final date and time. It's Thursday, April 27th. So like over a month away, you guys have plenty of time. It's two hours long, seven and nine p.m. Same setup as the midterm. There's a buffer. So you have 10 minutes beforehand and after in case you're running late or something like that. No worries. Same thing. It's administered through Canvas. I'm going to be on my email the whole time. So I mean, emails, if anything comes up that you think is wonky and I'll deal with it. We'll get it sorted. 10 minute buffer, but two hours, no matter when you start it. Yeah. If you start, I get to that later. But if you start at 8 p.m., you're only getting an hour and 10 minutes. There's 40 multiple choice questions. There are worth one and a half points each. Some are going to have pictures that you have to interpret. There's one matching question. It's worth four points each. And there's six short answer questions. And you want to write around the same as the midterm on the short answer. One brief paragraph. We give you like a guideline on how much for each question section. And that's worth six points each. So 36 points total. Again, the exam is open book. Same setup as the midterm. The questions are going to appear one by one in a random order within groups. So all your multiple choice. Then the matching question. Then all the short answer. And you will not be able to return to questions after completing them. Same as the midterm. Finals have got to be done individually. Don't work together. Don't share answers. Don't discuss answers. And if you're an A&D student, we will add the time beforehand. So you don't need to worry with that. Just if you haven't sent me your form, send it to me before the final, please. Pre-post midterm split, we get this one a lot. It's about 40% pre-mid term, 60% post-mid term. The short answers are a little more integrated where one is like hardcore pre-mid term, two is hardcore post-mid term, and then three are like synthesis questions where we want you to think about everything that's happened and put it all together. The final final notes, I love this one. It's worth 40% of your overall grade. We've posted the practice questions on Canvas. We don't, same as the midterm. We're directly giving you guys a sheet with all the answers, but if you're unsure of anything, put in the discussion board and we'll respond there. And the final includes all lecture content, but not assignment material. So anything on the assignment that's don't worry about, you don't need to study it, just what Chris talks about. There's a final discussion board. Post in it, ask us questions. They don't have to be from the practice questions. They can just be clarifying stuff. Discuss with your peers if you'd like. I'm going to be monitoring and answering. And Chris is going to do a review session where he's going to go over the stuff that's asked to be reviewed in this discussion board. So if you're like, I want more info on special, special signatures, same as the midterm, he'll know what to prep to bring for you. You need reliable internet. Please make sure you have reliable internet. Don't do this hot spotting off your phone. We're not reliable. We're not accountable for that. That being said, if you've got an emergency, your internet goes down for 10 minutes or something. Immediately email me. If you have a problem, immediately email me. Don't wait until the end. Just email me ASAP. And again, we can't answer content-related questions so we can't clarify anything. If you say, what does this mean, I'm going to respond. I can't answer that. If it comes up with a lot of students, we'll void the question. And then I'll give you probably two minutes extra time to account for you waiting for me to respond. If you open your exam at 8pm, you're still going to get cut off at 9pm. So make sure you start by at least 7.10pm. Play-durism, results in zero marks. Don't copy base sentences from slides of the internet. Answer your own words. Don't work with your neighbors and classmates. And then don't just change a few words and call it a day. That's still plagiarism. Good luck. Do the practice questions. Go over your notes. Be sure to write Chris highlights. He really highlights the important stuff. And you're going to do great. You got anything else? Yeah, I got two things. Grab my little microphone here. I got two things that I will just highlight from what Evan said. One is we always get students asking us questions or concerns about this slide, just about plagiarism. Because there are questions where you don't necessarily have to answer with whole sentences where maybe it's just an equation or something like that. This doesn't apply to those instances. So if we're asking you what is the equation of this and you take it from my slides or whatever, that's fine. This is really referring to more so the short answer questions, especially the synthesis ones, but just the short answer questions as a whole. When we are asking you to explain something, which means we want a couple sentences of you synthesizing some information, explaining some information, discussing some information, that's when we want you guys to be answering your own words. You can use whatever resources you want, any online resources, anything from the slides, but it has to be in your own words, which includes copying, pasting something, and then just changing one or two words around here there. You have to make sure it's answered in your own words. It's generally pretty easy for us to tell if it's not answered in your own words. And if we catch you doing that, then we have to give you zero for the whole exam, unfortunately. So, yeah, that's my note about that. Any questions about that? Okay, the other thing that I wanted to highlight was the discussion board. So if you have questions, clarifications about practice questions, the ones that Evan just posted on Canvas or about the ones that I go over at the end of each lecture, you can post about them there and we'll walk you through and try and confirm your answer for that. But we just don't give a key for all of the practice questions. That's the first thing, which Evan already mentioned. The second thing is our final exam review session will be Tuesday, April 4th. I believe it's the day. Yeah, Tuesday, April 4th, which I know is really early. That's way before our actual final exam. So I know it's tough to figure out what you want me to go over before then. But if there's anything that you, any topics, specific lectures that we've talked about throughout the semester that were particularly tricky for you, we often get, often the most common one is spectral signatures. But if there's any other particular lectures that were tricky for you, any other particular topics that were tricky, please post about it in the discussion board and just say, hey, can we go over this again in the review session and I'll create extra content so that we can go over it really, really at home, really make sure that you're prepped for the final exam. And then I'll also just review a couple things that I think are really key that you're likely to incorporate on the final exam and try and just clarify anything that looks like you guys might be struggling with through the discussion board. The discussion board's kind of our measuring stick for what we want to go over in the final exam. So please be active on it because it'll reflect in what we actually talk about in the review session. Make sense? Any questions about that? Okay. Anything else? No, we haven't done that. Is there any questions about the final exam in general as a whole? Yeah? So although there's like a, there might be like a designated room on the SSC, we can take it from anywhere. Yes, that's a good point. Thank you for asking that question. The whoever posts the schedule for the exams never looks at the form that they get me to fill out every single semester where I say the class is online because every single semester they still always schedule us a building. Don't go to that building. There's no in-person invigilation of this final exam. It's a fully online exam. It's fully through Canvas. There is no in-person writing. There's no in-person assessment. There's no in-person exam. It's all fully online. I know on your schedules it might have, there might be a building that has popped up on the schedule that says our exam is in that building. It's not. We won't be there. I emailed them about it, asked them if they could change it. They said they're working on it. I don't know if they have or not. Sounds like they probably haven't. I checked it today. It says ask prof. Okay. Ask me. We don't. No in-person. Don't go there. We always get people emailing us the day of the exam saying, hey, I am at the exam location. There's no in-person. It's an online exam. Okay. Drill that home. I know that's probably not you guys doing that anyways because you guys are actually in lecture. So, yeah. It's pretty much it then, right? Yeah. Okay. Sam's not here, but if you want to come down and ask us any questions about assignment five, you're welcome to. I'll put up the slides that Evan always has right now about what's due this week. I'll just leave it there. Yeah. Other night, you're welcome to head out and I'll see you guys next week. Bye. All right. Hi, everyone. Welcome. Hope you are all enjoying the sunshine we're getting. Just a couple reminders. So assignment six is due Thursday. You got blog post five, which is due Thursday of next week, not this week, and we'll talk about it on Monday. So a week today, I'll give you instructions or help provide some context for the blog post in class on Monday. Just a schedule for the rest of the semester. We're going over environmental change today and probably tomorrow as well. And then we'll probably start the future tomorrow. So talk about the future of Earth observation. And then on Monday following, we might finish off the future and then talk about Canada from space, which is the kind of lecture to go along with the blog post. And then the Tuesday after that, we'll have a final exam review session. So a week tomorrow, we'll have a final exam review session. And that will be the last day of class for us. So Sam has office hours this week. He should have just or is just finishing off, I guess, his office hours that are happening right now. And then he'll have some tomorrow Wednesday and Thursday as well. So we'll go over, I think I mentioned it already, but try to. I know it's kind of hard because our exam is way at the end of the exam period. But there's a discussion board up about final exam questions. If there's anything that you'd like me to go over during the final exam review session, please post it there. There's anything that, you know, does anything you post, I'll bring some stuff prepared to talk about. Otherwise, it'll just kind of be Q&A. And if you have questions, you can ask. And yeah, that's all it did. Okay, so today we are talking about how environmental change is monitored from space. We're going to talk about cyclical abrupt and gradual changes or patterns. This lecture does a good job, I think, of bringing together a lot of the different topics and concepts we've been talking about all semester. So it's a really good, not literally a review, but it's a really good review session in a way because it'll help us practice kind of bridging some topics that we've talked about in previous lectures. So I'll first just introduce the types of change that we'll talk about, which will be cyclical abrupt and gradual change. And then the data considerations for monitoring each of those kinds of change. And then for each kind of change that we'll discuss, cyclical abrupt and gradual, we'll go over a quick definition, we'll talk about using Earth observation data to measure these different kinds of environmental change, and we'll use some specific examples to kind of help finish wrap our heads around each of them. So just to provide some background context, we know that the Earth is constantly changing. We know that there's natural changes that occur, we know that there's human cause changes that occur, and we know that all of these different kinds of changes can occur at very, very, very different timescales. So we can have things like wildfires, forest fires. Do you have a question? They should be. Yeah. Do you not see them? You got it? Yeah. Okay. So we know that these changes, these environmental changes, whether they're natural, whether they're human cause, they can occur at really different scales through time and space. So things like forest fires, wildfires might burn over a couple of days, a couple of weeks. We know climate change is a phenomenon that's occurring over decades, potentially centuries. We know that in space, wildfires, forest fires might burn over, you know, 0.1 to 10 kilometers squared, pretty small, discrete area, whereas climate change is something that's potentially affecting the whole globe, the whole Earth. We know that it's important to monitor all of these different kinds of change, whether they are occurring at really small spatial scales, really large spatial scales, really broad or fine temporal scales, whether they are natural or human caused, because we want to be able to monitor and assess our impact on the environment. We want to be able to make sure that we are complying with different laws, whether that's related to things like, you know, resource extraction, mining, forest harvesting, making sure that we're complying with the laws that are set out in different political jurisdictions. We want to protect human health and we also want to be able to predict and understand our future so that we can start to plan now for how things might change with climate change. So based on what we have talked about so far in class, isn't a trick question. We've kind of at the end of every single lecture had very, very similar themes, concepts, ideas around what makes space-based satellite data really advantageous for monitoring different phenomena, whether that was oceans and fresh water or the cryosphere or the biosphere or wildlife. What were some of those advantages? Why do you think as a whole satellite data might be advantageous for monitoring something like environmental change? Yeah? You can see the whole earth totally. You've got really good spatial coverage. Yeah? It's standardized and consistent. Exactly. Same kind of things we've been talking about. Two really good points. It's efficient. We can get different kinds of information at different spatial and temporal scales. We can get different kinds of information from all across the electromagnetic spectrum. So lots of lots of different reasons why it's really, really advantageous and a really nice data set for us to be able to monitor change. Now, whenever we're monitoring environmental change, which is often just broadly one of the very common applications of space-based earth observation data, we have to consider what kind of data we need to do that. So we've talked about specification and characteristics of say, MODIS, Landsat, LIDAR, all these different kinds of data that we know we can collect. So now we're going to try and talk about it from the other perspective, which is now okay, rather than looking at a specific data set and understanding its characteristics, let's consider a specific change, a specific kind of environmental phenomena, and the characteristics or the data requirements needed to monitor and assess that change. So there's four that we'll talk about, the level of spatial detail, the region of the electromagnetic spectrum, the frequency of revisit, and the temporal dimension. So the level of spatial detail is just the smallest ground object that can be resolved in an image. And we know generally more detail generally implies less area covered by a single image and a bigger file size. Just as an example, you might require a moderate resolution image to detect an amount of forest lost or forest gained over a particular area. But we know that it requires a very, very fine level of resolution if we want to detect, say, changes at a single tree level. So really that just relates to spatial resolution, right? We talked about spatial resolution back when we had our resolutions lecture. And we know that spatial resolution can be a kind of a course, moderate or very, very fine scale. We have a course scales, things like ice sheets, very large things. We know generally for course scale applications, MODIS is a really, really good data set to use. We know at the moderate scale, mapping things like land cover, monitoring changes in forest lost and forest gain is a really common application. We know land sets are very common, moderate spatial resolution sensor. For fine scale, for monitoring things that may be an individual tree level, we know we need very, very fine spatial resolution data, maybe drone data or high spatial resolution satellite imagery that's at a spatial scale of maybe less than one meter or pixel size less than one meter. So our level of spatial detail ultimately relates back to our spatial resolution. We also want to consider what region of the electromagnetic spectrum the change is occurring in. The specific regions of the electromagnetic spectrum where the change occurs could have a large impact on the data set that we choose. So for trying to identify, trying to map broad classes like dead or live vegetation, we can maybe use a few wide spectral bands. If we want more specific classes like trying to understand different rocks, then maybe we need a very, very fine spectral resolution. Maybe we need lots of bands, lots of narrow bands. So that really just relates to our spectral resolution. If we want to measure something like vegetation health and how vegetation health is changing through time, we know that maybe all we need is the red and near infrared band. We can measure and monitor something like NDVI. If we want to measure and monitor how land cover is changing so that we can try and detect not just forest, but also soil, maybe different kinds of forest, deciduous forest, broadly forest, mixed forest, then we're going to need many, many more bands of spectral information. Okay, the next one to consider is our frequency of revisit. How often do you need to see an area to characterize the change? If we want to monitor and map logging or deforestation, something like that, probably the 16-day revisit time of land stat is going to be sufficient. Because that is, you know, a discrete event logging takes place of a particular area, maybe over a couple days to a couple weeks. So we don't really need a picture of somewhere every single day to be able to quantify how much an area has been logged. With something like a forest fire progression that's happening very, very, very, very, very quickly, we need to be able to have daily imagery of that area if we want to be able to detect and monitor that change in an appropriate way. So this is just an example of a fire perimeter map. So daily imagery would have been necessary here in order to kind of map out how this fire is changing on that scale. Now temporal dimension is something that we have alluded to a couple times throughout the course, but we haven't explicitly defined. So temporal dimension is how long do you need to collect data to be able to characterize the change? And that potentially varies with the scope of the study. So, for example, if you want to try and monitor changes in climate change or changes due to climate change, those changes are often really, really, really slow. So they require long-term information. Maybe you're going to have to collect decades and decades and decades of data in order to monitor or measure that change. But in contrast, a wildfire might occur over a couple of days. And all the information you're really interested in is either before the wildfire, during the wildfire, right after the wildfire. That wildfire only occurs for maybe a week, then maybe you only need to be collecting data for a week or two. So a week or two worths of data, so obviously very, very, very different from decades and decades and decades of data. So the change, how long that change is occurring for is something that's very important to consider. Now, we have aerial imagery generally dating back about the last hundred years or so, while satellite data we have only for the last 50 years or so. So when you're considering your temporal dimension, you're considering, based on that, what kind of Earth observation data you might need. If it's beyond, in the past 50 years, then you're probably going to have to incorporate some sort of aerial imagery. Just satellite data alone probably won't be sufficient. So we're now going to spend the rest of the lecture talking about different kinds of change and specific examples of those change. And for each of them, we'll run through a couple examples together as we go through them. But as we go through them, consider yourself as well, if we don't explicitly go through it together, try and consider the data that's necessary to detect the different kinds of changes that we're going to be looking at. When we talk about these different kinds of changes, think about what level of spatial detail would be required to detect it. Do we need a lot of spatial detail or not much, really? Think about what region of the electromagnetic spectrum the change is occurring in. If we're talking about vegetation and forests, then maybe the red and near infrared portion of the spectrum is sufficient. If we're talking about something like land surface temperature or sea surface temperature, then maybe thermal bands, parts of the thermal spectrum are more important. Think about the frequency of revisit. Think about do we need an image of a particular area every single day consecutively in order to map and monitor that area? Or do we just need one maybe once every week, maybe once every month? And then lastly, think about the temporal dimension that's required. Think about whether you only need data for a couple of weeks or a couple of months or whether you need data for years or decades or centuries to be able to detect and monitor the change that we're talking about. Okay, we're going to start by talking about cyclical change. Cyclical change is pretty simple, pretty easy to wrap our heads around. We're pretty familiar with it. Things that are repeating through time. So temperature goes up and down every day. Day length changes throughout the seasons. Snow cover in winters, especially up here in Canada. In winters, we had lots of snow and summers, all that snow kind of melts away. The greening and browning of vegetation every year. Animal migration, animal hibernation, Canadians going to Florida each winter. Some of our hobbies, like skiing in the winter, hiking and biking in the summer, all these are seasonal changes. They're all cyclical changes that we can see repeat through time. So a cyclical pattern is just a pattern that repeats over time. We are going to focus on vegetation phenology and vegetation phenology cycles and being able to monitor and track changes in vegetation phenology using two different kinds of data sets that will kind of compare and contrast a bit. We'll talk about using camera traps and then also using modus data. So we are pretty familiar with the fact that as temperature warms in the summer, snow melts, vegetation greens up. Vegetation greening up just means vegetation starts growing, deciduous leaves come out, and that vegetation just starts getting very, very healthy. And then as temperatures cool into the fall and as we get into the winter, the snow generally returns, the vegetation starts to brown, starts to die off, doesn't look nearly as healthy. Right? So we know this seasonal change of vegetation occurs. One way that we can monitor that change at a very fine spatial scale is with camera traps. So we can set up a network of cameras or just one if you're only interested in maybe one particular spot. And if we take a picture of our area of interest every single day, then we can get a single point measurement of red, green and blue spectral information every day for that particular point that we have the camera pointed at. So time lapse data from cameras is generally single point data. It's data collected just at the one spot that the camera has pointed at. It's very, very fine spatial scale because you can get very, very small pixels out of it. It has a very high temporal resolution. We can get imagery every single day. Sometimes it even has a higher temporal resolution if we want than what we can get with MODIS. MODIS only takes an image of an area every day at most with time lapse camera data. We can also be able to take images when there's poor weather. So when it's cloudy, MODIS won't be able to see an area. If it's cloudy, our time lapse camera can still see an area. And we get visible red, green and blue data. So we get spectral bands in the red, green and blue part of the visible spectrum. And we can use that kind of data to understand phenology patterns. We can use it to try and validate satellite data that we acquire. And we can also use it to analyze very, very fine scale phenology in space and time. So this is just an example of what a time lapse photography data set might look like. You'll see kind of the cyclical seasonal pattern occur through it. You'll see we'll start with the green vegetation. It'll start to get brown and kind of die off a little bit. You'll start to see snow and stuff like that start to come in. So this is one image taken every single day for all days of the year. This is just another example here. It starts with a snow melt. And you'll see the snow will melt away. We'll start to green up here. We'll start to see greener vegetation, healthier vegetation. And this itself is a really nice data set. We can get a really nice fine scale measurement of the vegetation phenology for these two particular areas that these cameras are pointed at. So how do we actually take that then and get some kind of meaningful measure from it? We're starting with just maybe hundreds of photos. 365 photos of the exact same point. We can take what we know about spectral signatures to try and derive some more information from this. So if we have a camera trap image here, a time lapse image, we're taking a photo of this particular area every single day. If we start here, we can see that there's not a lot of healthy vegetation here. There's a little bit of greenness kind of going on here, but lots of kind of brown, reddish action going on in behind here. So our spectral signature of that reason is going to look something like this. And here we got our blue band, our green band, and our red band. So the camera functions like any other remote sensing instrument. They have a defined spatial resolution. So they have a particular pixel size. And it just uses three bands rather than the kind of dozens of bands that we're used to with maybe Landsat or modus. It uses a red of green and a blue. So all digital cameras, camera that is on your phone kind of work in the exact same way. They take a measurement of red, green and blue visible light. And then by adding red, green and blue together in equal amounts, you're ultimately able to create a white color. So white represents things that kind of have maximum brightness, black represents things that have no brightness whatsoever. So you got red, green and blue here. We got our blue band, our green band, our red band here. So if we start here, we start kind of in the, you know, right after winter has ended, we have lots of brown, reddish colors going on here. We got our blue, green and red band here. We can see on our spectral signature, we got a very low amount of blue visible reflectance, a little bit of green and then quite a high amount of red visible reflectance. Now as we get into spring into summer, we'll see that plants will start emerging. We'll get much more of a green image. We'll see that visible red reflectance kind of drop down. We'll see that visible green reflectance start to increase. As we get deeper and deeper into summer, once we get fully mature plants, we'll see that fully take effect. Our red will kind of dip at its minimum point. Our green will kind of max and hit its highest point of reflectance. And then we get into fall. We'll see that same pattern reoccur. We'll see more kind of browns and reds coming out. We'll see this red reflectance start to increase. This green reflectance start to decrease. And ultimately, we'll get back to winter. We'll again see this high level of red reflectance and this lower level of green reflectance. Now, because a camera, again, works like any other remote sensing instrument, we can just derive a metric from these three bands to try to measure the health of vegetation in this area. So we can derive something called the 2G RBI metric, where you take two times the visible green reflectance minus the visible blue plus visible red reflectance. And then by getting that, you can get a high 2G RBI value when vegetation is very healthy. When we have lots of green reflectance, very low blue and red reflectance. And then we can get a low 2G RBI when vegetation is less healthier, when it's snessing, when it's brown or red, where we get much higher red reflectance and much lower visible green reflectance. If we plot that through time, so if we plot the 2G RBI metric that we've derived from our time lapse camera trap imagery, for every day of the year, all through the year, we'll end up with a curve that'll look something like this. And from this, we can identify key phenological events of that vegetation. We can see here the green up occurring. So we can see that steep increase in vegetation starting to get green, starting to kind of bud and show its leaves. And then ultimately, we can see it reach maturity kind of here where it levels off. So our 2G RBI value is kind of levels off here. And then we'll see a decrease again as we get into fall as we get into snescence. And we can take really nice measurements from this. We can say, okay, at what time of the year is green up occurring? At what time of the year is snescence occurring? At what time of the year is maturity occurring? We can also measure things like the growing season. So we can say, okay, how far away is this date from this date and get a sense for how long of the year or for how many weeks or how many months vegetation is actually growing? Really important implications of that data set in particular for farmers that need to understand at what points of the year they're going to be able to grow their crops. So you can get lots of different kinds of metrics from looking at a curve like this. But obviously, with time lapse camera imagery, gives us a detailed look at vegetation for a few select areas. Doesn't give us a view of the whole planet or necessarily even a whole country. Maybe we'll go away and set up one camera trap image if we're just interested in our backyard or something like that. Maybe we'll go and set up 30, 40 camera trap images. But we're not going to be setting cameras up all across the world. We're not going to be setting up millions and millions of cameras. So what if we want to look at vegetation cycles over much larger reasons or the whole planet? Well, generally for that we'll typically use modus data. So modus data gives us those daily satellite images at a 250 meter to 1000 meter spatial resolution. And modus really allows us to observe how vegetation changes throughout the year across the entire planet. So we can look at modus data the same way that we looked at that camera time lapse imagery. So in this case, this whole area might fall in one, two, maybe three modus pixels. But we can essentially look at the data. But we can essentially derive a similar metric. So from time lapse data, we're looking at the blue, green and red visible bands. We can get that 2G RBI value. And then for modus data, we can use red and near infrared. And this is a good example of why the NDVI metric is so valuable. You can see here the difference in red versus near infrared reflectance is really, really large. Whereas here the difference between visible red and visible green reflectance is not very much. So even though we can go away and get this 2G RBI value, all we're measuring is the difference of these kind of reflectance values here compared to here, which isn't changing very much. But in comparison to that, the near infrared portion of the spectrum, really, really, really low reflectance when the vegetation is unhealthy. Really, really, really high reflectance when the vegetation is healthy. So we can plot that with modus. We can get a very similar looking graph. So again here, if we say, okay, we want to look at the reflectance of this area, but now from a perspective of a modus pixel, we can say, okay, we'll start here. We start kind of in the browner, reddish kind of region. We got relatively moderate near infrared reflectance, pretty moderate red reflectance. We start to green up. We got much more mature healthy vegetation. We see that characteristic very, very high near infrared reflectance. Very, very low visible red reflectance. We get into fall. We start seeing that sinessing again. Leaves start to die. Leaves start to turn red and orange, those kinds of colors. We start to see much lower near infrared reflectance again, much higher visible red reflectance. And we can calculate NDVI from that. Same way we can calculate 2G RBI from our time lapse imagery. So we can see here for our healthy vegetation, we have that low visible red reflectance, that high near infrared reflectance, and overall we get a high NDVI value. We can see here for much unhealthier vegetation, maybe in the winter or fall, we get that lower near infrared reflectance, that higher visible red reflectance. We'll get a lower overall NDVI value. But similarly, just like with the 2G RBI value, we can plot NDVI through the year for every single day for each modus pixel. And we can still go away and try and identify key phenological events like senescence, like maturity, like green up, and then we can go away and try and calculate things like growing season. So this is a modus derived map showing growing season all across North America. You can see we obviously have much smaller, much shorter growing seasons as we get further North, as well as in mountainous areas. We have much longer growing seasons, kind of more in areas that are closer to the coast, as well as areas that are a little bit further south. And that growing season again is just a measure of how long we're in this kind of state here between senescence and green up. So with years of modus observations, we can see how these vegetation cycles might start to go. And those cycles might start changing with something like climate change. So if we can plot something like this, say NDVI, and we can see throughout the whole globe that cyclical change every single year, going up and down, and up and down, and up and down winter, summer, winter, summer, winter, summer, we can start to assess how those cyclical changes might be changing themselves. We can say, okay, if this is our growing season up here, we can see that the most positive part of the curve peaks. Is it shortening? Is it getting longer? Is it reaching a maximum higher level of NDVI, a maximum lower level of NDVI, so we can go away and try and understand how these cyclical changes might be changing through time? Okay, so just to review, covered two approaches there, we can get detailed information on vegetation cycles for a few areas of interest. And then we have modus data where we can get broad-scale information for potentially the whole planet. And both techniques where you're identifying changes in the spectral signatures of plants and then using different spectral bands. In the case of camera time-lapse imagery, we're using the green, blue, and red visible bands. In the case of modus, we're using red and near-infrared to get the NDVI metric. But in both cases, we're plotting that through time for every single day to try and pull out metrics like when greenups occurring, when maturity is reached, when senescence is occurring, and then try and derive from that information things like growing season. Okay, so really good example, exam question here. What are the data requirements if I want to measure phenological changes in my backyard due to this year's drought? And then in comparison, what are the data requirements if I want to measure global phenological changes due to climate change? So I don't want an answer here about just what data set would make the most sense. I want kind of justification through an explanation of the data requirements for measuring or monitoring each of these things. So if I want to measure phenological changes in my backyard due to this year's drought, what's the level of spatial detail I require? What's the frequency of revisit I require? What is the region of the electromagnetic spectrum I require? And what is the temporal dimension I require? So I'll give you guys a couple minutes to maybe try and brainstorm by yourself or ideally with a neighbor sitting close to you, what's the data requirements might be for each of these? And then we'll come back and try and answer them together. So I'll give you maybe two, three minutes, try and brainstorm the answers to these and then we'll talk about them. So we'll come back to our spatial material detail and see what we can do. Thank you. All three areas. All three areas. All three areas. All three areas. All three areas. All three areas. All three areas. Okay guys, let's try and talk about these ones. So what are the data requirements if I want to measure phenological changes in my backyard due to this year's drought? So let's start with level of spatial detail. What level of spatial detail do I need to measure philological changes in my backyard? Yeah. I need a very fine amount of spatial detail because we're just measuring my backyard. It's a really small area. Yeah, exactly. What is then the portion, what region of the electromagnetic spectrum would I require to measure those changes? Yeah. So what we need to measure is that near infrared and red would be the most ideal for sure. We could also get away with visible green and red. We talked about how the camera data can use the 2G RBI metric. So we could get away with that. But you're right in the sense that the most ideal, the best option would be red and near infrared. So I'm going to be able to measure my backyard. Yeah. Daily scale? Yeah, daily would be ideal. And then what about our temporal dimension? How long do we need to collect data for? Yeah. So it depends if you did for the year, then maybe you could see the long term effects of the drought. But if you were, I don't know if you didn't have the resources for that, then maybe you are a week before the drought during the drought and after. Yeah. Totally. Yeah. So I bear minimum just before the drought during the drought and just after the drought. So depending on how long the drought is lasting, maybe a couple months, maybe a year or two at maximum, but somewhere in that range of half a year, maybe two years maximum, something like that. Yeah, totally. So that would be ideal for measuring phenological changes in my backyard due to this year's drought. What if I want to measure, what are the data requirements for measuring global phenological changes due to climate change? So what is the level of spatial detail that I require that I would need? Yeah. You need a very broad spatial detail to be able to measure things with, um, measure incredibly large swaths of area at once. So I need a very broad amount of spatial detail. We're measuring the whole globe in this case. What portions of the electromagnetic spectrum would I want to be looking at ideally? Yeah. I guess it like heavily depends on what you're trying to measure. So if you're trying to measure like ice melt, you might use like lidar. If you're trying to measure. Well, so in this case, in this case, we're doing, we're saying vegetation phenology. Oh, vegetation. Yeah. So sorry, that's not there. But by phenological changes, I mean, vegetation phenology. That's okay. If you use a near infrared. Totally. So you would be able to calculate the NDVI. Exactly. Exactly. So you'd want near infrared or red to be able to monitor and measure something like NDVI. Some sort of metric for vegetation health. Absolutely. And then what is the revisit time that we would require for these phenological changes on a global scale? Yeah. Monthly. Monthly. Monthly might be a bit too coarse. We were talking, when we were talking just previously about measuring phenology, ideally you want daily data. So daily is probably our best option. You could maybe get away with monthly because you could see kind of the lows and the highs. But if we were trying to measure vegetation phenology traditionally, we would be trying to identify, say, the exact date of green up or the exact date of maturity or exactly how long our growing season was. We'd want to know it at least within a couple days. So we'd probably want at least a daily scale or close to. Maybe we could get away with a weekly, monthly would probably be a bit too coarse. And then lastly, what is the temporal dimension I would need? How long would I need to be collecting data for? Decades. Decades. Maybe even centuries. But if I want to measure changes due to climate change, I'm going to need a large temporal dimension of data required. So just to summarize, it's the same, all those answers that we just looked at. High level of spatial detail for monitoring my backyard, visible or near infrared ideal daily measurements required. Only one to two years of data required. So from that I can say, okay, well, what data set makes sense for me to use based off of these data requirements? Well, I can't necessarily, I mean, maybe I could find a camera that also measured near infrared. But if I just got a standard camera trap, that kind of comes closest to checking all these boxes minus the near infrared band. But we could use the visible and try and get the 2G RBI metric from using our camera trap. On the other hand, we wanted to measure global phenological changes due to climate change. We know it's just a low level of spatial detail required, visible and near infrared. We can get from a satellite, daily measurements required, probably decades of data required at minimum. MODIS gives us a really good option. The only thing that we're kind of restricted by with MODIS is really the temporal dimension. With MODIS, we have that coarse level of spatial detail. We have those visible and near infrared bands. We have those daily measurements. But with MODIS, we only have data back to about 1999 or very, very early 2000s. If we're trying to measure or monitor something related to climate change, two decades of data is maybe right on the brink of what would allow us to do that. Ideally, we'd probably want to go back into the kind of industrial kind of period of maybe 40s, 50s, 60s to really be able to get a good sense of the effective climate change. But given all that, given what our data requirements are, kind of our best option or closest to meeting all of those would probably be MODIS. So this is also a good example here of the data requirements that would be most ideal for a particular situation, for a particular application, and contrasting that and comparing that with what you do actually have available. When we have, you know, when I say, okay, I'm going to use a camera trap to collect time lapse imagery to understand vegetation phenology in my backyard. Sure, it doesn't collect near infrared data, but it checks all of these other boxes really well. So it's probably okay for me to settle with that. I've met the minimum data requirements necessary for me to detect that change because I can measure at least the visible red, green and blue portions of the spectrum. Similarly with MODIS here, checks all these boxes except maybe the decades of data required. We do at this point have over two decades of data acquired from MODIS, so we kind of just barely check that box. It's just enough, and we can probably settle with that. We can probably use that data to try and perform this analysis. So good job, guys. Really good example of an exam question. Really good kind of example of how I would like you guys to think through a question like this. So providing the data requirements for what would be ideal for measuring or monitoring particular change, and then ultimately suggesting a data set that would at least come close to meeting those. Maybe you can't check every single box perfectly, but you could kind of compare and contrast that a little bit in your answer. Make sense? Any questions? Okay. You'll probably see an exam question like this. Set it a couple times now, so don't be surprised. Okay. Next, we are going to talk about abrupt change. So abrupt change is a change that creates a rapid shift of the environment from one state or condition to another. So forest fire or wildfire is a really easy example of that. We can go from something that looks like this, really green healthy vegetation to immediately after a couple hours a day, something that looks like more like this, where we kind of have bare soil and lots of dead trees. We'll focus on three dominant types of change that are very commonly measured and monitored with earth observation. We'll talk about forest fires, forest harvesting, and then monitoring and measuring land cover and land use change. So first off, why do we care about monitoring and measuring each of these different kinds of change? Why do we care about forest fires, forest harvesting, and land cover and land use? They all in different ways can have large impacts on the global carbon cycle. It's essentially the short answer to that question. Forest fires will have carbon transferred to atmosphere in the short term, in the long term that forest typically regrows, carbon is often transferred back to the forest. Similarly, with forest harvest, carbon is transferred sometimes to the air, sometimes to forest products, houses, paper, things like that, but ultimately, usually, at least in British Columbia, it's mandated. It's illegal to not be planting in that harvested forest after it gets cut down. So generally, in BC and in Canada, forest regrow. Forests are replanted, and that carbon is transferred back to the forest. Land cover and land use change can be a bit trickier. How the carbon changes can depend a little bit on what's logged and what happens to those logs. But the big difference with land cover and land use change is typically the forest does not regrow. So carbon is generally not transferred back to the atmosphere. If we take an area that was forest, and then we convert it to a residential area or to an urban area, then there's not going to be that transfer of carbon back to the forest. And that can have a large impact on lots of different things, particularly for wildlife. Habitat fires are often a natural part of ecosystems. Recently burned areas can be really important habitat, particularly for large mammalian species like grizzly bears like moose. Forest harvesting, on the other hand, can potentially alter ecosystems in unnatural ways, can potentially remove important habitat. We know of a special unique case with grizzly bears in west central Alberta where they were found to be located in cup blocks or clear cuts pretty often. But that was more so as a surrogate or as a replacement for burned areas, for areas that had been subject to forest fire, because we've historically suppressed those very often. So lots of unique modern novel phenomena that can go on here with these different kinds of environmental changes and how they can affect different ecosystems. Again, big difference between each of these, being that land cover and land use, typically result in some kind of conversion. So some kind of change from forest to agriculture to some development that doesn't ultimately kind of cycle back into that forest regrowing. Now detecting abrupt change with earth observation imagery is typically done in a couple steps. So first we have to detect where and when the change occurred. Then we have to identify what type of change it was or what type of disturbance it was. Disturbance you can think of just as a discrete event or change. We often think of a disturbance as being an abrupt change, although you can also argue that an insect infestation is a type of disturbance, but that's more of a gradual change. So you'll see me use this word kind of disturbance a little bit loosely. More or less it means an abrupt change or some kind of change. So if we look at these two images, we can ask the question here, okay, we got a before wildfire image and after wildfire image of the same area. How would we actually detect or monitor, try and identify that a change occurred here by looking at this satellite imagery? Well, again, we would just use what our knowledge is on spectral signatures. So if we look at an area before a fire occurs, before wildfire occurs, we can see that characteristic large amount of near infrared reflectance that very, very low red reflectance. Immediately after the fire has burned that area, we'll see a much lower level of near infrared reflectance and a slightly higher level of visible red reflectance. And that's just due to that live vegetation, that healthy vegetation being burned and removed from that particular area. Now, it's not very efficient to look at the entire spectral signature. So similarly to how we did with vegetation phenology, we can derive a metric that kind of summarizes vegetation health for us. So we'll look at NDVI as an example. So before a fire, again, we have that high near infrared reflectance, low visible red reflectance, will have a high NDVI value. After the fire will have a much lower near infrared reflectance, much higher visible red reflectance, will have a pretty low NDVI value. So that means that if we look at a time series of NDVI over time for that particular area, we're going to get something that looks like this. We're going to see before the fire, healthy vegetation, we're going to see right after the fire, an immediate drop in healthy vegetation, so a much lower NDVI value. And then over time, we'll probably see that vegetation regrow again, which is characteristic of a forest fire. So if we were to just look at this graph, right, if I were to not give you this image or this image or this image to tell you what's going on here, you would be able to deduce just from looking at this time series of NDVI that a disturbance occurred somewhere in the range of 1994, and you'd be able to say after that disturbance occurred, the vegetation or forest regrew, because you'd see that slow increase in NDVI after the disturbance occurred. Okay, now how are we then able to differentiate forest fire versus harvesting? Forest fire and harvesting are both going to have a curve that looks like this. They're both going to have high NDVI values, a quick drop off, and then a slow increase after either that vegetation starts to recover after the burn, or after someone goes in and replants the forest that was cut down. So to try and differentiate fires and harvests, we'll typically look at the different shapes and sizes of the disturbances that we'll see across the landscape. Fires have very irregular patterns and perimeters and generally burn very large areas. So some that looks more like this, where the kind of purple area is the burned region. Harvest, forest harvest, particularly in BC and Alberta and in kind of Western North America here, have quite regular shapes and are subject to a relatively smaller area than forest fires typically are. So you can see here kind of in the pink, these are areas that have been recently harvested, and you can see you kind of get almost a checkered board pattern here that's kind of this, you know, pretty distinct pattern, pretty different looking than what you can see here. So by looking at those different shapes, we can identify whether it is a fire or whether it is forest harvest. So then how can you differentiate between, say, forest harvest and land conversion or land use change? Well, forest is going to regrow after harvest. You have to go in and plant where you cut down trees and those trees and that vegetation will eventually regrow. After a land use conversion, the forest won't regrow. So you'll see that quick immediate drop off of NDVI, but you'll see that low NDVI value persist through time. You won't see that increase as vegetation recovers through time, so then you're able to deduce from something that looks like this. Okay, this must have been some sort of land use conversion. So summary of that process step one, you detect the change where and when did the disturbance occur. You can identify a quick, large change in NDVI or some other spectral data. We're just using NDVI as an example here. And then you've got to figure out what type of disturbance was it. Was it fire or was it forest harvest? So for this, you can use the information on the shape and size of the disturbance to try and make it out. And then if you want to compare, okay, I've determined that it is probably not a forest fire based on its shape. You determine maybe its forest harvest, but it could still be a land use conversion. If a particularly small area had a disturbance occur, had an abrupt change occur, you might not know whether it's forest harvest or some sort of land use conversion or maybe some development is going on, unless you look back at your NDVI time series and see whether there's a recovery in the vegetation after the disturbance occurred. If there's a recovery after the disturbance occurred, then it must have been a forest harvest or a fire. If there was not, then there must have been some sort of land conversion going on there. Now there's this kind of very impactful paper that came out in 2013 by Hanson et al. And they used Landsat data to map abrupt changes across the entire planet from 2000 to 2010. So this was a particularly large undertaking because generally we know from kind of discussing in class a little bit, if you're looking at things on a global scale, you probably want to use MODIS data. A 250 meter pixel size is a little bit easier to use when you're processing data for the whole planet than say a 30 meter pixel size. But Hanson et al. went away and said, well, that may be true, but for monitoring abrupt changes, for trying to map and detect forest fires as well as forest harvesting and other kinds of land use and land cover conversion, like the creation of agricultural areas, for example, that 250 meter pixel size isn't ideal. It's better to have a more moderate spatial resolution like the 30 meters from Landsat. So again, I kind of answered, I guess the question that's up there. I meant to pose that to you guys. But essentially Landsat is a bit more ideal for mapping abrupt changes because the size that abrupt changes occur at are generally a bit more fine than say a vegetation phenology change covering the whole world. Forest harvest, forest fire, conversion of areas to agricultural regions are a little bit more difficult to monitor with a 250 meter pixel. There's much more ideal to look at them with a 30 meter pixel. So they were able to come up with this map if I can pop it up. See here, if I can do this mouse working. I just wanted to show you guys that we could explore a bit just to give you guys a sense of quickly the breadth of this information. So this is a map here showing I'm going to pop up their loss extent gained. So here we got forest loss across the whole planet from 2000 to 2012 or 2019 in red. So we got forest loss in red, forest gain in blue, both loss and gain in purple. And then we got forest extent. So just what areas across the world have forest and green here. So we zoom in on say British Columbia here, where we kind of are. So you guys can you guys kind of see the spatial patterns going on here. What do you think this kind of big red region is here? Fire? Yeah, big wildfire that occurred. What about what about these kind of these kind of blue things here? Couple of blocks. Couple of blocks. Yeah, those are clear cuts. So in this case, these blue areas, these underwent forest gain. So these would have been areas where forest was harvested prior to 2000 and then planting occurred and then the vegetation kind of regrew here after So in this case, these blue areas, these underwent forest gain. So these in this case represent areas of forest gain. You can see if we go kind of back out to a coastal area area that you guys looked at in your assignment recently. So we got Nupka Island here. So we can see both some forest loss and forest gain. Couple of areas that underwent both forest loss and forest gain. Hopefully you guys figured out in your assignment when you were in the forest. You guys figured out in your assignment what kind of changes this, what kind of disturbance is going on here? Or you're about to get mad at me because you gave the wrong answer. What do you guys think this was? Couple of blocks. Yeah, there's clear cuts. There's forest harvesting. So sorry if you got that wrong. But that's what's going on here. You can make out kind of from the shape of these, these aren't those big kind of widespread random patterns that we'll see with, you know, forest fires like we'll see up here with these big guys. Those are more of those kind of discrete cup block looking things. The point is this is a really, really invaluable data set. We can look across the whole planet here at all the forest loss and all the forest gain that's occurred from all different kinds of disturbances. So really, really valuable data set. In this case, at a 30 meter spatial scale. Okay. Back to here. So they were able to deduce kind of some summary statistics from that, from that Henson et al paper. They were able to find that from 2000 to 2012, 2.3 million kilometers squared, a forest were lost, 0.8 million kilometers squared, a forest were gained. And also assess some spatial trends of deforestation. They found that deforestation in Brazil decreased through time, but increased in other tropical countries. In this case, they have Indonesia here, showing that deforestation increased in Indonesia through time. So just an example here of a really, really valuable data set, a very, very quantifiable level of forest loss and forest gain that can help us really understand what our impact is on the forest. In this case, from deforestation, from forest harvesting, as well as what natural causes may have an impact on the forest. Things like wildfires and other natural disturbances that we can assess. Okay. I'm going to dive in now to a couple of the different uses or applications of the ability to detect and monitor some of these abrupt changes, including wildfire forest harvest land cover land use. So I'll keep going for about another 10 minutes or so. Sam is not coming today, but he is coming tomorrow, so I'll leave like 10 minutes or so at the end of class in case you guys have any questions about the assignment for me. Okay. So with wildfire detection, we can generally use satellite imagery to detect fires. So for monitoring hotspots, we can use it to monitor fire progression. So to see how large a particular fire is getting through time. We can calculate burned area to try and get a measurement for the extent of forest fires. We can try and also monitor vegetation recovery after a fire. So really good example here, kind of good practice if you're trying to study for the exam, walking through what the data requirements might be for each of these different kinds of change, because even though they're all wildfire related, there's going to be different levels of data requirements associated with each of these for detecting fires, for example, if you want to know just where a fire is and whether one is occurring immediately, you need to be looking pretty much all across the world all the time. So you need a daily revisit, you don't need a particularly large temporal dimension, you just want to know where it is in real time, but you do need a really frequent revisit. On the other hand, for say calculating a burned area, all you need is an image before the fire occurred and an image after the fire occurred. So you don't need a very, you don't need a high frequency of revisit, you can just have an image before and an image after. So just an example to kind of get you guys thinking, hopefully help you if you're trying to figure out what to study of some practice questions. Hot spot detections in particular are a cool example. These are areas on the ground that are distinctly hotter than their surroundings, and so we use thermal sensors on board different satellites to be able to measure the surface temperature and thermal properties of targets, which allows for early detection and coordination of forest firefighting efforts. So we might use this case here, if you guys want to check it out, it's kind of a cool interactive map. That's a combination of the veers data sets, so the nighttime data set that we talked about, as well as Landsat and Modus and the thermal bands on board, those satellites to be able to go and monitor hot spots. So these are essentially all areas where wildfires are essentially have potentially a high probability of occurring. These are areas where they've noted from the satellite data, okay, this particular spot on the ground is much hotter than its surroundings at the moment, so maybe there's a fire going on there. And we can do this, we can collect this data, map this out in near real time, so we can find out about it almost immediately. So really valuable for planning for communities and for wildfire fighting. So this is kind of just an animation of what that would look like, this is just from 2002, you can see kind of the seasonal pattern of green up and of snow, as well as where these hot spots are located in this case through continental United States. Now we can also look at that for the entire globe, so this is a similar animation showing wildfires occurring all across the world. And again, when it comes to kind of linking back to trying to classify these different kinds of change, because that's definitely a question I'll ask you on the final exam is trying to identify and classify whether something's a cyclical or an abrupt or gradual change. You really got to think about kind of the details of the context of the change. I'll make it really clear to you in the example or in the question on the exam, but just as an example, when we think about fires, fires inherently one single fire, that's an abrupt change. But we can also think about fires as a cyclical change. We have fires that occur seasonally, we have fires that will occur in a greater frequency in the summer and in a much lower frequency in the winter. We could also, if we kind of plotted the frequency of fires through time, all the way from, you know, say the 50s all the way up to the current time, then maybe we would potentially see an increase in the frequency of fires through that time, and that'd be more of a gradual change, maybe related to climate change, or some other kind of change. So the point is, fires is a really good example of something that could be an abrupt, a cyclical or a gradual change. So kind of making sure you read the question carefully, looking at the details that we provide you in the question to try and identify what change it might be is really important. Okay, with satellite imagery from space, we can also detect forest harvest, so see where areas have been chopped down essentially. We can calculate harvested areas, calculate how much area has been subject to forest harvest, and again we can monitor forest recovery. We can get a sense for how the vegetation is regrowing after the forest has been cut down. Again, different data requirements potentially for each of these, so try and think those through on your own time. One particularly useful endeavor for monitoring forest harvest with satellite data is that government and industry will often make targets on forest harvest, so they'll set a limit oftentimes for how much forest can be harvested by industry. And then generally industry have to create reports about the amount of harvesting they actually do. Satellite data is a really nice, independent, standardized and objective data set that allows us to go and look at a region, look at an area, and very distinctly quantify the amount of forest harvest that's occurred so that government or other agencies can go and look at the numbers, the metrics that industry has provided and say, okay, did you guys actually cut down as much forest as you say you did or not? So it acts as a sense, a kind of validation data in that way as well, where we can compare it to how much forest is being harvested on the ground according to those companies. Okay, I'll end talking about this last example, the idea of land cover versus land use, and I'll use forest harvest as an example. So on the final example, I'll definitely ask you a question about what the difference is between land cover and land use, and I'll use the deforestation versus forest harvest as an example to kind of walk us through. So with deforestation, the forest is cut down to start. With forest harvest to start, the forest is also cut down, so it starts in the exact same way. With deforestation, farms, plantations and communities may be developed, so there's some kind of land use change going on there. With forest harvest, the trees are planted and regrown. So the land cover change in the case of deforestation goes from forest to maybe soil or crops or buildings, whereas with forest harvest, it goes from forest to bare ground initially when the trees are cut down, but then eventually back to forest. The land use change in the case of deforestation might go from forestry or just nature reserve or something like that to farm plantation, maybe community. The land use change in the case of forest harvest goes from forestry to forestry. There's no change going on there. So in the case of forest harvest, land use never changes. Land cover does change, but it just goes from forest to bare ground then back to forest. In the case of deforestation, we see that there's a land cover change, a permanent land cover change, not one where it goes from something to something back to something. We see a permanent land cover change here, and we see a land use change, where maybe we're going from forestry or natural environment or something like that to a farm or plantation or community. So a land cover is the physical and biological cover over the surface of land. So that might be water, vegetation, bare soil, something like that. It's the physical material on the ground. Land use is related to human activities like agriculture, forestry, urban construction. It's about what humans are actually using that parcel of land for, whereas land cover is about what physical material is actually located on the ground. So often a change in land cover could be temporary. So logging, forest harvest, the land use does not change. Still forestry, it's still managed forest. But the land cover may undergo a temporary change. You might go from forest to bare soil, then regrowth of that vegetation, so bare soil to shrubs, ultimately back to forest. While a change in land use could be much more permanent, could be long term. So in the case of a residential development, maybe the land use is changing from natural ecosystem to residential area. You're going from in land cover forest to concrete, that's it. That's the permanent change. There's no regrowth after that. Land use versus land cover change is an important distinction. They have very, very different consequences. If land cover changes, then potentially that land cover could change back. If land use changes, generally a much more permanent long term change. There's often much larger impacts of a land use change than a land cover change. So this is an example of a land use change here, some satellite imagery from Bolivia, from Landsat, going from forest to plantations or agricultural environments. So you can see here, land use change. Land cover is also changing, maybe going from forest to crop lands. Land use maybe going from natural environment to agriculture or agroforest. On the other hand, this could be an example here of no land use change occurring, but a land cover change occurring. In this case, a wildfire in Yellowstone that occurred. So the area maybe went from forest to bare ground after the fire, but then after some time, the vegetation regrew, went from bare ground, land cover back to forest. The land use never changed. Make sense? Any questions? Okay, sweet. We will end there for today. We'll pick back up, we'll finish this off tomorrow, and then we'll start talking about the future, and then we only have two lectures just after that next week, and that'll be it. See you guys tomorrow! All right, hi everyone. Happy Tuesday. I'm going to finish off talking about change, and then i will Probably start talking about the future lecture. Might not finish it. We might finish it next monday. Then talk about the last blog post, or second last blog post. Then we'll have our review session after that. As i've mentioned a couple times, if there's anything that you Want me to go over in the review session, which is a week Today, please post it on the final discussion board. I'll make sure that i have slides and other content to try and Go over it and clarify anything that you're interested in me Talking about. You got assignment six due this Thursday. Same will be here at the end of class if you need to ask any Questions to him. Then, like i already mentioned, blog post five will talk about Next monday in class. Back to where we were here. We've talked about cyclical and abrupt change so far. We started by talking about data requirements for detecting And monitoring change. Talked about the level of spatial detail required. The region of the electromagnetic spectrum. The frequency of revisit required. The temporal dimension required. We will continue to kind of apply that style of questioning And those concepts throughout this latter part of the lecture. So we talked about cyclical specifically talked about looking At vegetation phenology patterns using camera time lapse data Versus mottus data, the different kinds of vegetation Phenology patterns we can get a sense of from those data sets. We talked about abrupt change in a lot of detail. We talked about forest harvest. We talked about wildfires. We talked about land cover and land use conversion. Kind of defined the difference between land cover and land use. Land cover just being the physical material on the surface of the Ground. Land use being how humans actually use that Particular area or parcel of land. Kind of the differences you might see with forest harvest Versus deforestation in terms of how land cover might change and How land use might change. Then we just ended the last thing we talked about was just some Specific applications of the uses of being able to detect and Monitor fires, forest harvesting and land use and land cover Conversion. So the last kind of change we're going to talk About is going to be gradual change. So gradual change is a slow shift from one state or condition To another or a trend over some extended period of time. So we can look at something like rising sea surface temperatures As an example of a gradual change, something that's happening Over decades and decades, maybe centuries of time. We can also see if we look at this graph here of global sea Surface temperature, we can see that annual kind of cycle of up And down and up and down and up and down. So this is another reminder of an exam setting being weary of the Context and details i give you or we give you around trying to Identify whether certain kinds of changes are gradual or Abrupt or cyclical. Sea surface temperature itself could be cyclical or could be gradual depending if you're talking about just the seasonal Cycle of sea surface temperature, the daily cycle of sea surface Temperature, those might both be cyclical types of change. But if we're talking about the slow increase in sea surface Temperature measured through time over decades to centuries due To something like climate change, we'd be talking about more of a gradual change. Now a lot of the focus in this course and in this lecture is Kind of more on forests because a lot of the research in Particular that comes out in the earth observation, remote sensing Realm of work is often forest focused. So what kind of gradual trends can we see in forests in particular? We can see for example slow mortality of trees due to Insects or disease. We can see gradual regrowth of vegetation in forests after a Disturbance event like a fire like a forest harvest event. We can also see potentially reforestation so we can see maybe Areas that are transitioning from agriculture or develop land Back to forests, maybe after particular areas where abandoned Or something like that and slowly vegetation starts to encroach Back in that region. These are all gradual changes. They all happen very slowly and they're kind of just very gradual in a particular direction. We'll talk about these three examples. So the one that is kind of most relevant at least in a western North America, british columbia, alberta type sense, is the Western mountain pine beetle. So the mountain pine beetle is a native insect to western North America, lives in the barks of trees. And even though it's only about five millimeters in size, they've Caused a really large amount of damage to forests in b.c. And then alberta and then kind of the western portion of North American forests. Now they have been native to North America for a very long time. But as minimum or as the lowest temperatures in winter have Slowly started to increase, meaning it's not getting quite as Cold in winters for as long of periods of time as it used to. These beetles don't die off as easily in the winter anymore. So oftentimes the really cold extended temperatures that we Would get in b.c. and alberta would kill off these mountain pine Beetles in the winter. But because that's happening less, they've been able to expand Their range and have caused quite a lot of damage throughout Western North American forests. So in the most kind of recent outbreak from the 90s to the 2000s, it's affected about 18.1 million hectares of forest. And they've killed a total of about 710 million cubic meters of Timber. If you look at their map kind of across b.c. And alberta you can see they're kind of everywhere. So they've caused a ton of damage throughout the 90s and 2000s. The way that they actually kill trees is essentially adult Beetles will lay eggs under the bark of pine trees. And the beetles prevent the movement of nutrients through the Tree, effectively kind of choking the tree. And then adults will also release a fungus that damages the Tree. So there's kind of two ways that the beetles actually Understandably impact these trees. One just the beetles prevent the movement of nutrients and then The adults also release a fungus that will damage the tree. If you look across a forest that's being infected by a mountain Pine beetle infestation, you'll often see trees that are in One of three stages. A green attack, a red attack, or a grey attack. The green attack is where the adult beetles have tunneled under The bark and the tree begins to die. But the needles still remain green. So the beetles don't immediately kill the tree. It's a slow process. And so we have this first stage of the green attack where the Leaves still appear relatively green on the tree. Then we have the red attack where after the tree has been dead For a couple of months, the needles begin to turn red. Then we have the grey attack where the dead needles eventually Fall off the tree and just a bare dead tree is left. If we look at an image like this, so this is where a mountain Pine beetle infestation has occurred, you can kind of see the Green, red and grey stages of attack. You can see some green trees, you can see some red trees, you Can see some grey trees here. So you can see all of these stages of attack. Now, for a moment, just remember that from a satellite's Perspective, when we're talking about space-based earth Observation, this might all fall under one pixel, right? We're generally not getting individual tree measurements From satellite-based earth observation measurements. So an insect outbreak appears particularly gradual from space Because the needles do not fall immediately when the tree dies And not all trees die at the exact same time. So if we're taking a Landsat pixel, for example, we'll, you know, See maybe a pixel full of healthy trees, maybe a pixel that has a Mix of healthy and dead trees, and maybe a pixel that has all dead trees. Each of these might have a slightly different spectral response, but They're going to slowly kind of gradually move from this state to This state. It's not like we have healthy trees and then boom, we Wake up a week later and all of a sudden all the trees are dead and All their leaves have fallen. The way that insect infestations generally work, so you start with The healthy trees, the infestation starts to occur, a couple of trees get Hit, then a couple of more, so gradually transitions into this kind Of more dead kind of forest like we have on the right here. If we plot like we've been doing to be able to detect and monitor These changes through time, if we plot NDVI, or no, we're not We're not applying NDVI on this one. We are looking at the spectral signature. So if we look at the spectral signature to be able to derive our NDVI values, we can say, okay, well, if we look at a pixel that has a healthy forest, We get, again, that characteristic low visible red reflectance, high near Infrared reflectance, with a high NDVI. Slowly as the infestation occurs and gets worse and worse, we'll Gradually, on a pixel-by-pixel basis, see a slightly lower near Infrared reflectance, slightly higher visible red reflectance, more of a moderate NDVI value, and then eventually when we get to lots of dead trees, we'll see a much Lower near Infrared reflectance, a much higher visible red reflectance, Ultimately a pretty low NDVI value. So again, we can plot that through time. So this is our NDVI time series, going in 85 to 2010 in this case. We have our NDVI value on the left here. Now, hopefully you can remember, think back to yesterday, I know it was a while ago, But hopefully you can remember what this graph looked like with, say, An abrupt change. When there was forest harvest or a fire, we saw high NDVI And then a quick drop, then we saw recovery. With the insect infestation here, we're seeing high NDVI, a slow, gradual drop Of NDVI, and then recovery after the infestation has finished. So we get healthy trees here, slowly a mix of healthy and dying trees, Gradually to a larger proportion of dead trees till we hit this low value of NDVI right here, and then we'll get that vegetation regrowth eventually. So again, we can see here, if we compare over time, we'll get a slow, gradual change From a spectral signature like this, high near infrared, low red, moderate near infrared, Moderate red, low near infrared, high red, versus an abrupt change. We'll quickly, maybe in just a couple of days, couple of weeks, go from a spectral Signature that looks like this, very low red, very high near infrared reflectance To all of a sudden, boom, very low near infrared reflectance, very high visible red reflectance. So side by side, this is what that might look at. So in an exam, I might also give you a graph that just looks like this, and say, Okay, hypothesize to me what kind of change you're likely seeing here. Do you think it is a gradual change and abrupt change? If you think it's a gradual change, what might it be? Is it an insect infestation? Probably if it's occurring over maybe five, ten, fifteen years, it's something like an Infect insect infestation. If it's occurring over decades and decades and decades and maybe up to centuries, Like sea level rise does, then maybe it's not an insect infestation. With fire and harvest, we know that if it's a quick drop off of Venn DVI, It's probably an abrupt change, maybe something like forest fire or forest harvest. If we see that vegetation recovery after the disturbance, That it is indeed probably a forest harvest or a forest fire as opposed to a land use Conversion where maybe we'd see that steep drop and then it would just stay low Because the area has been converted to a residential area, a development, something like that. Now, again, while fires burn most trees, mountain pine beetles will only attack certain trees, So the amount of damage will depend on what species are in the forest. We might, if we're looking at a gradual change, say from insect infestation, say from mountain pine beetles, We might see a pretty distinct drop off if it's a pretty intense infestation. But maybe the beetles only are attacking, say half of the trees or something like that. So the decrease in NDVI might not be as drastic as it is up here. Mountain pine beetles are known to only infect lodgepole pine stands. Lodgepole pine is a particular species of tree that is pretty common in parts of British Columbia and Alberta. But if it's a mixed stand of trees where maybe there's lodgepole pine trees, but also a bunch of other trees, Then the infestation might not be as intense, so you might not see as large of a drop off. So here we go from an NDVI of.7, maybe down to.1, whereas here we go from.7, maybe only down to.4 or so, If there's not as many trees being infected. So this is just an example of what that might look like from a satellite, what a mountain pine beetle outbreak might look like from a satellite. This is just south of Prince George, some more kind of northern BC. And you'll see we're going here from 2003 to 2007, we're having a kind of slow gradual transition going from very, very green, healthy forest, to slightly more red, a bit more of a mix here, and then ultimately to a kind of large, widespread area that's been infected by a mountain pine beetle infestation. Just question for you guys, to just get you going, what do you think these are? These little pinkish things right here on this satellite image. Couple of blocks, absolutely, yeah, hopefully you guys are nice and familiar with how those look by now from a satellite. Okay, now with mountain pine beetles and several different kinds of insect infestations, you can get one particularly unique phenomenon that you can quantify from space. So here in this example, we're starting with our healthy vegetation, healthy forest here, you can see a couple of cup blocks there, some newer ones, also a couple of cup blocks here that look like they've started to regrow a little bit. There's been some vegetation regrowth in them. And then into 2004 here, we're starting to see some more damage, starting to see some less green areas, less healthy forests, and then boom here in 2006, we got pretty widespread damage from mountain pine beetle infestation. Now, following that, sometimes in order to save the monetary or economic value of the timber that's being infected by mountain pine beetles, sometimes industry will actually go in and perform something called salvage logging, where they go and cut down all the trees that are either partially semi or fully infested by the beetles. To try and get some lumber, some timber out of the stand, that essentially is going to have no value once it's fully infected by beetles, and then kind of all decomposes and starts regrowing. So sometimes you'll see the infestation will gradually occur from the mountain pine beetles, and then in that same exact area, you might see some forest harvest occurring, some cup blocks being created. So that might look like if we plotted it as NDVI through time, something more like this, we might see that high level of NDVI representing a healthy forest, and then slowly that bit of decrease, but gradually through time, and then boom, right around here, we said, okay, this forest is kind of toast. Let's try to get some value out of it still. Let's go and harvest what's left there. So this kind of quicker drop off here would represent some salvage logging. So healthy forest, insect infestation, salvage logging, vegetation recovery. All things that I'm hoping you'll be able to describe and identify on a final exam. So this is just what that would look like, what each of these events would look like from space in terms of their satellite imagery. Nice green, healthy forest, insect damage here, salvage logging here, eventually some vegetation recovery. Any questions about that? Okay, see. Forests that are impacted by mountain pine beetle are also more likely to burn, which is sometimes the reason that will also just go in and salvage log instead of just letting it burn. So mountain pine beetle followed by fire is potentially another example of a gradual change followed by an abrupt change. So in the example we looked at here, we said, healthy forest, gradual change from insect damage, abrupt change from salvage logging, gradual change again as vegetation recovery occurred here. But without looking at the imagery and identifying the patterns of the abrupt change spatially to say, okay, was it clear cut or the creation of cup blocks from forest harvesting or was it a fire? We wouldn't necessarily be able to know that. But this is just an example where if I didn't give you kind of the context of this satellite imagery here and what the disturbance looked like here, then you would look at say just this graph and be able to say, okay, healthy forest, gradual distillation, gradual disturbance, gradual change, abrupt change, and then gradual change afterwards again. This might have been an insect infestation. This may be salvage logging. Maybe it was just a forest fire and then here probably vegetation recovery. So it could be a fire, could be forest harvest depending on what the shapes of those disturbances look like. Okay, there's a couple other types of gradual change in forest that we can observe from satellite imagery. We can observe mortality from disease and just overall change in forest health. It's not just insect infestation. We can also observe as climate change occurs areas may be becoming more unsuitable or potentially more suitable for forests. So as climate changes and species of trees of vegetation that are kind of on the brink of their range, kind of on the edge of the environmental characteristics that they like to live in, as the climate changes, they might either contract their range a little bit or expand their range a little bit. But that'd be a slow process. That'd be a gradual change through time. So these are some of the other kinds of gradual changes that we can often identify with satellite imagery from space. We often care about monitoring these gradual changes because even though they're not abrupt, they still can have very large impacts on carbon storage and habitat over time. If you think back to how large of an area was impacted by the mountain pine beetle infestation in BC and Alberta, gradual change through time, but really, really large consequences for carbon storage, for timber, for the forestry industry, for wildlife habitats, lots of lots of different implications. Now we talked about land conversion so far really as an abrupt change. We talked about the conversion of forests to say development, the conversion of forests to say agriculture being a pretty abrupt change, but not all land conversions are abrupt, especially if you're going the other way around from say a development or agriculture, maybe back to a nature reserve or natural ecosystem or forest. If we go from an agricultural area to a forest, or if we go from say a developed area to a forest, that's also probably going to look like a gradual change. A really nice example of that is Detroit. So Detroit had this large decline in manufacturing jobs between kind of 78 and 2008. And as a result, the population of the city substantially kind of dropped off. And with that decrease in population, a lot of neighborhoods just became abandoned. So while a lot of American cities were growing in population, Detroit during this time had a lot of abandoned areas increasing in their neighborhoods. And so what you saw from say looking at an NDVI value around Detroit is the development of these Detroit neighborhoods would have been an abrupt change. So what'll look like this? Boom, quick drop off of NDVI, and then it would have stayed low because it's converted into concrete or some urban area. But the abandonment of those neighborhoods and then vegetation slowly recapturing and re-growing in those previously concrete areas resulted in this slow gradual change back to an increased level of NDVI. So this is just a map showing the changes in NDVI from 1975 to 2005 in Detroit from Landsat images. And we can see increases in NDVI across the city. So this being 1975 NDVI values, this being 2005 NDVI values. And again, this isn't because there was more green space or more parks being created. This is because there were so many buildings that were abandoned and uncapped that there was this large encroachment of vegetation back into those previously concrete areas. Yeah. Why is it like the major roads and like downtown? Why have they also increased NDVI? Is it like this or less cars or something? So wouldn't be due to cars? Probably similar thing, it would be due to the areas directly adjacent to the roads, so essentially on the roads, just being abandoned. So there'd just be abandoned buildings there as opposed to maybe where there was previously corner stores, grocery stores, more bustling things where people had to maintain the buildings and stuff like that. All of a sudden those buildings weren't being maintained, so then we saw this kind of vegetation encroaching in the area. That was just kind of widespread all over the city. Yeah. So the changes that we were able to map using Landsat are not always disturbance or habitat loss. There's also forest gain. There's also things like vegetation recovery after forest harvest, after forest fire. And those are equally as important to monitor and detect as the disturbances themselves where we're losing forest because it allows us to get an entire holistic view or pay to whole picture about carbon storage and about the health of our forests and ecosystems. Okay. So just in review, one more time, we talked about four data requirements for monitoring change, detecting change, the level of spatial detail, the revisitability of the forest. The region of the electromagnetic spectrum, the temporal dimension required. And then we talked about three types of change, cyclical, particularly with vegetation phenology, abrupt change, particularly wildfires, forest harvest, land cover, or land use change. And then also we talked about gradual changes, particularly insect infestation, forest recovery, and also potentially land cover or land use change. Okay. So I want to give you guys a moment to practice. So I want to try to get you guys to match these graphs that are showing different kinds of change to the potential change that might be occurring there. So I want to give you a moment to try to practice this. So you want to kind of match a B, C, D, and E here with one, two, three, four, and five, and then also tell me whether it is a cyclical abrupt or gradual change. So I'll give you a couple minutes, maybe two, three minutes to go over that either by yourself, hopefully, maybe with a neighbor sitting close to you. So you guys can chat about it. Good practice here for the final exam. And then I got some more practice questions right after that here that will go over after that. So let's go over these first. We'll go over the practice questions, and then we'll move on to the next lecture after that. So I'll give you a couple minutes here. Okay guys, we can go over these and then we can move on. So let's start with the, start with graph A there. What kind of disturbance cyclical abrupt or gradual is graph A? Yeah. Gradual. Exactly. And what kind of specifically what kind of disturbance do you think it is? One, two, three, four, or five? Five. Reforestation. Yep, definitely. Nice. What about graph B? What is going on in graph B? Yeah. Abrupt. Abrupt. And you think it's fire. Why do you think that? Because it drops abruptly and then it gradually grows out. Exactly. Because you see that abrupt change, then you see that gradual increase in NDVI after suggesting vegetation recovery, probably a fire. Nice. What is C? What do you think is going on in C? Yeah. Yeah. Gradual change. And what specific kind of gradual change? One, two, three, four, five. Which one, sorry? Insect damage. Yeah. Probably an insect damage and then followed by some vegetation recovery. And then D. What is D? Three. Clearing a forest and building a parking lot. Yep. Nice work. That would be an abrupt change. And then lastly E. Last one left. One. Vegetation, phenology, cyclical change. Nice. Make sure on, well you'll probably see potentially, maybe, who knows. On the final exam, you know, you might see these graphs or you might come across these questions. Or you have to identify the multiple changes going on within them. Right? So for B, for example, there is an abrupt change. But that is followed by a gradual change technically. So you have the abrupt change of the, in this case, probably a forest fire. And then the gradual change of vegetation recovery following that. So make sure that in an exam question you're specifying kind of all the changes going on in that particular question. Nice. Straight forward. Okay. I will continue with the future lecture. But I do want to give you guys a couple minutes to try and practice answer these questions. So I'll give you how long do you guys want? Three, four minutes. Something like that? Okay. Give you about three, four minutes. Go over these. Ideally again with a partner, if not. Maybe just by yourself. Practice them. In an exam practice questions. And then we'll go over them together. And then we will move on to the future. I'll probably lecture on the future for about 20 minutes or so. Maybe 25. Might probably won't get through it all. But then we'll wrap it up. And then I'll finish it off next week and then talk about the last blog post. So take a moment to try and answer these guys. And then we'll go over them. Okay. Let's do these and then we can move on. So what are the four data requirements to consider when using satellite data to detect change? Easy one. Yeah. Yeah. Exactly. Level of spatial detail. Frequency of revisit. Region of the electromagnetic spectrum. And temporal dimension. Can someone describe the data requirements necessary to detect sea surface temperature rise due to climate change? So what is the level of spatial detail? Frequency of revisit. Region of the EMS. And temporal dimension necessary to detect sea surface temperature rise due to climate change. We can go one by one if we want. So what is the level of spatial detail required? Yeah. Very coarse spatial detail. Very totally. Yep. Very coarse. Very broad spatial detail. What is the frequency of revisit required? How often do we need a measurement of sea surface temperature? Yeah. Yeah. Probably monthly. We don't necessarily need it every single day. But we need enough to be able to kind of get a sense for the variation throughout the year. So probably monthly. Weekly or daily would also suffice. That one's pretty flexible. What about the region of the electromagnetic spectrum? What kind of band might we want? What region of the electromagnetic spectrum would we want to measure? Yep. Yeah. What portion of the infrared specifically? We got the near infrared and we got the. Starts with the T. No? Not sure? Yeah. Thermal infrared. What we use to measure temperature. We also talked about using passive microwave sensors to measure sea surface temperature as well. So that would also work. But more common one would be thermal infrared. And then what about temporal dimension? How long do we need to collect data for? Yep. Many, many years. Many, many years. Probably several decades. Maybe a century plus. Something like that. Awesome. Okay. What is the difference between land cover and land use? And maybe use a specific example to help explain the difference. Yep. Land use is related to human activity. For an example, a forest that is up down for climate change and land cover. But if it is a managed forest, it remains the largest particular thing. Exactly. Yeah. Exactly. Yeah. So land cover just to repeat in case anyone didn't hear that. Land cover is the physical material on the surface of the earth. Land use is how humans use that particular area, that particular parcel of land. And then a specific example, maybe using forest harvest versus deforestation with forest harvest. Trees are cut down. The land cover will change from forest to bare soil. But maybe eventually back to forest. The land use does not change. If it is a managed forest, if it is there for forestry, it is still there for forestry. On the other hand, with deforestation, if we cut down trees to create a parking lot, the land cover will change, potentially permanently. So go from forest to concrete and then stay as concrete. And the land use may also change. So the land use maybe went from natural ecosystem or forestry to residential or urban or shopping or something like that. Awesome. Great answer. Classify the following changes as cyclical, abrupt or gradual. A single wildfire. Cyclical, abrupt or gradual. Someone just shout it out. What's that? What about the seasonal change and frequency of wildfire occurrence? Yeah. Yeah. Good answer. Yeah. Cyclical and gradual. Not really enough context there to necessarily make it out. We have cyclical changes of fires occurring because in winter they occur less. In summer they occur more. If we look at that, though, across maybe decades, we might see an increase in the environment. So seasonally cyclical change but maybe annually more of a gradual change potentially. Okay. What about an insect outbreak in a forest followed by salvage logging and then vegetation regrowth? Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Exactly. Yeah. So the insect outbreak gradual change, salvage logging, abrupt change, regrowth, gradual change. Okay. What about sea surface temperature? Cyclical, abrupt or gradual change? Yeah. Yeah. That's right. Gradual. Gradual if we're considering how it's changed over decades, maybe centuries due to climate change for sure. Could it be argued something else as well? I'm going to say like a completely dependent on the temporal range. Totally. If you're measuring it at just one place one time, then it could be a growth, for example. Absolutely. If you're measuring it like the seasonal change over a few years, it could be cyclical. But if you're measuring like at the same time over a year over multiple years, that could be considered right. Exactly. So really, really good point there. Really good thing to consider for final exam question. That one totally depends on the scale of time that you're looking at. If you're going to see a crisis, then maybe it's more cyclical. Are you considering how it's changing seasonally? And again, maybe more cyclical. Are you considering how it's changing over decades or centuries due to climate change? Maybe more gradual. Are you considering how it's changing because there's an oil spill that has a fire occurring right on that portion of the sea that wasn't happening five minutes ago? Then maybe it's a big abrupt change. So totally depends on the circumstances and context. So these are a bit harder than realistically you would see on the final exam, but I just want to get you guys thinking about those. Okay. Awesome. Any questions? Cool. Okay. We will get started on this next lecture here then. I will. I will. Okay, sweet. So, we are going to talk about some new, some future satellite missions. We'll talk about satellite constellations, the concept of the democratization of space and open skies, and then also briefly kind of end on observing other planets using satellites. And then we'll touch on drones a little bit as well. So, Landsat, again, if there's one thing that hopefully you remember from this course, it's Landsat, what Landsat is, what we're actually able to measure and monitor with Landsat. We know the first Landsat launched in 1972 was Landsat 1. We know that Landsat 4 never reached, or sorry, Landsat 6 never reached orbit. And then we know that Landsat 9 is the most recently launched Landsat. So Landsat 9 was just launched in September of 2021. It has the OLI 2 and Tiers 2 sensors onboard it, which are essentially just copies of the OLI and Tiers sensor from Landsat 8, just with a slightly improved radiometric resolution. Not something that we talked about in this class, so don't worry about what radiometric resolution means. Essentially, just an improved sensitivity on the Landsat 9 versus Landsat 8 sensors. Quick video here about Landsat 9. Landsat 9 will provide critical data on how Earth is changing, circling the globe every 9 minutes, 14 moments a day continuing decades of observations. The impact of the Landsat record is the sheer amount of information you've collected all across the world since 1972. And it is high quality science-selling data enabling us to actually track changes over time. Now, 50 years of Las Vegas expanding may be very simple to notice, but we can also observe short-term changes, like the roads of farm crops through a season in South Central Kansas. With more than one Landsat satellite in orbit, plus the European Sentinel-2 satellites, we will get data several times each week improving our ability to track crop health and more. The temperature measurements from Landsat 9 will be used to calculate how much water was used by each farm's unit. The central Platte Natural Resources District, like many throughout the western United States, relies on Landsat data to manage your engagement in increased water efficiency. Landsat 9 will also improve monitoring with coastal waters. The increased precision and data sent back from Landsat 9 will allow finer distinctions in the levels of light reflected from water, making it easier to identify any pollutants that are present. Around the globe, growing population and expanding development result in higher amounts of runoff, damaging sensitive near-shore ecosystems. Landsat's long history lets us look into the past to see the effects of land use changes. The consequences of climate change can also be seen in Landsat's long data record. Scientists have used Landsat to track shrinking glaciers for decades, and Landsat 9 will continue that effort. The glaciers in the Himalayas are a tea water source for billions of people in South Asia. Due to global warming, the increased milk bar collects in large lakes at high altitudes and poses a flood on risk to downstream villages. Landsat data is essential to monitor the growth of these lakes. Because of their location, glaciers are not easy to study in person, but Landsat's view from space allows us to study glaciers all around the globe. Landsat 9's improvements will make it easier to see features on the glacier surface. With that, we can better track how fast the glacier is moving, knowing the velocity of the ice now and how it has changed over the past decades. Helps us forecast likely contributions to rising sea levels in a changing climate. Landsat 9 joins Landsat 8 to continue the unbroken stream of Landsat data. For five decades, we rely on Landsat's high-caliber science-quality observations to understand and protect our old land. And while Landsat 9 begins sending back data, we are already planning for the next evolution in the Landsat program. Okay, so the literal next Landsat mission that's to be launched is called Landsat Next. So it is planned to be launched in 2030, and it's actually going to be a constellation of three satellites. So up until now, all the Landsat satellites are just one satellite operating by itself. Landsat 1 is one satellite, Landsat 2 is one satellite, so on and so forth. Landsat Next, which will be after Landsat 9, is going to be three identical satellites. And so that's going to essentially artificially improve the temporal resolution of the Landsat satellite. It'll be three satellites staggered in the exact same orbit, so it'll take less orbits and thus less time for the next Landsat satellite from the Landsat Next constellation of satellites to go view the exact same point on the Earth. So it's going to have this increased level of temporal resolution because it's a constellation. We'll talk about constellations in a bit more detail in a moment here, but a constellation of satellites is essentially a group of identical satellites all operating together. With all of these kind of future Landsat missions that we're seeing planned and future satellite missions as a whole, we're seeing this continued increase of spatial resolution, continued increase of spectral resolution, many more bands. You can see in comparison here Landsat 8 and Landsat 9 bands versus Landsat Next bands. Landsat Next is going to have 26 bands, so a much, much higher spectral resolution than Landsat 8 and Landsat 9 is two predecessors. And then also we're seeing just increased sensitivity, which really just means increased radiometric resolution. The plan for Landsat Next to have three satellites to increase its temporal resolution is a part of a more common thing we're seeing now with newer satellite missions being launched, where they are being launched in constellations, where multiple identical satellites are being launched, put into orbit, just staggered behind one another so that we can get quicker images of the same area on the surface of the Earth from the same sensor, just mounted on board two different spacecraft or two different satellites. Okay, quick video about Landsat Next here. The NASA Jet Propulsion Laboratory, NASA Jet Propulsion Laboratory, California Institute of Technology and Technology, California Institute of Technology absolutely love the hype music. So one thing to note there, I don't know if you guys picked that up, but on a couple of the bands that Landsat Next will have, they're going to have an increased spatial resolution. So generally speaking, when we've talked about Landsat, we've talked about Landsat having a 30 meter spatial resolution. And for example, if I ask you what the spatial resolution of Landsat is, should say 30 meters, Landsat Next isn't launched into space yet. But a little bit of a hint, maybe, kind of sort of, for the final exam. Something for you guys to kind of consider, maybe think about. Landsat has, you know, this record of data collected since the 70s, 80s with a 30 meter spatial resolution. All of the newer Landsat satellites that have been launched, including Landsat 7, Landsat 8, Landsat 9, also all have this 30 meter spatial resolution. Do you think that there might be any issues with having a spatial resolution on newer, to be launched satellites from Landsat that are finer, that have a smaller pixel size? There is a trade-off, potentially, between increasing the quality of the data and having data continuity. One of the most valuable parts about Landsat is that it has 30 meter spatial resolution imagery all the way back to the 70s and 80s. And it's always been that 30 meter spatial resolution. So no matter what satellite you download Landsat data from, it's always the exact same resolution. That makes integration of data between the different Landsat satellites really, really easy. Because the pixels are the exact same size covering the exact same area. Now, if we have 10 meter spatial resolution, that sounds really, really awesome, and probably will be really awesome. I'm super excited about the ability to have 10 meter spatial resolution covering the entire Earth every six days in the case of Landsat Next. But that also poses issues for data continuity. It's going to make it a lot harder to integrate 10 meter spatial resolution data from that new Landsat Next satellite with historical archives of Landsat data from Landsat 1, 2, 3, 4, 5, 7, 8, and 9. Right? You might see something like that on the exam. Maybe. Maybe not. See? But just something to think about. That's kind of more of a, I don't even actually have any slides about it. It's a bit more of kind of an open-ended food for thought kind of question that there won't necessarily be a right answer to if it is on the exam. Who knows? Might not be. We'll see. But yeah, so bit of an open-ended question, just food for thought. Something for you guys to think about just in general. Okay. They also mentioned on board or in that video there the European Space Agency's Sentinel series of satellites. And it's relatively a newer series of satellites. The European Space Agency undertook the Sentinel series as a whole in order to replace a lot of their older European Space Agency satellites. So there's six satellites in the Sentinel series mission. We don't talk about it as much in this course because we kind of focus on North American centric satellites. Like Landsat and Modus. But Sentinel has, they're all just named Sentinel one, two, three, four, five and six. So Sentinel one is a weather satellite. It's a radar satellite. Sentinel two is kind of a close to satellite to the Landsat series of satellites. It's a multi-spectral sensor. Sentinel three is for ocean and land monitoring. Sentinel four is for atmospheric monitoring. It actually has not been launched yet. Sentinel five is again for atmospheric monitoring. It has been launched. And Sentinel six is for global surface height for climate studies. It again has been launched. It's also a radar satellite. So Landsat, our Sentinel five here in this case is the only one. Sorry. Sentinel four is the only one that has not been launched yet. Now we've already talked about ISAT, which we know is a space based LiDAR sensor. The global ecosystem dynamics investigation LiDAR, or short term Jedi, is on the International Space Station. It is also a LiDAR sensor. It's one of the few other space based LiDAR sensors that we have. And its main goal is for getting biomass estimations across forests because we know that LiDAR data enables us to get that structural 3D information about forest, about vegetation, about biomass. So it's kind of a newer sensor that's onboard the International Space Station. I mentioned a little bit earlier about what constellations of satellites are. So constellations are groups of identical satellites working together. And the main reason to have a constellation of satellites is that it improves the temporal resolution of those particular satellites. So Earth observation satellites and constellations are generally just offset in the exact same orbit. So that when one satellite comes over, it passes over, the next one's going to come back, and the next one's going to come back, and they'll kind of continue rotating in a staggered sense in orbit around the Earth. And for the future, we're probably going to see a lot more continuous development of global navigation satellite systems and of Earth observation constellations. So we talked about, again, this was pre-mid term, but we talked a while ago about global navigation satellite systems, like GPS, like Baidu. Those are inherently constellations of satellites. They're groups of identical satellites all working together. So GPS is one constellation of satellites, for example. The Landsat next satellite mission will be a constellation of three satellites. So any constellations, just a group of identical satellites that are all working together. One particularly interesting development with constellations of satellites is the advent of very, very small Earth observing satellites launched in very large constellations, potentially hundreds and hundreds of small identical satellites launched together. These are called CubeSats, and we're going to talk a bit more about those in a moment later in the lecture. I first want to touch on what the concept of open skies is. So open skies, well, I'll first just provide a little bit of historical context. So historically, there's always been very high costs of operating large Earth observing satellites, and that always made it really difficult for a single nation or company to collect all the relevant data. We've shown or I've shown a couple times throughout this course, the creation, kind of the engineering that's gone into the creation of Landsat satellites, MODIS satellites. Sometimes they're as big as an entire tractor trailer. They can be absolutely massive, right? They're really, really expensive, takes a lot of money to launch those very, very technologically advanced, very, very large satellites into space. Now, because of that, we have generally had open skies, which just means that because not everyone in the world had the capability to launch satellites into space, we had, say, pretty well-off nations like America, like the United States, like the European Space Agency as a whole, launching satellites. So, for example, NASA from the States, from America launched the Landsat series of satellites, but the Landsat series of satellites are completely public, open source, and free. So anybody can download Landsat data. So the fact that it was really hard for nations to go away and launch their own satellites generally created this sense of open skies where any imagery with these kind of moderate to course spatial resolution, could be taken legally across all areas of the Earth, and then because that imagery was open source, freely available for anybody to access, it allowed anybody, any country, myself, people in New Zealand, Australia, South America, Asia, wherever you are, anybody can go and download Landsat imagery for any portion of the surface of the Earth. Now, because we have started to see higher and higher spatial resolutions from satellites, this has led to a little bit of a couple of issues, a little bit of controversy, right? So we know that, say, with worldview satellites now, we can get spatial resolutions all the way down to 30 centimeters, and some areas may be kind of sensitive for satellite imagery. In other words, some nations, some people, some groups, whoever might be, might not want high spatial resolution imagery of particular areas available to the public. That was never really too much of an issue with, say, MODIS Landsat data, because we only had a moderate to course spatial resolution. Now, these satellites that are very or pretty high spatial resolutions are generally commercial, which means that the imagery is not free, it's not open source, it's not publicly available. And so on websites where satellite imagery and aerial images can be freely viewed, many areas are typically blurred out. So oftentimes, Google will have large licenses, pay a lot of money to some of these commercial satellite companies that have high spatial resolution data so that they can get really aesthetic and pretty looking base maps. But oftentimes, they'll just blur out areas that are kind of sensitive for the public to know, right? So this imagery here, this probably isn't a 30 meter Landsat image, this is a much higher spatial resolution image. In this case, this is the NATO headquarters in the Netherlands. They didn't want this to be publicly available, so they have it all blurred out. Now, it no longer takes necessarily an entire nation to build a satellite. Lots of private enterprises can enter into Earth observation now for much, much lower costs. And that's due to technological advancements. And for example, we have these miniature satellites, these CubeSats. So Planet Labs is a pretty well-known company that has gone away and launched these many, many satellites, hundreds and hundreds of them, to kind of start what's called the democratization of space. Whereby, nowadays, it doesn't take a really rich nation to launch a satellite into space. If you really wanted to, if you were a pretty smart engineer, you could probably go to home hardware and find the materials needed to create a little CubeSat that was maybe 10 centimeters by 10 centimeters by 10 centimeters, put a camera on it, launch it into space, and all of a sudden you have an Earth-observing satellite. So, just to kind of clarify, the concept of democratization of space is that concept of anybody really being able now to enter into Earth observation. It's a lot more accessible. You know, that's not necessarily true, not literally everybody, but it's much more accessible than it historically used to be. While Open Skies is more related to the concept of legally allowing missions like Landsat to take imagery all across the world, and then having that imagery be open source, being able to be downloaded by anybody for any purpose. Okay, I'm going to save talking about CubeSats until next week. So on Monday, we'll finish off talking about CubeSats, finish off talking about this future lecture. We'll talk about the last blog post that is due. I will wrap up now. Do you have anything you want to announce? Just kind of Q&A. Okay, sweet. If you want to come down and chat to Sam about the assignment that's due Thursday, you're welcome to. Otherwise, I will see you next week. Don't forget to post on the discussion board if you want me to go and review anything in class a week today. So on Tuesday when we have our final exam review session. Awesome. Thanks, guys. See you next week. Thanks. Bye. Thank you. All right. Hi, everyone. We are nearly done for the semester. So second last class today, we are last class tomorrow for our final review session. I'll talk today first. I'll finish off the lecture about kind of the future of Earth observation. And then I will dive into briefly the Canada from space lecture, which is pretty brief. It's mostly just me introducing what you're going to do for blog post five. So that will have just been posted, but I'll go over the instructions in my slides as well. And then I got a couple kind of final reminders to go over at the end. Julia is not going to be here today for assignment seven, but she will be here tomorrow if you need any help outside office hours on assignment seven. So I'm going to continue where I finished off last week. So just to jog your memory bit, the last kind of thing we talked about were the two concepts of the democratization of space and of open skies. So open skies was kind of this idea that we have Landsat and other satellite missions that can legally go all around the world and collect moderate to course spatial resolution imagery. And anyone can download it. And it's free and it's open source. They can use it for whatever they want. And that was mostly due to historically it being hard to access or create your own Earth observation program or mission. Satellites have historically always been really expensive, not easy to launch. It's not easy to get something into space. But that ultimately has led us to where we are now and kind of the creation of this democratization of space. And the democratization of space is the trend that we're seeing now where the technologies and the materials used to create satellites is getting really, really inexpensive. So anyone can now really create a satellite, enter into the Earth observation kind of field. And this has led to this democratization of space. Should I give a moment? I know it was restless. Second last day of class. I hear you. We're almost done. We can do it. Okay. So the next thing that we kind of, that this kind of leads into, this idea of democratization of space, of materials becoming really inexpensive, of anyone really being able to get access into the Earth observation field is the creation of something called CubeSats. So CubeSats are pretty new. They've been developed over the past five to ten years or so. They are miniature satellites that are usually about ten centimeters by ten centimeters by ten centimeters. So they're, you know, literally the shape of about a cube, something like this size. They generally weigh less than about a kilogram. And there's been hundreds of CubeSats already launched from governments, by NASA, by private companies, because anyone can go away and build these. You can go and just get a standard optical little camera from Best Buy or something like that. You can mount it onto a little CubeSat that has some solar panels. And you can figure out a way to just attach it to some other, you know, large space program where they're launching a bunch of other CubeSats. And it's not that hard to actually get it up into space and into orbit. So these CubeSats are very, very inexpensive. They generally use off the shelf technologies, which again just means it's very accessible to find the materials to build these CubeSats. They can be built very, very quickly. And they have a really simple design, which ultimately means that you can efficiently build up a very, very large constellation of satellites. Now just to remind you guys, a constellation of satellites is a series of multiple satellites that are all identical, all working together. Typically they're all in the same orbit and they're just kind of staggered a little bit in that same orbit. And that allows that satellite constellation to have a very high temporal resolution, because there's, you know, instead of one satellite orbiting, there's maybe 10 or 20 or 30 or 40 or 50 all in the same orbit. So it takes less time for us to see an image because a different satellite will just come by and look over that same area of interest very, very quickly. Now CubeSats often are also flown at pretty low altitudes, which means that potentially they can also have very high or at least moderate spatial resolution with again potentially global daily coverage. So the kind of advent of CubeSats, the kind of technical specifications of why they're kind of considered this new frontier of Earth observation, is because we can potentially with CubeSats get daily imagery, so an image of the entire globe, every single day, sometimes at a spatial resolution, less than a meter. So 50 centimeters, 20 centimeters, 30 centimeters. Now again, none of these CubeSat companies have publicly available data. You have to, you know, agree to all of their licenses and you have to pay quite a lot of money to get their data. But potentially, this could be quite revolutionary. When we think about modus data, we're thinking about, yes, that daily global imagery, but that's not a spatial resolution of 250 to 1000 meters. So with CubeSats here, we're talking about the potential for 10 to 50 centimeters spatial resolution data, fine enough to potentially make out individuals, people on the surface of the ground, at a global daily scale. So a pretty kind of incredible breadth of potential data that we can see. Now, can anyone kind of off the top of their head think about or give me an idea what, you know, an issue might be with CubeSats? Like the idea of daily, very, very fine spatial resolution data covering the entire globe sounds amazing. But why do you think maybe a CubeSat constellation could be an issue? Yeah. Absolutely. Yeah, probably contribute to space junk rather than launching one satellite. You might be launching, launching hundreds of satellites. That means they're probably not going to be built to the same rigor or have the same lifetime as those large satellites that take years and years and years and years to develop and launch. So potentially contributing to more space junk. Yeah. Probably like privacy issues with private individuals owning their own satellite. Yeah, privacy issues, issues with potentially these CubeSats taking high resolution imagery of sensitive areas. Yeah. It can't be used kind of to look back, you mean, in time? Yeah. Is that what you mean? Yeah. So you don't have that kind of data standardization back through decades and decades like you do with Landsat data. We have the same Landsat data all the way back to the 70s. So potentially with these newer CubeSats, we don't get that temporal dimension. Yeah. There might be what? Sorry? A lack of regulation. Yeah. Absolutely. That could definitely be a problem. One other thing. Well, I'll maybe mention it after we're going to watch as we do a lot in this course, we're going to watch another hype video. It's going to be awesome. This one is a TED talk from a CubeSat company. That's kind of one of the, you know, one of the companies that's kind of starting to break the boundaries of entry into Earth observation is trying to industrialize CubeSats. So let's watch this video. And then right after they hype us up, I'm going to kind of bring us back to Earth a little bit. The Earth needs no introduction. It needs no introduction in part because it was 17 astronauts when they were herding around the moon in 1972. So this I quite image. It galvanized the whole generation of human beings to realize everyone's spaceship Earth. So I draw on the final evidence it is, and we need to take care of it. But while this picture is beautiful, it's static. And the Earth is constantly changing. It's changing on days time to go with human activity. And some of the images we have of it today is old, typically years old. And that's important because you can't fix what you can't see. But we ideally want these images of the whole planet every day. So what's standing in our way? What's the problem? This is the problem. Satellite are big, expensive, and they're slow. This one weighs three tons. It's six meters tall, four meters wide. It took up the entire fairing of a rocket just to launch it. One satellite on rocket. It cost $855 dollars. Satellite studies have done an amazing job at helping us to understand our planet. But if we want to understand it much more regularly, we need lots of satellites. And this model isn't scalable. So me and my friends, we started planet labs to make satellites ultra compact and small and highly capable. I'm going to show you what our satellite looks like. This is our satellite. This is not the scale model. This is the real size. It's 10 by 10 by 30 cent meters. It weighs four kilograms. And we've stuffed the latest and greatest electronics and sensor systems into this little package. So that even though this is really small, this can take pictures 10 times the resolution of the big satellite here, even though it weighs 1,000th of the mass. And we call this satellite dove. I'm thinking that we call this a satellite dove. And we call it dove because a satellite is specifically named after birds, but normally birds are prey like eagle, hawks, fruit, kill, or catch the light, these sort of things. But that's how a humanitarian mission. So we wanted to call it dove. And we haven't just built one, we've lost one. And not just one, but many. It all started in our garage. Yes, we got our first satellite prototype in our garage. Now this is pretty normal for a Silicon Valley company that we are, but we believe it's the first time for a space company. And that's not the only trick we learned from Silicon Valley. We rapidly prototype our satellite. We use early release often on our software. And we take a different risk approach. We take about an outside and test them. We even put satellites in space just to test the satellites. And we've learned to manufacture our satellites at scale. We've used modern production techniques so we can build large numbers of them. I think for the first time, we call it agile aerospace. And that's what the layer is to put so much capabilities into this little box. Now, what has bonded our team over the years is the idea of democratizing access to satellite information. In fact, the founders of our company, Chris Robbie and I, we met over 15 years ago at the United Nations when they were hosting a conference about exactly that question. How do you sell that to help humanity? How do you sell that to help people in developing countries or with climate change? And this is what has bonded to us. Our entire team is passionate about using satellites to help humanity. You could say we're space eat, but not only do we care about what's up there, we care about what's down here too. I'm going to show you a video from just four weeks ago of two of our satellites being lost from the International Space Station. This is not an animation. This is a video taken by the astronaut looking out of the window. It gives you a bit of a sense of scale to satellite. It's like some of the smallest satellites ever being lost, the biggest satellite ever. And by the end, the solar array glids in the sun is really cool. Wait for it. Boom. It's like money shot. So we did just a little tour of them like this. We launched 28 of them. It's the largest constellation of earth imaging satellites in human history. And it's going to provide a completely radical new data set about our changing plan. But that's just the beginning. You see, we're going to launch more than a hundred of these satellites like these over the course of the next year. It's going to be the largest constellation of satellites in human history. And this is what it's going to do. Acting in a single orbit plane that stays fixed with respect to the sun, the Earth will rotate underneath. They're all cameras pointing down and they sort of scan across as the Earth will rotate underneath. The Earth will take every 24 hours. So we scan every planet every 24 hours. It's a light scanner to the planet. We don't take a picture of any rather planet every day. We take a picture of every single place on the planet every day. Even though we launched these just a couple of weeks ago, we've already got some initial imagery from the satellite. And I'm going to show it publicly to the first time right now. This is the very first picture taken by a satellite. It happened to be over the UC Davis campus in California where we can't run. But what's even cooler is when we compare it to the previous latest image of that area, which was taken many months ago. And the image on the left is from our satellite and we see buildings that have been built. The general point is that we will be able to track urban roads as it happens around the whole world in all cities every day. Water as well. Thank you. We'll be able to see the extent of all water bodies around the whole world every day. And how water security. From water security to food security, we'll see crops as they grow in all the fields and every farmer's field around them and every day. And help them to improve crop yield. This is a beautiful image that was taken just a few hours ago when the satellite was flying over Argentina. The general point is there are probably hundreds of thousands of applications this day. I mentioned a few, but there's others at deforestation, the ice caps melting. We can track all these things every tree on the planet every day. If you took the different seed today's image and yesterday's image, you'd see much more news. You'd see floods and fires and earthquakes. And we have decided therefore that the best thing that we can do with our data is to ensure universal access to it. We want to ensure everyone can see it. Thank you. We want to empower NGOs and scientists and journalists to be able to ask the questions that they have about the planet. We want to enable the developer community to run their apps on our data. In short, we want to democratize access to information about our planet. Which brings me back to this. You see, this will be an entirely new global data set. And we believe that together we can help to take care of our spaceship earth. And what I would like to leave you with is the following question. If you had access to imagery of the whole planet every single day, what would you do, that data? What problems would you solve? What exploration would you do? Well, I invite you to come explore with us. Thank you very much. All right. Massive hype video. So that TED talk I think was from 2008. So their technology has come, or maybe it's more recent. Maybe like 2012 or so. So it's not brand brand new. The idea of CubeSats isn't brand brand new, but it's starting to be used more and more in research. Couple notes, just to my personal notes, I guess. But he mentions, he asked, he leaves you with the question at the end of that TED talk. What would you do if you had imagery of the whole planet every single day as if we haven't had that for years before he gave that talk? But we've had modus for a while. Modus gives us imagery of the whole earth every single day in a really, really high quality data set. So we have had imagery of the whole earth every single day for a little bit already, at least since the 90s, early 2000s. He also mentions that they want to give data, that they want to make it universally available to everyone. He or that company called Planet Labs, they do make a lot of partnerships, collaborations with NGOs, with government, with academic researchers, whatever it might be. But generally speaking, that data is not available to the public. You do have to go and pay for it. It's really expensive. So when he says we're going to make it available for everyone, they make it available for NGOs, for organizations that help also bolster their image a little bit. So not to say that he's lying, he's not lying. But if you just sitting here wanted to go access this imagery, you couldn't, per se, it'd be really, really hard. Which is vastly different from Landsat, modus imagery, which you guys have gone and downloaded yourself. And then the last note about kind of CubeSats and the advent of CubeSats. And the one thing that you guys all made really good points about what some of the issues with CubeSats might be, the one thing that's kind of extra that I'll add to that, is when you have hundreds of different satellites taking imagery versus Sage just one, the variation in data collected is typically massive. All that means is the imagery that's collected by these CubeSats, because there's so many of them. And the technology, you know, they try to standardize it best they can so that every single image that they take represents a similar type of data set. But inherently, no matter what, if you're taking data from hundreds of different sensors versus just one sensor, then you're going to have a whole variety of different errors, standardization issues. I say that just to say that I've briefly a little bit worked with some CubeSat based data. And it's often riddled with noise and errors. It's just not nearly as clean of a data set that we might get from say to Landsat or Modus or some of these kind of larger satellite programs that have been, have taken, you know, decades and decades to be developed. So those are kind of my couple notes about that. Any questions about CubeSats? Anything like that? Okay. Sweet. Yeah. Is no recreation around the world that can be slunging this kind of stuff in the space? I mean, there will be many countries, many people, many things, and we can be tons of stuff. Absolutely. No, not at the moment. Yeah. Yeah. Yeah. I mean, you know, there's still our barriers to launch them. We say, or I say, you know, when we think about the democratization of space, we're thinking about accessibility for anyone to launch them. In reality, you do still need to, you know, have the materials, know how to put them together, be able to have the right connections, to be able to get that satellite onto a space shuttle mission, where we can get launched to the space station, and then, and then ejected from there. So there still are some barriers, but, but yes, there are lots of private companies now entering Earth observation. And at the moment, there aren't really any regulations around space and how much we can put up there. And there's no, you know, there's no real regulations for companies dealing with their space junk or removing it. There aren't any commitments they have to make around that at the moment. It's starting to really, really come to the forefront now, I'd say, of policymakers just in the past year or two, because there's just been this crazy uptick in CubeSats, especially starting to be launched. There's been a lot of conversations and, you know, articles and stuff coming out about regulations related to space junk, related to having all these, you know, artificial materials in space. So it's starting to come to the forefront a lot more now, but it's a huge issue for sure. And we're kind of just, we kind of just start out the tip of the iceberg right now. Any other questions? Okay, sweet. So I just wanted to mention that, you know, we've talked about throughout this whole course, Earth observation from space, there are also lots of other satellite observation missions that don't observe space. So there's currently, or have been satellite missions to observe Mars, Saturn, Venus, Mercury, Jupiter. We also have other satellites that are observing moons and asteroids, and these satellites are mainly observing atmospheric composition and geological composition terrain sometimes as well. There's a LIDAR instrument that's orbited Mars for quite a period of time to get a sense of the topography of Mars. A really interesting example is Pluto, and Pluto is just kind of fun because it's one of the furthest planets in our, not a planet anymore dwarf planet, but was one of the furthest planets in our solar system. And Pluto was observed using the new horizon spacecraft in 2015, which you see an image from on the right here. And that was kind of a really big deal because it was the first kind of higher resolution image that we had ever seen of Pluto. And that was pretty recent. That was only 2015. Up until then, the best image we had of Pluto and its moon was this, which kind of didn't really look like much. So I just got a brief video about the the new horizon satellite here. Pluto was the last of the nine traditional planets to be explored. This was due to the distance from us, but also can we believe this? It wasn't considered a very interesting celestial object. Thankfully, the new horizon team pushed hard for this mission to be approved. And in 2006, new horizons launched as part of NASA's new primtiers program for medium-budget space missions. The goal of this mission was to get to Pluto as soon as possible. And as such, new horizons was the fastest launch ever. It being a light spacecraft on the most powerful rocket of all time we had as fire. It was passed the moon in only nine hours. The Apollo missions took 10 times long on its way to Pluto and used Jupiter to gravitate the Earth, which shaped three years up their arrival time. It also used Jupiter as a trial-around of Earth systems, taking some remarkable videos of the images of the planet and its moons. After this successful trial, new horizons went into hibernation mode to prevent wearing tear over its instruments. Leading up to its approach in 2015, the team turned the systems back online and every day its spacecraft sent back images of the Pluto system. This was an incredibly exciting time for enthusiasts following the story. We began to get hints of what Pluto would possibly look like. And saw how different Pluto was from its biggest new, camera. Every day, the resolution got higher and higher, and more details could be made out. Yes, there were other scientific goals for the mission, but the most interesting thing to me was what it looked like. And soon, they could be seen what looked to be a heart shape on the dwarf planet. On the 14th of July, the new horizons of the probe made its closest approach, at only 12,500 kilometers from the surface of Pluto. However, mission controllers didn't get a lot of straight away. Firstly, the probe was too busy taking a lot of photos during the flight life, the zen-back-anting immediately. Once they had transferred immense, they had to deal with the slow-up speed of only 1 kilobits per second. Further to that, there was a 4.5-hour latency between the spacecraft and Earth. But what the latency and zen-back was spectacular, managing ranges, ice planes, glaciers and an atmosphere. So that was the first kind of high resolution image we got of Pluto. It was in 2015. Like that video said, it was the new horizon spacecraft was launched, even 06 is what it said. So it took almost 10 years for it to get all the way up to Pluto to take a high resolution image. Okay, we're going to kind of end off this course talking about drones. So drones are not, obviously, Earth observation from space, but they are still a really important kind of future path of remote sensing. There's already a ton of work and research and even industrialization of drones into forestry, resource conservation, management. So I just want to talk a little bit briefly about drones, some of their advantages, what's led to the kind of increase of drone use. So generally speaking, when we use drones for research purposes, for conservation, for environmental purposes, we program the drones to fly on a predetermined path at a predetermined altitude, and then they just take photos. However frequently we want them to take photos. So maybe we plan it so that the drone is going to fly through this lecture hall back and forth and back and forth all the way, and it's going to take a photo every two seconds or so. So you end up with a bunch of overlapping photos. I'll kind of show how we take that data and get some measurements and metrics from it. Generally speaking, drones can be used to determine plant health and cover, determine mineral locations, create 3D models of areas, potentially map wildlife migrations. They're often frequently used in emergency responses, for tracking storms, and a whole lot more. There's three key technologies that I'll talk about that have allowed the advancements of drones. So if I ask you on the exam, what are the three key technologies allowing the advancements of drones? That would be this right here. The first is the price. So the electronics that are used on drones have gotten much much cheaper over the past 10 to 15 years or so. The GPS has gotten much much better. We get accuracies from GPS measurements at a much much higher precision than we ever used to. So that means that when the drone is kind of flying back and forth over a particular area, it knows where it's taking a picture from in a very, very, very accurate measurement because we have such accurate GPS's these days. And then lastly, commonly a limiting factor with drones is the battery. The battery is generally the heaviest part of the drone. And typically, might only last 30 to 40 minutes. We get some drones that can maybe last an hour or so max. There were times where the batteries and drones might only last 10, 15, 20 minutes. So we've come a long way from then. Batteries have continued to get lighter, continued to become longer lasting so that we can fly larger and larger areas with drones. So this is kind of how the path of a drone might work. So generally, you have a predetermined path. So this is the path that this drone flew back and forth over this study area. And then each blue dot here is a photo it took just looking straight down. So each blue dot was a photo. And this would have been a pre-programmed flight path. So we would have already gone into our computer system and programmed out and mapped the exact path we wanted the drone to fly. Then we would have gone out into the field, popped open our drone case, started up the drone, clicked fly. The drone would go up, fly the path that was supposed to fly and then take photos. However often we tell it to take every five, 10, 30 seconds, maybe something like that. And then using that, it can kind of stitch together all of the individual images that it took to create kind of one whole composite of the whole area that it's looking at. Now generally with drones, we get very, very high resolution imagery, oftentimes sub centimeter. So potentially millimeters in pixel size. So we can make out individual leaves and shrubs and plants and grasses from imagery that's that high of a resolution. We can often get a custom spectral resolution with drones. We can just mount kind of a standard normal camera on it that just collects RGB visible information. But sometimes we can mount a near infrared camera on it as well. We can mount a thermal near infrared camera on it to be able to try and detect exactly where wildlife might be, where people are located. We can maybe have LiDAR on it as well to be able to try and piece together some 3D structural information that we can get from LiDAR. So we can kind of put whatever sensor we want on a drone. We can put just a normal camera. We could put a infrared camera. We could put a LiDAR sensor on it, whatever we want. And then we can fly a drone. However, frequently we want. We can fly it every day if we want, every week, every month, every year. Most times, at least, you know, for research purposes, for industry purposes, you're not going to see drones being flown every single day. It's possible, but it's not very common because you have to go away and charge the batteries and get out to the field wherever you're flying the drones. But there are studies that might fly drones every week, every month, build up imagery of that area and be able to track changes at, again, maybe a sub-centimeter spatial resolution occurring every week, every month, something like that. Now, that sounds amazing. As I often kind of pitch with a lot of the technologies in this course, there's a lot of really cool things about them, a lot of advantages of them. There are still disadvantages to drones. They're still heavily limited by their flight time. So generally, batteries might only last 30 to 45 minutes. If you're trying to fly an entire forest stand and you're only getting 30 to 45 minutes of flight time, it can be a real pain because, you know, it might take you hours and hours and hours to collect imagery for a forest stand or for a particular area of interest that you're studying. If you have to change the battery every 30 to 45 minutes, it can be quite tough. It can be quite a limiting factor. So it reduces really the ability of drones to get large coverage. You can't usually use drones to study an area that's maybe bigger than, I don't know, maybe 10, 20, 30, 40 meters squared. Even that is kind of getting large. Once you're getting into like 100 meters squared, 200 meters squared, that's going to be probably too large to be collecting drone imagery for. So the flight time reduces our ability to get large coverage with drones. The battery lives will continue to improve as technology continues to improve, but still a big limiting factor. Another key limiting factor is licensing and flight zones. So you often need a license to fly certain drones. Most drones, if they're under, believe it's 250 grams, you don't need a license to fly. So a lot of drone companies will make their drones 249 grams so that they're kind of the maximum size without needing a license to fly them. But anything bigger than that, which are often used in industry, in research, especially if you're putting a big heavy sensor on it, can only be flown in certain areas by certain pilots. So you have to have licenses to fly the drones. You often also have to have a license to be in the area where you're flying the drone. So for example, you generally can't fly drones in urban areas. I'll go to the park and often see drones being flown around by whoever. Technically speaking, pretty sure that's illegal. Obviously, I'm not going to go up to them and say, hey, that's illegal. But generally speaking, in kind of more urban areas, you can't legally fly drones. People will do it. But particularly anywhere near airports, anything like that, anywhere where there's any other aircraft, you can't fly drones. I'll give you just one brief example of why that might be an issue. There's researchers in the forestry faculty at UBC that used drones to monitor kind of coastal mud flats of the Fraser estuary. And that's an area that's very, very close to the Richmond airport. So they have a ton of issues oftentimes getting licenses to be able to fly their drones. They can also only often only fly them at certain times of the year, certain times of the day, where they'll have a very specific window. If they decide that there's good weather conditions and they want to fly their drone this week, a lot of the time they just can't because they can't get the licensing sorted out in time. So it's still a big disadvantage and kind of limitation of drones. Now this is what kind of data set from drone imagery might look like. So each of these blue dots for this particular forest stand represent a photo that was taken. So each individual dot here is a photo that was taken by the drone. And the drone would have flown back and forth and back and forth and back and forth all the way across the study area taking pictures again and again and again and again and again. That's going to wind up with potentially thousands of overlapping photos. So you can see the same features in a lot of these photos. You can see the road, you know, you can see kind of this area is moving over up there. But this is a set now derived from this of all of these overlapping photos. With those overlapping photos, we can plug them into computer algorithms and because the photos overlap in such a way where you get multiple angles of the exact same features, computer algorithms can go away and generate 3D models in a process called photogrammetry. So because, you know, say we get an image of this tree over here at the edge of the image from an angle where it's kind of leaning out a little bit, you know, you can see kind of the center of this image here. We're looking straight down at these trees, but kind of toward the edge of the image here, we're kind of looking at the sides of these trees a bit. So if we get overlapping photos again and again and again, we get images from a bunch of different angles of the same trees. So using that, we can plug them into algorithms, use photogrammetry and ultimately go away and build up 3D models of the forest stand. So this is the real imagery. This is what this forest stand looks like in reality. And then this is the photogrammetry based drone derived model of that same forest stand. Again, really, really high resolution 3D model here just from overlapping photos, not from using LIDAR or any kind of fancy instrument, just a typical standard camera. So pretty cool kind of progress and advent of drone imagery. Now, generally, drones, like I mentioned earlier, used for a wide variety of things right now, very commonly used these days to create 3D models of forest, potentially again using imagery or LIDAR. And this is really, really valuable for predicting timber volume, forest structure and fire regimes at a really fine scale. And oftentimes, flying these drones is a lot less expensive than say flying aerial based LIDAR. So the comparison there is, you know, flying a drone, whether it's for, whether it's with a LIDAR sensor on board or just a normal camera that's taking RGB photos where you're going to apply photogrammetry to build up a 3D model. Either way, if you're using a drone with a sensor on board, a drone is a lot cheaper than an airplane. Airplanes are a lot more expensive, a lot more inaccessible. So there's been kind of a large push towards drones recently because for a lot of companies, for a lot of researchers, there are much more cost-effective option than trying to get someone to fly a plane so you can collect LIDAR or something like that. Okay, any questions about that? Yeah? Is that the French or French or French or French or French or French or French or French? That's not what, sorry? Is that on drones or no? So both. So the aerial imagery reference is airplanes generally. But, you know, the word aerial technically means any image taken from air. So, you know, we have historically lots of aerial imagery from planes, potentially just with a normal camera or with LIDAR. And then the kind of future here that I'm talking about is using drones still as aerial imagery technically, but as a replacement for aeroplane-based imagery because it's a lot more cost-effective. Is that, answer questions? Okay. Any other questions? Yeah? So you can use LIDAR with drones? Yeah, absolutely. Yeah, you can put a LIDAR sensor on a drone, for sure, if you'd like. Okay. So you got some review questions here. I will maybe, do you guys want some time to practice these or do you want me to just answer them with you right now? And we can move on to the kind of last lecture. Nobody cares? Either way. Okay. Well, let's take a break then. Let's do like two, three minutes. I'll give you a moment to answer these questions. We'll come back. We'll answer them. And then we'll go on to the last lecture and finish off. Okay. Okay. Okay. Okay. Let's do these guys. So what technologies, just three of them, are allowing the advancements of drones. Yeah? Yeah. Yeah. Yeah, exactly. So cheaper electronics, highly accurate positioning measurements from better BPS and GIS. And then longer lasting and cheaper batteries. Awesome. What are cube Sats? Yeah. Yeah. Yeah. Just a really small satellite, usually 10 by 10 by 10 centimeters. Absolutely. What is a satellite constellation? Yeah. Yeah. Exactly. Identical satellites that are working together, all collecting the same imagery, typically all in the same orbit, just staggered in the exact same orbit. Okay. Lastly, what has led to the democratization of space? This is the last end of lecture question you guys ever have to do with me. This is it. You're still going to make me wait for it. Yes. Yes. Sorry. You said high cost of satellites? You said high cost of satellites? Right. So that's kind of more related to the open skies concept. So the open skies concept is kind of related to satellites being really expensive to create and launch. So we just had kind of one or two satellites legally collecting data across the whole world and being able to share their data with everyone. So the democratization of space has come from. Yeah. Yeah. Yeah. Yeah. Yeah. So just to kind of summarize key difference, good exam question, difference between open skies and democratization of space, open skies is really related to, you know, historically satellites being really expensive. So them not being very accessible, the materials and the ability to launch a satellite, not being very accessible. So we have programs like Landsat, like MODIS, that were legally allowed to collect data all around the world, and they shared their data freely for anyone for any purpose. Democratization of space is what has led to the democratization of space is kind of the opposite in the sense that what's led to the democratization of space is materials becoming much, much, much cheaper. So anyone can really go and find the materials now to build a satellite, and so this accessibility to be able to build and launch a satellite has led to this democratization of space where now it doesn't require an entire nation. It doesn't take, you know, a large nation like America or China or Russia to launch a satellite. Anyone can really go and do it now if they really want to. Okay, awesome. I'm going to, I will stop. One here. OK. So we're just going to briefly introduce Canada's Eco Zones, which is often how we actually classify the different regions of Canada that we can look at from a satellite's perspective. And then I'm going to kind of introduce you to the instructions for blog post 5. I hope it's useful. In the past, we've done this blog post a little bit differently. But often I've found that because some of you are art students are kind of from all faculties across campus, you might have not yet been introduced to how to look up scientific literature and how to look up peer-reviewed journals. So I'm going to talk about that a little bit, because that's what you'll be doing for blog post 5. And hopefully it'll be something that is useful for you guys in the future. So introduction to Canada as a whole. So second largest country in the world after Russia has 60% of the world's lakes, 10% of the world's forests, almost 10 million kilometers squared in landmass. Canada is a large, large, large country, has a ton of resources. And its large size leads to a vast diversity of landscapes and climates. Often the way that we classify these different landscapes and climates is with something called Eco Zones. So Eco Zones are this method that we use to describe ecosystems all across Canada. We generally have 15 terrestrial Eco Zones in Canada, the largest one being the boreal shield. It's this big green one across the center here that has about 20% of Canada's landmass and about 10% of its fresh water. And what you can notice from looking at this map of the Eco Zones, kind of all distinguished with different colors here. And then looking at a composite from remotely sensed imagery from Landsat in this case of all of these Eco Zones, you can see just visually some of the spectral differences that reflect the different characteristics of the ecological areas we're looking at. We can see kind of the boreal plains here, lots of green and some brownish up here. We got lots of ice up here, lots of tundra, potentially permafrost. We got much greener areas kind of down here in the Atlantic maritime and the boreal shield. Lots of mountains in the Montaigne-Kadillira here and the Pacific maritime, lots of green forest out on the island here. So you can see a lot of these spectral differences that reflects a lot of the different ecological characteristics and makeups of these different Eco Zones. So oftentimes when we are trying to take, you know, a place like Canada say, classify it into some of these different Eco Zones that we've mapped out here, we might look across the whole area and we might try and use the spectral information derived from say Landsat or some other satellite, classify land cover, classify forest cover, derive a bunch of metrics and measurements about the landscape and then categorize them based on that into, you know, in this case, the maritime region versus Montaigne-Kadillira, et cetera, et cetera. So this is how we ultimately in a bigger picture go and try and map out different ecological regions across Canada. So for blog post five, what I want you to do is look up some research about how Earth observation remote sensing is being used in one of Canada's terrestrial Eco Zones. There's some marine Eco Zones as well. Please emit those, just look at the terrestrial Eco Zones. We want you to specifically find a peer reviewed journal article about remote sensing, about Earth observation and just briefly describe it, just with three sentences, four sentences max, to answer these couple of questions. What is the unique feature of the Eco Zone that the authors are monitoring? What's the specific process or phenomena that Earth observation is being used to measure? And what is the specific satellite or sensor or data set being used in that particular study? Now, first off, we want you to find peer reviewed literature. So what is peer reviewed literature? In scientific literature, which just means science that gets published in scientific journals, they always undergo a rigorous peer review process, which means that before an article is published, it has to be reviewed by other well-established scientists in that field. And that's essentially a way to perform quality control on the science. Generally, in science, when you create science, when you perform science, you write up a report, you write up an article about it, about your methods, about what you found. And then if you want it kind of recognized as peer reviewed science, if you want it published, then you have to submit it to a journal, at that journal it goes through this peer review process where they send it to scientists relevant in the field that you're studying, and they get back and say, yep, this is good science or no, this is poor science, these things need to be changed. So it's essentially how the scientific community performs quality control before articles are published. Now, finding a peer reviewed article is not too difficult. What I want you to try and be able to do here is combine some research on different eco-zones with your knowledge from class to be able to find articles that are relevant. So you can use an academic search engine like Google Scholar to do that. There's other ones out there, you can use the UBC library search engine if you want. But I just wanted to walk through an example of what that might look like. So on this map of Canada with all of our different eco-zones, we kind of live in the bottom southwest corner here, way down here, and the eco-zone that we live in is the Pacific Maritime. So we can go to the website that I've given you that this kind of map is from, and it looks like this. So the link is in the blog post and in these slides here. And you can read about that particular eco-zone, you can read about its landforms and climate, the wildlife, the plants in that particular eco-zone. And then from that, you can try and relate what we've talked about in class to what you found out about this eco-zone to try and find a peer-reviewed article or research that has occurred in that area. So for example, if I look up the Pacific Maritime Eco-zone, I'm reading about it on this website that we posted. I noticed that one of the common human activities there is forestry. I know from class forest harvesting is often monitored and detected with land sap. So I can use some keywords based off knowing that to be able to try and look up an article. Now, the reason that I'm going through this is because oftentimes students really struggle to find a relevant article. One big tip I'd give, don't use the eco-zone name itself as a keyword. You know, oftentimes students want to just say Pacific Maritime satellites. If you look up the eco-zone, it's a lot tougher to find hits, to find relevant papers. It's much easier to look up a geographic area or region within that eco-zone. So for example, I looked up the Pacific Maritime. I found that forest harvesting is really common in the Pacific Maritime. I know from class, Landsat is frequently used with measuring and monitoring forest harvest. And I know that the Pacific Maritime includes Vancouver Island. And I know Vancouver Island has a lot of potentially forest harvest going on. So I can go to Google Scholar here. Can look up Landsat Forest Harvest Vancouver Island. Landsat Forest Harvest Vancouver Island. You're going to get a couple of different hits, right? You're going to get a couple that you're going to have to search through to try and find something that makes sense to you. I like this one here, characterizing stand level forest canopy cover and height using Landsat Time Series, samples of airborne lidar and the random forest algorithm. A lot of jargon, a lot of technical information there. But I review the abstract that say, okay, they're just trying to map forest attributes essentially. So then I pick this article. Looks like something that's useful. So this is that same article that I just looked up just by looking up Landsat Forest Harvest Vancouver Island. Again, Vancouver Island, a lot better than specifying the eco-zone Pacific Maritime. You're not going to get nearly as many hits. It's much better just specify a geographic area. And then I look at this paper and the study area of this paper is located within the Pacific Maritime Eco-zone. I can confirm that. I look at the study area map. It's in Vancouver Island. And what they're looking at is they're monitoring forest inventories in areas with active forest management and they're combining Landsat and lidar data to derive better metrics of forest canopy and height across mature and young forest stands. That's the answer to my blog post five right there. They're looking at monitoring forest inventories. So the phenomenon that's particularly unique or common to this eco-zone is that there's lots of forest harvesting. What they're actually monitoring is changing characteristics of the forest, changing forest canopy cover, changing forest height across different kinds of forest stands across mature and young forest stands. And the specific data set that they're using is they're combining Landsat and lidar data. Make sure you include a screenshot of the first title page so it should look kind of something like this. And again, needs to be peer reviewed, needs to be from a scientific journal. So just a website or a government document isn't going to work. Needs to be peer reviewed scientific journal published in a scientific journal. Okay, note, you can't use the paper that I've given today as an example. So you need to look up your own paper. You can't use the Pacific maritime Landsat forest harvest as a topic to look up. You need to use your own topic, look up your own eco-zone. No plagiarism, so it must be written in your own words. Other than that, try to have fun with it. Try to find something interesting to research. Try to use what you've learned in this course, as well as understanding kind of some of the broad characteristics across different eco-zones to try and be able to find something that's interesting to you. Hopefully you have fun with it. This used to be an exam question, and we kind of moved away to that and have made it a bit of a blog post, so it's a little bit less high stakes because I kind of recognize that some of you might not have much experience in looking up literature, but I still think, you know, for taking a science course, it's a really important skill to have, so I'd encourage you to try and play around with keywords, play around with trying to find a piece of research that's appealing to you, that's interesting to you, and that you can kind of try and decipher and summarize a little bit. Okay, couple of due dates, just finishing off here, blog post five, so the blog post I've just talked about today is do this Thursday, so quick turnaround for that. Simon seven is due next Thursday, April 13th. Last blog post, blog post six is posted now. It's due next Thursday as well, April 13th. It's a pretty easy one. We're just getting you to kind of tell us about what you learned in the course, whether or not you enjoyed it or not. Office hours for Simon seven by Julia. We got some tomorrow, Thursday, and then next week, Tuesday and Wednesday as well. Julia's gonna come to class tomorrow. The end of our review session, if you have any questions for her, you're welcome to come and ask her then. Last note, final exam review session is tomorrow. This is the last class, there's no class next week. Tomorrow is the last class, don't come next week, I won't be here, no one will be here. Tomorrow is the last class. Please post what you want me to go over in the final exam discussion board. So if you post on there, I'll have slides, I'll have topics to go over. If you don't post anything, I'm not gonna know what to go over, I'm not gonna have any slides ready. It'll essentially just be a Q and A. So please, please post on that. Another mat, I will see you guys tomorrow for the last class. metres. Okay, guys, let's get started. It's another quick lecture today, so you'll be out here pretty fast, I'd say. I'm going to try to not use a mic, so I'm just going to try to project. If you can't hear me at the back, just put up your hand or thumbs up. Nice. Yeah, so today we're talking about observing your position on the earth. I was, Tristan was going to come to class today and introduce himself, and then he just messaged me about 30 minutes ago saying he's going to come Monday instead, so skip that. So in this lecture today we're going to talk about two main things. One, how to define your position on the surface of the earth, and two, what is the shape of the earth and how do we generally define the shape of the earth. So I wanted to start with this map, it's an old map, it's from 1482, and I want you to take a moment, I'll give you about five minutes or so, maybe a little bit less, well, not about five minutes, and I want you to try and see what you can recognize on this map in terms of waypoints on the map that maybe you can recognize. Then I want you to think about, given that this is a really, really old map from 1482, why might this map be historically significant? Why might I be forcing you to look at this thing? So take five minutes, chat with someone sitting next to you, if you don't know them, again, introduce yourself, but brainstorm that a little bit, we'll come back, we'll discuss it quickly and then we'll go on. So start with that. So where do you think the texture you can tell was Partners then? I took it. Yeah, I took it. Yeah, you're a people. You're a people. You're a people. Yeah, I did. Yeah, it's very good. Yeah, I did. Yeah, I did. Yeah, I did. I did. Yeah, I did. Okay, guys, let's come back. Generally speaking, I like to often give you guys a chance to talk to your neighbors about stuff, and then I try to come back and try to lead a couple minutes of discussion. I'll kind of ask you guys to help me with some things, something like this, for example, where I get you to brainstorm. If no one's really helping me out, I'll typically just say, please don't make me beg. So, you know, that's just a preface. I'm hoping that I don't have to say that. It's not like to shame you guys or anything like that. It's just I try to say that to provide a bit of comic relief and stuff like that. So, yeah. Okay, cool. So, what do you guys think? What can you guys see on this map? Can anyone tell me they can see you? Probably because this is pre-christian or globalist journey. And the southern heavens fear is mostly non-firital. Yeah, for sure. That's something like in terms of waypoints that you can see with continents and countries totally. Anyone else? Yeah? The parts of the map that are maybe the most accurate beer towards Europe and North Africa may be the closer part of the Middle East. Yeah. So, you could maybe assume that the map may pair over the group of people that collaborate for more central than that. Sure. For sure. Yeah. That totally makes sense. What about in terms of not, you know, not a geographic sense in terms of where things are and what you can recognize in terms of countries and continents and things like that? What else in terms of maybe more along the lines of map elements can you notice? Yeah? Is latitude and longitude seeming? Yeah, exactly. That is super impressive. Yeah. This shows the purpose of the round of the object of the cloud. Yeah, exactly. Yeah, exactly. So, anyone else? Yeah? They have the first point of the cloud. They have the first point of the cloud. Yeah. They're religion kind of constant. Yeah, totally. For sure. Yeah. So, the reason, those are all really good observations. The reason that I show this map at the start of class is because this is the first map. It's made by this guy named Tolumi. And he, or, yeah, he had the, he created this map. And it's the first map to have a coordinate system. So, you can see along the bottom here, all these numbers. And along the side here, all these numbers. And then these kind of grid lines, these gradicals. That represented latitude and longitude, someone said. And this was the first map that had a coordinate system that used latitude and longitude. Which is historically really important because nowadays, we still use latitude and longitude. It's still the fundamental way in which we locate where we are on the surface of the earth. Or where other people are, or where we are with our phone, or whatever it might be. So, like I said, you're on 1482 by Tolumi. It's not important. In terms of its detail, although I'm glad you guys were able to point out lots of cool things about it. But it was the first map to use a coordinate system. It was the first map with latitude and longitude. And for the rest of, well, for the first half of this lecture anyways, we're going to go into some detail about what latitude is, what longitude is, and how we use that to map where we are on the surface of the earth. So, the first is latitude. Latitude is an angle that describes the north-south position. So, you can imagine if you have, you know, a point kind of right in the center of the earth. You can imagine literally if you went to the center-center core of the earth and then drew a line out to the equator, it'd be a zero-degree line of latitude because you'd be going straight out. And then slowly as you go either north or south, the degrees are going to be a bit more. All the way to, if you go to the north pole straight from here, you can see that's going to be a 90-degree angle right there. So, that is what latitude measures. That's what it is. A parallel is any circle connecting all locations with a given latitude. So, a parallel is three-dimensional. It would travel all the way around the surface of the earth. These are examples of parallels here. The Arctic Circle is an example of a parallel. The Tropic of Cancer and Tropic of Capricorn are examples of parallels. The equator is also an example of a parallel. The equator is the largest parallel because it's right at the middle part of the earth. And its intersection is with the earth's surface, with a plane perpendicular to the axis of rotation, which just means that we have the north pole and south pole here, and that shows the axis that the earth actually rotates on, and then the equator cuts right through that axis of rotation. The poles are where the earth's axis of rotation meet its surface. So, again, if I were to kind of, you know, imaginary, put my finger on the north pole and the south pole, I could have my fingers there and the earth would just rotate in between my two fingers. The Arctic Circle is the southernmost latitude in the northern hemisphere where the sun can remain continuously above or below the horizon for 24 hours. The Antarctic Circle is the same thing, but in the southern hemisphere, the northernmost point that the sun can remain continuously above the horizon for 24 hours, and I'll explain how that works in a moment. Similarly, the Tropic of Cancer and Tropic of Capricorn are the, in the case of Tropic of Cancer, the northernmost circle of latitude, so you can see here in the northern hemisphere, where the sun can be directly overhead, and the Tropic of Capricorn is the southernmost circle of latitude where the sun can be directly overhead. In a exam setting, in a midterm setting, for example, hint in twink, wink, nudge, nudge, if I were to ask you what the Tropic of Cancer or Tropic of Capricorn is, you could use these definitions, but remember that fundamentally, the Tropic of Cancer and Tropic of Capricorn are also just a specific type of parallel, or a specific parallel, and their specific latitude and longitude for the Tropic of Cancer is 23.5 degrees north, and for the Tropic of Capricorn, 23.5 degrees south. Now, the way that works in terms of measuring where that is, it's not just that we've arbitrarily assigned these latitude values to say the Tropic of Cancer and Tropic of Capricorn, and to the Arctic Circle and Antarctic Circle, those latitudes are based off of these definitions of what each of these parallels are, and the way that works is because the Earth is tilted. So because the Earth is tilted, when the northern hemisphere is tilted towards the sun, so you can imagine the, I meant to bring my blow-up globe today, and I forgot, but I'm gonna make a fake sun here. Let's say this telephone, you know, is my sun. I got my Earth here. The Earth is tilted this way. This is the northern hemisphere when it's tilted that way. It's summer, and then it's gonna rotate here, all the way around to the other side of the sun. It's still gonna be tilted, still gonna be tilted in the same direction, so now the bottom of it is gonna be closer to the sun. That would be summer in the southern hemisphere. This would be summer in the northern hemisphere. And the way that works for the Tropic of Cancer, the Tropic of Capricorn, and the Antarctic and Arctic Circle is because of where this Tropic of Capricorn is and the tilt of the Earth, the sun will remain, or can potentially remain directly above the Tropic of Capricorn at some point during the day, at all days of the year. So any time, any latitude north of the Tropic of Cancer, at some point of the day, the sun will not be directly overhead. And by directly overhead, I just mean if you were standing in a given spot on the surface of the Earth and you look straight up perfectly perpendicular away from the surface of the Earth, then you would see the sun directly in your plain sight. And that only happens if you are either below the Tropic of Cancer, or above the Tropic of Capricorn. The Arctic and Arctic circles are the only places where either below the Antarctic Circle or above the Arctic Circle, where the sun can potentially remain above or below the horizon for all 24 hours of the day. And again, that's just because of the axial tilt of the Earth. Does that make sense? Any questions about that? Yay, nay? Cool. Okay. Now, elements of longitude are how we describe East-West. The Meridian is a half of a circle, essentially, that goes north to south and terminates at the north pole and terminates at the north pole and the south pole. So connects all points of equal longitude. The Primeridian is typically the origin of our measurements for longitude, historically and oftentimes, now a day still, the Primeridian is in Greenwich, England. And that's kind of just this arbitrary Primeridian. You can kind of see it. I think they pointed out here. Yeah, it goes through. That's the UK up there. And there's this Observatory in Greenwich, England, and this line right here is literally the Primeridian that they have there. So if you wanted to go and stand on the Primeridian, now you know where to go, I'm probably not going to visit there myself. But if you wanted to, you could. So longitude essentially measures, again, with an angle based on where the Primeridian is. So we'll say this is zero. It measures East and a measure, sorry, West in that case, measures West and East, how far away we are from that Primeridian. And a given meridian is just, again, if you took a great circle, which just is a circle that goes all the way around the Earth through the North Pole, through the South Pole, and cut it in half, that would be a meridian. Primeridian is where we start zero for longitude. Okay. This is just another animation that kind of breaks this down and shows what I mean in terms of measuring it from the center of the Earth. So we have our North Pole and our South Pole here. Our lines of latitude are called parallels. Again, there, you can see if we draw a straight line out, that's a zero degrees. Then we have a greater angle, greater angle, greater angle, all the way up to 90 degrees. That's our North Pole and our South Pole. And then similarly, it's going to show us longitude. And then same thing here. From the center of the Earth, you've got your Primeridian, and then to the left, you've got West. It's the right, you've got East. You can also imagine, when I was kind of talking about the tropics and the Arctic and Antarctic circles, you can see here, this is a good depiction of the tilt of the surface of, or the tilt of the Earth. So you can see that the North Pole and the South Pole isn't just perfectly at the top here and at the bottom. So if I had, you know, the sun right in the middle here, imaginary, then it'd be the summertime for the northern part of the hemisphere, because it'd be closer to the sun there. See, that makes sense. Any questions on latitude, longitude? You'll get a bit more familiar with it in the first assignment as well. Well, sound good? See, okay. So the next thing that we're going to talk about are models of the Earth. So all models that we have of the Earth are just that. They're models, which just means that they are not perfectly ever reflecting reality. Something has to be simplified in order to create a model. And generally speaking, the more you simplify it, the less representative of reality it's going to be. So we're going to talk about a couple examples here. On the left, we have the Earth. So this, you can imagine, is reality. And then as we go to the right here, we have more, more simple and simple and simple models of the shape of the Earth. And then on the farthest left here, we have reality. And thus, the most complex, most realistic model of the Earth. And then it kind of gets least complex and least reflective of reality as we move to the right here. So the first example there is the geoid. The geoid is a physical approximation of the figure of the Earth. It's the shape of the surface of the Earth based on calmed oceans and the absence of other influences such as winds and tides. All that means is just that it simplifies the surface of oceans based off of tides and currents that we know exist. It says, okay, if we pretend those things don't exist, what would the shape of the ocean look like? And that's all computed using these complex physical models based off of gravity readings on the surface of the Earth. So there's this satellite called the Grace Satellite that we're going to talk about later in the semester that essentially orbits Earth and measures what the gravitational pull is directly below it. It then relates that to how much mass there is directly below it and based off that is able to really accurately approximate what the shape of the Earth looks like. So generally speaking, it's just used to measure surface elevations with a really high degree of accuracy. When we're out in reality as remote sensing scientists, Earth observation scientists, we're often not using something as complex as a geoid to measure or map parts of the surface of the Earth. It's just so complex, the algorithms and the data required to build this model. It's just not realistic to be able to plug that in and use it for our maps and our modeling because it'll just take too long. It's just too complex. So what we generally do most of the time is use something called ellipsoid. An ellipsoid is a mathematical approximation of the shape of the Earth. So it's like a sphere, but it's flattened. So it's flattened at the poles and bulges at the equator, and that's based off of the revolution of the Earth. So it's suitable for direct mathematical computations, and all that means is that it's just a lot simpler than something like a geoid. We can apply it in a mathematical way much, much simpler. It's based off of these two values that represent the radius of a sphere, and in this case they're called the semi-major axes and semi-minor axes. The semi-major axes for an ellipsoid is always going to be larger than the semi-minor axes. So this A value right here is always going to be larger than this B value here. And that's essentially what separates it from the simplest model that we have of the Earth, which is the sphere. So in this case of the sphere, the semi-major and semi-minor axes, which is just the radius going out in this direction and going out perpendicular to that direction, are equal. So the semi-major and semi-minor axes are always equal, which just means that Earth's radius is always constant with the sphere. This is generally speaking the least accurate approximation of the shape of the Earth. So sometimes we use it if we want a really, really, really simple measurement to base our mapping off of. But generally speaking, when we're looking at the surface of the Earth, when we're mapping and trying to position where we are on the surface of the Earth, we generally use the ellipsoid. The geoids typically, too complicated. The sphere is too simple. It's quite perfectly, or at least doesn't quite represent reality as much as the ellipsoid does. So we generally use the ellipsoid. In general, though, just main takeaway, the Earth is not quite a perfect sphere. In reality, it is bulging at the equator, so the ellipsoid makes it slightly more accurate. And this whole kind of universe of research and mathematical computation that looks at these measurements and tries to approximate what the Earth actually looks like, it's called geodesy. And based off of geodesy, we have all of these different models that in some way define or represent the Earth's surface, varying in their complexity, varying in their accuracy, varying in how well they represent reality. Generally, the geoid is the most accurate, represents reality the most. The sphere is the least accurate, represents reality the least, but the sphere is also the simplest. And that kind of is what makes the ellipsoid the happy medium. It's pretty simple to compute mathematically, but it's also not crazy, crazy complicated the way the geoid is. Okay, believe it or not, that is it for today. So, a couple of practice questions. I'm starting you guys off easy. My class last semester, this lecture and the past lecture were one lecture, and then I forgot that we had imagined a, so I kind of had to cancel my first class and mix those together and I didn't decide to give you guys an extra lecture because of that. So, you have two really, really short lectures between yesterday and today. So, these are a couple of practice questions. Again, these practice questions I give at the end of lecture, as well as what I highlight during lecture about what might be. Okay, you guys, attention for a second? Or at least I think you'll find it important. Exam stuff. I just thought maybe you'd all quite identify said exam stuff. So, when I highlight things and say, you know, hint, hint, wink, wink, nudge, nudge, you might see this on a midterm or final exam. Take that to heart, you probably will. Those things that I say might be on the exam as I'm lecturing throughout. You'll probably see them on. I can almost maybe even guarantee it. These questions at the end of each lecture are super representative of what you might see on the midterm or final exam. Some of them are taken right from here. Some of them are very, very similar. Maybe where it is slightly differently you're asking about something slightly different. But that's why I give these. So, usually at this point at the end of the class, I give you again a couple minutes to try and brainstorm these answers with your notes or with my slides with someone sitting next to you. So, take a couple minutes, brainstorm these. You also don't have to stay if you don't want me to go over the answers with you guys. If you want to head out right now, you're welcome to. If you want to stay about three, four, five minutes, practice these questions with someone sitting next to you, come back and talk with me, and the whole class will go over the answers. And that will be it for today. Thank you. Hey. Yeah. Yeah. It was specifically when I said, when I was talking about, I was just highlighting that if I were to ask you in a midterm or final exam setting what the Tropic of Cancer and Tropic of Capricorn are, there's like three key things that I want you to hit just beyond the definition. One of them is just the definition, which I just write in the slides. The other is what the actual latitude and latitude values are of the Tropic of Cancer and Tropic of Capricorn. And the other is just that they are a parallel. They're each just two different parallels. Just for questions. The exam's are not visualizing. They're open for us. Yeah. Yeah. Hey. Is it going to be like reflection paper or does answering question more? There's no like, I mean, maybe at most you'll have a short answer. But generally speaking, most of the assignments, you submit the questions as true or false multiple choice fill in the blank. Okay. That's just the form of the assignment. Yeah. So it won't be like researching. No. And also, you just mentioned it will be like all like, it's it. So it will be like open though? Yeah. Okay. Yeah. So I'm going to have a hand up over here. Yeah. Yeah. Hey. Why? You said the meridian was like a semicircle. Right? Yeah. It's like a half circle. The parallels are full circle. Yeah. So like on the back of the meridian, is that like 180? So the back of a meridian is so the meridians connect on either side to form something called a great circle. But in terms of measuring longitude with meridians, you have zero degrees, which is your prime meridian. And then it goes up in degrees west or east, depending on which way you go, all the way back to essentially 180, either east or west. That's the same point. And that's it. Okay. So it doesn't, so the prime meridian. Yeah. It doesn't go one to three miles to three miles to the left or right. Yeah. It goes both ways and then connects on the other end. And that's 180 east or west. Yeah. Okay. With latitude, it's a full circle. Look at our lives. Yes. So there's no 180 degree latitude, right? Maximum 90? Exactly. Yeah. That's cool. Yeah. Thank you. Yeah. Thank you. No problem. Okay. If you're heading out, that's no problem. Just scurry for me. I'm just going to go over these questions quickly. If you're heading out, it's all good. Go ahead. But just head out if you're going to head out. And I'm going to go over the answer to these. Mm. Okay. Sweet. So defining latitude and longitude. So someone tell me what is the Tropic of Cancer. Please. Don't make me back. Yeah. Is the angle that describes the northwest position? North south. Yeah. Angle that describes the north south position. Exactly. What is the latitude value of the Tropic of Cancer? Yeah. 23.5 degrees north. And fundamentally what is the Tropic of Cancer? Yeah. The north and most claim of the storm can be directly above you. Yeah. What did you say the Tropic of Cancer was the first time? You defined latitude. Sorry. Yeah. You defined latitude. Right? Yeah. Yeah. Sorry. Sorry. That's not the Tropic of Cancer. Oh. First question is defining latitude. Do you think I was asking you to find latitude? My bad. If I ask the question, sorry. Reset. What is the Tropic of Cancer? The answer was down here. Yeah. The northernmost point where the sun can be directly above you. The northernmost point where the sun at some point of the year can still be directly above you. Or at all times of the year. So the northernmost point where the sun can be directly above you. The value is 23.5 degrees north. And what is beyond just defining where it is, what is the Tropic of Cancer? Yeah. It's a parallel. Exactly. Tropic of Capricorn is also a parallel. 23.5 degrees south. And then how do we define where that is? Yeah. Exactly. The southernmost latitude where the sun can also be directly above you. Okay. Approximations of the shape of the earth. We talked about three. Which one is the most accurate? Which one most accurately represents earth? The geoid and the least accurate? The sphere? What one is in the middle? Ellipsoid. Exactly. Cool. Yeah. Oh, for the Tropic of Cancer, is it one in the year? Like some day in the year at some time it will be directly above? Every day. Every day of the year. Every day of the year. At some point during that day. It can be any point during that day. But at every day of the year at some point during a day, the sun will be directly overhead. In both the Tropic of Cancer and the Tropic of Capricorn. And anywhere within those two latitudes. Any other questions? Any clarifications? I'll be down here if you want to come ask a question or you need to talk to me at all. Otherwise, that's it. I promise the lectures won't always be this short, but generally speaking I always leave a lot of time at the end for you to ask questions as such. That's it. Have a good week. Good day. Good day. Good day. Good day. Good day. Good day. Good day. Good day. Good day. Good day. Good day. Good day. Good day. Good day. Hey. Hey. I didn't hear about the most accurate. The most accurate. What's the difference? Oh, sorry. No, it's good. What are the answers? So the most accurate is the geoid. Oh. It's also the most complex. Okay. And then the least accurate is the sphere. Yeah, those are the kind of assignments that we're expecting. Like paper writing or? The assignments are essentially instructions that you often have to use with some sort of online tool, some sort of website. And then you just answer questions based off the task that we've given you. So your actual submission is just answering a bunch of typically multiple choice fill in the blank. Maybe a couple short answer questions, maybe uploading a file or something, but it's no paper writing or anything like that. Okay. Thank you. Hey. So in the slides, like when we were discussing about the Arctic Circle, so it's the southernmost latitude in the northern hemisphere. It is the southernmost latitude in the northern hemisphere. So why is it like the southernmost latitude if it's in the northern hemisphere? Yeah, because so essentially the way to think about it is in the Arctic, there's in the summer in the Arctic, it's possible for the sun to be above the horizon. So that means that there's daylight for 24 hours of the day. So that's a certain latitude below the Arctic Circle that then there is no longer a single day in the year in which the sun can be directly, the sun can be over the horizon or just available for 24 hours of the day. Yeah. So that's why it's like these, oh, I see, so that's why it's the southernmost latitude in the northern hemisphere because below that south of that latitude, it would be no longer the daylight for 24 hours. Exactly. Just at some point of the year, the way it's it's more it gets more extreme the further like north you get, but just anywhere below the line of the circle of the Arctic, then it is not possible to have 24 hours of daylight or 24 hours of. Night time. Yeah. Yeah. Hey. I have a question for my first blog post. Sure. This is a picture of Venice pre-pandemic during pandemic, which was like the book traffic and the quality of the waters with loads and bad foods. Is that okay for a blog? Yeah, that's great. Yeah. Thank you. Yeah. Yeah. For the Arctic Circle, southernmost that northern hemisphere. Yeah. So what that means is the Arctic Circle, where the Arctic Circle is, is defined by what latitude in the northern hemisphere, at which below that latitude, it's no longer possible for you to experience either 24 hours of daylight or 24 hours of nighttime. Oh, okay. Does that make more sense? Yeah. Yeah. I just want to clarify on the degree is my mind like kind of blanked out. I don't know. Is it 23.5? Yep. 23.5 for both. For both? Yeah. But it's just like north and south. Yeah. Okay. And then also for the tropics, it's the sun is directly above only at some point of the year. Not all year. So the sun in the tropic, at the tropic within, between the tropic of cancer and the tropic of Capricorn, the sun is directly, and it's just the sun. So it's kind of like, directly above you is directly above at least at some point during the day, every day of the year. Oh, okay. Does that make sense? Yeah. So it's kind of like summer, like, depending on season? No, it's so, so it's like, so that that's the thing. It's not dependent on season. So between the tropic of cancer and the tropic of Capricorn, at all points of the year, so 365 days a year, it's possible every single day, and it's not just possible, it happens, every single day. At some point during the day, the sun is directly overhead. Okay. Does that make sense? Yeah. So at some point of the day, the sun's directly. When the sun is directly overhead, changes based on the season. That's true. But the point of where the tropic of Capricorn and where the tropic of cancer is defined is based off of, if you go either north or south of either of those latitudes, then no matter what, at some point during the year, the sun will not be directly overhead on a given day. Does that make sense? That's a more confusing way to think about it, I think. It's easier to just think about it as between the tropic of cancer and the tropic of Capricorn. If you are standing anywhere between those latitudes, every single day of the year, the sun will be directly overhead you at some point during the day, every single day. Okay. That point during the day will change depending on the season, but at some point during the day, every single day, between the tropic of cancer and Capricorn, the sun will pass directly overhead you. Okay. Does that make sense? Yeah. Okay. And that's just all that, the only reason we talk about that is just because that's used to define where the tropic of cancer and Capricorn are. So the 23.5 degrees north and south is just based off of that measurement of where it's no longer possible to have the sun overhead you at some point during the day, every single day. Does that make sense? Yeah. Okay. Cool. Hey. What does it mean by parallel? Parallel. So a parallel is any circle around the entire earth connecting all points of equal latitude. You know, coming in. I don't think I quite understood. So, do you know what latitude is, right? Yes. Okay. So our latitude measures how far north or south we are on the surface of the earth. A parallel is a line connecting all of the equal points of latitude. Oh. So the equator is a parallel because it connects all the points that are exactly zero degrees. Okay. What about long periods? Can we count them as parallel? No. The longitude is based off of meridians. Okay. So meridians are essentially if you took a parallel, right, if you took the equator, which is a parallel, and you turned it 90 degrees, right? So now it's passing through the north pole and the south pole. Yes. That's called a great circle. If you cut that in half, that's called a meridian. So a meridian is a straight line essentially that you draw from the north pole to the south pole that connects it. Okay. And that will be all equal points of longitude along that line. The prime meridian is just where we define zero degrees longitude. And then anywhere east or west of that is going to have, is going to be, you know, ten degrees east, ten degrees west, twenty degrees east, twenty degrees west. Does that make sense? Yes, it does. Okay. Thank you. Yeah, no problem. I'm just wondering, like, for longitudes, are they parallel to each other or are like the parallel on the... So, so parallels are just the names of the circles that connect points of equal latitude. Okay, so the longitudes aren't parallel to each other. So like each meridian. Well, so, so... I think it's a circle as well. Right, but a meridian, which is what we use to measure longitude, is a half circle. Oh. Right, so if you have a meridian that goes around both sides of the earth, that's called a great circle. Okay. If you cut it in half, that's called a meridian. The meridians are what we actually use to measure longitude. Mm-hmm. And they are, you know, if you were to put them kind of, depending on how you projected them, if you were to put them on a map that preserved shape, they would be technically parallel. But that's what I'm saying. The term parallels, when we're referring to parallels of latitude, doesn't... Apply to longitude. Well, it just doesn't mean we don't use that word because the lines are literally parallel. They are, but they're not, if that makes sense. Okay, I understand. Because if you take a... Because it depends on how you project the map, and we'll get into this in the next lecture a little bit more. But you can think of parallels. I wouldn't think of them as... Don't think of them as they're just lines that are parallel to each other. Think of parallels as lines of latitude that connect all equal points of latitude around the surface of the Earth. So the equator is a parallel. The tropics, tropic cancer, tropic capicorn are parallel. The arctic and antarctic circle are parallels. Any circle that connects all equal points of latitude is a parallel. A line that connects equal points of longitude is called a meridian. That's the easiest way to think about it. And also one more question about how the 24-hour days and 24 hours of nights in the south of the northern world. So once we cross that 23.5 latitude-ish circle, so why is that? Is it because the Earth is tilting? So the 24 hours of daytime or nighttime only occurs either north or south of the Arctic or Antarctic circle. So that doesn't have to do with tropic cancer or capicorn. What is the tropic cancer or capicorn that you do with your structures? So where they are is defined by... is defined by if you go as long as you're within those two tropics. So if you are south of the tropic of cancer, north of the tropic of capicorn, or just you're at any point in between the two parallels that represent the tropics, the tropic of cancer, tropic of capicorn, then every single day of the year, at some point during the day, the sun will pass directly overhead you. If you're north, if you're outside of the tropics, so if you're either north of the northernmost tropic or south of the southernmost tropic, then at some point during the year, the sun will not pass directly overhead you. Does that make sense? I know it's a bit confusing to it. Is that the Earth's tilt? Yeah, it's because of the Earth's tilt. So when you are, for example, when it's... I again wish I had my globe, but you know... I'll go back here. If this is our sun, right? This is my Earth, right? And this is the north pole, right at the top here, and it's tilted like this, right? Then you can imagine at some point coming up, eventually, you are going to have a direct line between the sun and that point on the Earth, right? Like that. If you keep rotating at different parts of the year, then the sun's not going to be passing directly overhead. Okay. Yeah. Yeah. So we... I'm trying to visualize this in my head. Thank you. Yeah, no problem. I just have to... What would we call this? Your shadow's tracking. Yeah, exactly. Exactly. Yeah. This is a very quick question. So it's Google Earth's a form of gurai. A form of what, sorry? So the model you're talking about, like the model describes the Earth, we have the most accurate one with the... The geoid? The geoid? Yeah. Yeah. And the least accurate one with the lipsoil. The sphere is the least accurate. Yeah. So is the Google Earth, can we understand it? It's actually a form, like, or a triangle of gurai. The Google Earth... Oh, Google Earth, you're saying. Yeah, Google Earth is like a type of gurai. Okay. Yeah. That's my question. Yeah. Yeah, no problem. How does that apply to the southern hemisphere? I mean, the 24 hours on the forehead. Well, because of the tilt of the Earth... Yeah, it is tilted towards northern hemisphere, right? When it's summer in the northern hemisphere, yes. Okay. When it's summer in the southern hemisphere, then the Earth is, you know, if it's tilted like this, and it's summer right here in the northern hemisphere, then when the Earth travels around the sun, and now it's on the other side here, it's still tilted the same way, but now the southern hemisphere is closer to the sun. So then it's summer in the southern hemisphere. Oh, okay. Yeah. I'll bring my globe next week, and I'll try and do a much better approximation of what that looks like. But yeah. Thank you. Yeah, no problem. Do you have a question? I would just... Okay, cool. Is there another example of the joy, other than Google Earth? No, not really. I mean, I mean, the... The joy is just meant kind of as a term for something, for a model of the Earth that can't be kind of simply calculated using some sort of approximation of an ellipsoid or a sphere. Okay. Okay, so... So the joy and the spheroid and the sphere are really just meant to... Yeah. Like, they're used in reality, especially the ellipsoid and the sphere are used, like that's the most common way to... That's the most common way we use... We apply projections to maps or the most common way we think about the Earth. When we, for example, have Google Maps on our phone, that usually uses some sort of... It's based off of some sort of ellipsoid. When you have a really accurate representation of all of the kind of little bumps and elevation points and every all the topography and stuff of the Earth and how much mass there is and all the time. That's kind of what we mean by a joy. And it's not really used in reality when it comes to things like Earth observation because it's just way too complicated. Except for Google Earth. Yeah, and I mean... Is the joy to do that? That was kind of just to... To like, help simplify for him because, like, if I said that that joy wasn't Google Earth, then I think he might have lost his mind. But the point of that is just the most complex models of the Earth that we have are geodes. The simplest versions, the simplest models of the Earth we have are spheres. And the most frequently used models of the Earth we have are typically ellipsoids. And that's just because they're a little bit more accurate than a sphere, but a billion times less complicated than something like a geoid. So the geoid in particular is your right. It's more of an abstraction. The ellipsoid and the sphere are real. It is like we will take an ellipsoid that has a defined semi-major and semi-minor axes in order to approximate the shape of the Earth and then where we are on the shape of the Earth. The geoid is never really used, to be honest. Yeah, no problem. I was just wondering if this is how the degrees are like, representative just like this? Yeah, exactly. And also, I was up there, like, and then there was another student that was asking about parallel lines, and I didn't really get all of it. What really defines the parallel? A parallel is any circle around the Earth that connects points of all the points of equal latitude. So it's just like, can I imagine like a ring? Yeah, just like that. Yeah, just like that. But it has to connect points of all equal latitude. So that's why the equator is a parallel because anything along the equator is zero degrees. That's why the Tropic of Cancer and Tropic of Capricorn are parallels because anything along those lines is either 23.5 degrees north or south. That's why the Arctic Circle and Antarctic Circle are parallels as well. Okay, I'll get that. Well, thank you very much. Yeah, no problem. Hey. I have a question. Do you know how to look at old satellite pictures through Google Earth? I thought the gas is strictly cool. I see it in the headlight. You're probably not, they're, oh, through Google Earth. I'm not sure of how to change. I feel like Google Earth just has base maps and I don't know if you can really pick a year of what that base map is from. Generally speaking, for trying to find satellite imagery, your best bet is kind of just like Google places. It's hard to find them on just Google Earth or Google Maps. Later in the semester, we get you to use this website called Earth Explorer where you can download actual satellite imagery of the exact same place at any point in the past 30 years or so. But that's kind of really only available on that website. You can't really do it generally speaking from something like Google Earth or Google Maps. So I would just, you're just looking for the blog post, right? Yes. I'm seeing. Yeah. So I would just try and, like, honestly, your best bet is just you just like Google images. The images aren't perfect. The last one, you can compare scenes. Yeah, the exact comparisons are like, are like literally based off like Googling an exact comparison. So it's like you Google a place and then say like, you know, comparison, satellite imagery or something like that. And that's probably where they're getting those. I think I might change locations because these are too zoomed in. It might also be in like, if you search, you modified, if you also just search like in rather images just here. Sometimes it's easier because they'll pop up in like articles and stuff like that. Yeah, I think in articles I saw some good ones. Gazzas strip comparison satellite imagery. Yeah, I mean the Gazzas strip is a pretty small area, right? So that's going to be zoomed in pretty well. If you search maybe more on the scale of like a state or like just something like larger than it might, you're saying your problem is that these images are like too zoomed in for like, that's like a block. Yeah, I don't think I would get worse. Yeah, so where is, so I would do instead just something like comparison, satellite imagery in Israel. Yes, okay, yeah. And then maybe something that like way more zoomed out will pop up. And then you can just say, you can just, when you talk about it, you can say, I Google these, but I'm looking at the Gazzas strip specifically, kind of thing. I don't know if you buy, no one did that, I thought, cool. Cool. Thanks. Yeah, no problem. Hey. Is that a question I got the same? Is it still? All right. All right, hi, everyone. Can i grab your attention? Take a seat. Happy monday. Hopefully you had a good weekend. I'm going to start with evan so that he can update you on what You're supposed to be working on this week. You have questions about blog post or course logistics. Anything related to that? This is a good time to ask him. I'll give it to him and then i'll get going for the day. All right. Hello, everyone. Start with semesters. Not too much going on yet. Things you should be working on and keeping in mind is next Thursday at 11.59 pm. The first assignment is due. The first blog post is due. If you haven't looked at it yet, it says you either submit your Own response just to the prompt or you can respond to someone else's Response. You don't need to do both. Just pick one or the other. You can get a bunch of emails about this. Do your own response or respond to someone else. Leanna's got office hours over the next two weeks. There's four hours in total. She's also available via e-mail. They're up here. On this Tuesday and next Tuesday, there'll be three to four pm PST. This Thursday and the Thursday Afterwards, 10 a.m. to 11 p.m. PST. That was a great time to ask me about any course logistics Or anything like that. If not, i'm going to dip. Any questions for Evan? You can always email them or anything like that too. Email whenever. All right. I'll see you soon. Yep. Okay. So today we are talking about projections for most of the Lecture and then we'll talk a little bit about scale. Evan gave his update already. I wanted to start with some review clarifications from last Tuesday. There are some things i think i could Explain a bit better. I got my props today so i can give some demos and kind of more Clearly explain what was going on. Specifically i'll talk about the elements of latitude. Go over again. Trappic of cancer. Trappic of capricorn. Arctic circle. Why those parallels are the Latitude values that they are. I'll talk about briefly the models of the earth we have again. The geoid, ellipsoid, the sphere. And then most of the day stay we'll talk about projections. So, again, these are things i'm going to review. First thing i wanted to go over was the elements of latitude. So this seemed a bit confusing for a couple of you. I got a couple of questions. I just want to go over it again. Parallels are all of these lines that connect all the points Of the earth with an equal latitude value. So the equator is a parallel connects all the points on the Earth that are at zero degrees latitude. Trappic of cancer is a parallel. Trappic of capricorn is a Parallel and the Arctic and antarctic circle are both Parallels. The Arctic circle, which is 66.5 degrees north, Is denoted by that latitude value because anywhere below the Arctic circle in the northern hemisphere is not capable of Having continuous 24 hours of daylight or night time at some Point during the year. And the same but in the southern Seamus, the same but in the southern hemisphere for the Antarctic circle. I'm going to explain that with my Globe again. So if that's still confusing, don't worry. The tropic of cancer and the tropic of capricorn are Essentially between those two latitudes between those two Guys, excuse me? Thank you. Between those two parallels Between those two values of latitude, it is never possible For the sun to be directly overhead. And i'll explain a little Bit more what that means. I think this graphic does a pretty Good job depicting how that looks. I was hoping to get, maybe I'll do it right here. Can i get a volunteer to come down And be my son? Can i get a volunteer to come down and be my Son? Yeah? Can you come up here? You like that one? Nice. Okay. Can you stand right here? Kind of on this gross spot right there? Perfect. You can face any direction. Okay. So summer solstice, i'll kind Of go in line with what the graph looks like up here. So summer solstice, i'm over here, right? The northern hemisphere Is pointed towards the sun when it's summer in the northern hemisphere. 24 hours in a 24 hour cycle, the earth is rotating on its axes Like this, and then in a whole year it's coming all the way around The sun like this, right? The whole time it's doing that, it's Tilted on its axis. So it's not rotating like this relative to The sun. It's rotating like this relative to the sun. It's tilted. And it's always tilted like that no matter if it's here Or here. But that's what creates summer and winter. So when it's the summer solstice, and we have the longest days of The year in the northern hemisphere, the earth is tilted, the Northern part of the earth is tilted towards the sun. And on the day of the summer solstice, the sun at high noon Is directly overhead, the tropic of cancer. Does that make sense? Because, again, if i'm tilted here, my Tropic of cancer is kind of somewhere around here. The sun is kind of directly perpendicular with that latitude, With that point on the earth. This tilt never changes. So if i come over here, halfway between the summer solstice And the winter solstice, then the sun is directly over Above, over above. But directly over or above the Equator. So the sun is directly over the equator twice a year. When i come over here to the winter solstice, the sun is Directly over the tropic of Capricorn, and then the sun comes, the earth comes back over here. And then again, it's directly overhead the equator. So it's never possible for the sun to be directly overhead If you're not within the Tropic of cancer and Tropic of Capricorn. Now similarly, if i am here, right, summer solstice, Summer in the northern hemisphere, and i am right at The Arctic Circle, if i'm right on the Arctic Circle For one day at the summer solstice, i'm going to get 24 hours of daylight. I'm going to get one day A full sun. Similarly, winter solstice In the northern hemisphere over here, i'm going to get On the day of the winter solstice, if i'm right on the Antarctic Circle, i'm going to get 24 hours of daylight. So anywhere between, so above the Antarctic Circle, and below the Arctic Circle Is never capable of getting 24 hours of either daylight Or sunlight. Sorry, daylight or night time. Daylight or sunlight is the same thing. Does that make sense? Any questions about that? Yeah? Yeah. Thanks. You can sit down. Thank you. So, like, what are we feeling? Is it like once per day? Exactly. Yeah. Once a day or once per year? Once? Well, so both. Technically. The sun passes directly overhead. If it's possible for it to be directly overhead. Once a day. At high noon. Right? Because no matter where On the earth, the sun's always going to rise and set. So it's never going to be directly overhead for the whole day. And then it's, you know, other than the Equator, everywhere else, it would be It would go, let's see, up, down. It would be twice a day. If it was, or twice a year, sorry. If it was going to be directly overhead. So it's directly overhead. So it's the tropic of cancer once a year. Directly overhead the tropic of Capricorn once a year. Directly overhead everywhere between there. Twice. Or it would be the Equator. Just the Equator. Twice. I think. Oh, yeah, yeah. Equator just equated. Or between the Equator. I think it would be twice. It's a point where otherwise I would control it. Exactly. Yeah. Yeah. So twice. Does that make sense? Yeah? Twice or everywhere between the tropics. Exactly. Yeah. And then once on to the tropics. Exactly. Yeah. But no matter where, no matter if it's Possible to have the sun directly overhead once or twice a year. Per day, it's only ever going to be once. Just at high noon when the sun's at its highest In the sky. High noon is just, it means noon but noon we denote in society as noon 12 o'clock. High noon just means the point when the sun is at its highest in the sky. Yeah. So, yeah. Sorry, per me, Can you guys quite down? Thank you. Yeah. So it's twice a year for everywhere except the tropics once a year for the tropics. And then during that one day At one point during that day, high noon is where The sun's at. Exactly. Yeah. Exactly. Yeah. That makes sense. That is, this is all extra. I don't expect you to know Like the, that kind of detail. I wanted to explain this so you guys could wrap your head around it. All I really expect you to understand is why the Tropic of Cancer, Tropic of Capricorn as well as the Arctic and Antarctic Circle have the values of latitude that they have. Which is these reasons here. But I wanted to try and give you a better Graphic to wrap your head around. Any other questions about that? Yeah. So north of the Tropic of Cancer the sun can never be greatly over here. Correct. Yeah. Correct. And same with the Tropic of Capricorn If you're south of that. Yeah. Any other questions About that? About the Tropics? Yeah. So above the Arctic, where the Antarctic What? Yep. The Antarctic or the Arctic? Above both? Yeah. Sure. Yeah. There's only one day where There's either no sun or a Thai sun. No. Only if you're right on the Arctic Circle Or Antarctic Circle. Will there only be one day? If you're Well above the Arctic Circle, there'll probably be a couple days. Maybe a couple weeks, maybe a couple months. Where you'll have continuous 24-hour day Light or continuous 24-hour nighttime. Yeah. That makes sense? Any other questions? Sweet. Okay. So the other thing I wanted to go over was quickly the shapes Of the Earth. There was maybe a little bit of confusion around that. So again, we have our Reality over here. What Earth actually looks like. We have our geoid Which is our most accurate, most complex approximation of the shape of the Earth. We have the ellipsoid here and we have the sphere. In reality, For the most part, when we talk about something like I mainly wanted to go over this because last week someone asked me if Google Earth uses a geoid. Didn't think that through properly. I said yes, it doesn't. Whoever asked me that. I changed my answer. Google Earth, generally speaking, uses an ellipsoid. And the reason that Google Earth and most mapping services in general Use an ellipsoid as the model of the Earth is because Something like a geoid is way too complex. The whole point of a Geoid is that it's not a mathematical Shape. You can't approximate it by just calculating A few simple radii or radius like you can with the ellipsoid. The ellipsoid is just defined by this semi-major and semi-minor axes Which is just the radius out to here and the radius Up to here is what defines an ellipsoid. A sphere is just defined by its radius which is constant in every direction from the center. A geoid doesn't have a constant shape. You see how it's kind of Unglating and it's kind of this weird kind of funky shape? That's the whole point of a geoid. It's meant to represent more closely What the surface of the Earth actually looks like. And in reality The surface of the Earth does undulate like this. It's not a perfect ellipsoid or sphere the way we often model it. But To say that it's a geoid for mapping purposes, whether it's Google Earth Google Maps, whatever, is generally way too complicated For that software or just would require a lot of computations And just makes it more complex than it needs to be. So generally speaking We use an ellipsoid to approximate the shape of the Earth for Google Earth or other Mapping services. Really the geoid is only used if you need a Very very very very precise Specific location measurement or height measurement or elevation measurement So I know sometimes when they are trying to measure How far below the surface an earthquake occurred They'll maybe measure that relative to a geoid because they need a very very Precise measurement. But generally speaking for most Mapping services you're going to see them using an ellipsoid. Because for an ellipsoid All you have to say is, okay my semi major axes is this wide That's it. We have an approximation of what the shape of the Earth looks like. Any questions about that? Does that make sense? Okay. I'm going to assume yes. Sweet. Okay, so today we are mostly going to be talking about projections. We're going to be talking about how the Earth is projected onto maps. What units we use to observe the Earth. And then we're going to talk about the scales of the universe Going from very very minute scales to very very course scales And talk about how we observe those different scales and how we Represent them on maps. So to start I need Volunteers in groups of two So I need three pairs of two to come down and Blow up one of these for starters. And then I'll tell you what You're going to do. But can I get some Can I get some volunteers? Yeah, you two. You want to come up? You two? Pick someone back? Yeah, you two? Sorry guys. All right. Can you guys blow up a globe? First things first. Just one globe for two of you. Can you guys blow up a globe? First things first. Just one globe for two of you. Can you guys blow up a globe? Okay. You can do it. You just got to pinch the little like You got to pinch it. As you blow air and you got it. It just takes some effort. Can you feel it working? If you can't do it, I've practiced this a couple times so I can also try and do it. Yeah, you got it. It takes a bit of time. You'll get there. Okay. Then you guys, you can each pick somewhere Up on the wall here. So one of you on the left here. One of you kind of in the middle here and one of you on the far right here. Oh, that's okay. Don't worry about it. Where did? You got it. You got it. You're getting there. Okay. One team right here. Can I get you guys in the center? Right over here. Can I get you guys over here in the center? Then you guys, yeah. You guys can go right here. All right. You're almost there. Yeah. Almost got it. Okay. I'm going to explain the rules as you guys are finishing blowing up here. You're almost there. Nice. If it makes you feeling better, someone last term literally couldn't blow it up and I had to blow it up for them. So you guys are already doing way better than they were. Okay. So this is what these guys are going to do. They're each going to have A globe in a moment that's blown up. I gave them each One set of scissors. One roll of tape. Tape. And what they are going to try to do is in a minute. I'm going to put a timer on. They are going to try to take this Globe that they have and make it Flat. So you have the tape so that you can tape it up to the wall here. I want you to try to do your best so that you can see The entire globe on this flat surface. So not just one side of it, But all of it, you can do whatever you want to it. But I want you to try and minimize how much damage you do To it. So you can do whatever you want. But the more you Change about it, the worse you're doing. If that makes sense. Yeah. Just like mathematically. What did that mean? We'll talk about it. Okay. You look almost good there. That's good enough. You can close it up. Okay. You guys can move over a little bit. So you got the tape, you got the scissors. You can do whatever you want to it. The more you change it, the worse off. The lower ranking I'm Going to give you. But you can do whatever you want. But you want to try and get it. The whole globe nice and flat up on here. Okay. Okay. You guys ready? You ready? You got the scissors? You got the tape? You got one minute. So 60 seconds. Okay. Ready? Set? Go. You got to go fast. Only one minute. Don't cut yourself. Please. I got, I tried to get safety scissors. Get it yourself. You already have 20 seconds to give you context. You got to go. You got to go fast, fast, fast. I unfortunately only Have an hour and a half to teach you guys so. You already have 30 seconds. I don't even see anyone Having flat on the wall yet. Okay. I'll give you guys an extra minute. Because this is not going as smoothly as I hope to win. So you're approaching a minute now. You got twice as much time. You got another minute. Do whatever you want to do to it to get it up there flat. No you don't have to. No you don't have to. You don't have to. Do whatever you want. Okay. You're almost out of minute and a half here. Okay. You're almost out of minute and a half Here. Alright. I'll give you guys 30 more seconds. 30 more seconds starting now. So does that make your final, make your finishing touches. Okay. Looking nice. Alright. 10 more seconds. Alright. I'll give you guys an extra 10. This is it though. Five more seconds. Alright. And that is it. Alright. Stop there. Let's see what we got. Alright. Not bad. It's gonna be worse. Okay. You guys can sit down unless you want me. I don't want to have to shame you guys while you're up here. Yeah. Let's give him a round of applause. Okay. So we're ranking these based off of how flat they were able to get it, how much of the earth they were able to get flat and the amount. And we penalize them a little bit for those that changed it. More that changed the glow. More than others. But really what we want here is flat. So whoever got it flat us, or whoever you guys think the best job. Let's start on this side. We're gonna vote. We got one vote. Who thinks this one is the best job? Okay. What about this one? Okay. What about this one? Really? Okay. Nice. Good job. That was actually pretty good to be honest. Okay. The point is the reason I got them to do this was how do you guys find out? Was that hard? It was hard? Okay. So the reason it's hard, you almost gave it away there, is that it's physically impossible to take the earth and make it flat on a map without doing some sort of distortion. One of the things that I did when I got some students to do it last year, I think they've done this before. They kind of cheated. But they took the globe and kind of along the lines of what you guys were doing here they made like several slices all the way to the center and kind of spread out each of those slices. That got it pretty flat. So that's obviously heavily changing what that globe looks like. This whole procedure of taking a 3D globe and putting it onto a flattened 2D surface is something that has been a field of research for a very, very, very long time. And it's generally called projections. So a map projection is a set of mathematical formulas and equations that enables the curved surface of the earth so a 3D globe like this to be shown on a flat map. Only a globe that kind of looks like this, only a 3 dimensional curved surface globe can accurately represent all of Earth's metric properties. Things like area, distance, shape and direction can only all be perfectly represented on something 3 dimensional like this. On a flat surface, only some of these properties can be accurately presented. Now that means that sometimes you can accurately represent area or you can accurately represent shape or you can accurately represent distance but it is impossible to act to the... it is impossible to accurately represent all of those things. Every single 2D map that you look at, every single map that's on a flattened surface unless it has some sort of 3 dimensional aspect to it, has some sort of distortion. So essentially no matter what kind of map you're looking at, if it's a 2D map, in some way shape or form is lying to you. It's not telling you in reality how that looks. But map projections are super, super useful to us for navigation purposes, for understanding our landscapes, for mapping the world, we still use them all the time. But no matter what, there's going to be some error associated with them. Okay, I have a little video here that kind of helps describe some of that phenomena. And even still, it's almost impossible to get away from. And that right there is the eternal love of mathematics. The surface of a sphere cannot be represented as a plane without some form of distortion. That was magnetic occurred by this guy a long time ago. Since around the 1500s, map matitions have set about creating algorithms that would translate the globe into something flat. As few as these are called projections. Popular rectangular maps use a cylindrical projection. Imagine putting a theoretical cylinder over the globe and projecting each of the points of the sphere onto the cylinder's surface unload the cylinder and you look flat or rectangular in that. But you could also project the globe onto other objects. And the map used by map makers to project the globe will affect the way the map looks when it's all flattened out. And here's the big problem. Everyone of these projections comes with trade-offs in shape, distance, direction, and land area. Certain map projections can either be misleading or very helpful depending on what you're using them for. Here's an example. This map is called the Mercator Projection. If you're American you've publicly studied this map in school. It's also the projection of Google Maxes. The Mercator Projection is popular for a couple of reasons. First, it generally preserves the shape of countries. Brazil on the globe has the same shape as Brazil on the Mercator Projection. But the original purpose of the Mercator Projection was navigation. It preserves direction which is a big deal between navigating the ocean with only a compass. It was designed to bet a line drawn between two points on the map would provide the exact angles of follow-on compass to travel between those two points. If we go back to the globe you can see that this line is not the shortest route but at least it provides a simple, reliable way to navigate across the ocean. Gerard's Mercator who created the projection in the 16th century was able to preserve direction by varying the distance between the latitude lines and also making them straight, creating a grid of right angles. But that created some other problems. Where the Mercator fails is its representation of size. Look at the size of Africa as compared to Greenland. On the Mercator map they looked about the same size but if you look at a globe for Greenland's true size you will see that it's very small of Africa by a factor of 14 in fact. If we put a bunch of dots onto the globe that are all the same size and then project that onto the Mercator map we will end up with this. The circles between there are round shapes but are in large so they get closer to the poles. One modern critique of this is that the distortion perpetuates a continuous attitude of European domination over the southern hemisphere. Maybe Mercator projection has fostered European and perilous down to its recensory and hit an ethnic bias against the globe. Really? So if you want to see a map that more accurately displays land area you can use the Galkiva's projection. This is called an equal area map. Look at Greenland and Africa now. The size of the Earth state is actor. Much better than Mercator but it's obvious that the country shapes are totally distorted. Here are those dots again so that we can see how the projection preserves area while totally distorting shape. Something happening in the late 60s that would change the whole purpose of mapping in a way that we think about projections. Satellites already in our planet started sending location and navigation data to little receiver units all around the world. Today, more than 10 lives of the many navigation satellite systems provide round the clock, all from precise position fixes, from space to units everywhere, get any kind of weapon. This global positioning system wiped out the need for paper maps as a means of navigating wall to sea and sky. Map injection choices became less about national imperatives and more about aesthetic, design, and presentation. The Mercator projection that once vital tool of pre-GPS navigation was shunned by photographers from outside as misleading but even still most web mapping tools like Google maps use the Mercator. This is because the Mercator's ability to preserve shape and angles makes close-up views of cities more accurate. A negative red left turn on the map is a 90 degree left turn on the street that you're driving down. The distortion is minimal when you're close up, but on a normal map scale, cartographers rarely use the Mercator. Most modern cartographers have settled on a variety of non-retangular projection that's slightly different between sorting either size or shape. In 1998, the National Geographic Society got to the Winkle-Tripple projection because of its pleasant balance between size and shape accuracy. But the fact remains that there's no right projection. Cartographers and mathematicians have created a huge library of available projections each with a new perspective on the planet and it's useful for different tasks. The best way to see the Earth is to look at a globe. But as long as we use flat maps, we'll have to deal with the trade-offs of projections. And just remember, there's no right answer. Okay, so there are two ways that we can classify or categorize projections. One of them is based off of the shape or surface that we use to project the Earth onto. So in this case, we talk about cylindrical projections, conic projections, and planar projections. The cylindrical projection is similar to a Mercator projection. That's what that guy showed in the video, where you take a cylinder, cover it around the globe, and use that to project the Earth's surface onto it. Roll it out, you get this nice rectangular looking map. You can also do a conic projection where you project onto a cone. You can also do a planar projection where you just project onto a plane. Sometimes these are categorized with the second type of categorization that you can use for projections. I've shown them separately here because in the past, I've shown them together for simplicity, but I think that just ended up confusing students. And you'll see why that is in a moment. So the first way that we classify different types of projections is just based off of the shape that is used to project the world onto. And for exam purposes, for midterm purposes, all I expect you to know is that you either project onto a cylindrical surface, a conic surface, or a planar surface, or just a plane. The other way we categorize projections is based off of what map elements are preserved and what map elements are distorted. So the first category we talk about is the conformal projection. The conformal projections preserve angles similar to, again, what was talked about in that video with the Mercator projection. These make them really convenient for navigation. So for something like C navigation or for Google Maps, like it was showing, a 90 degree right turn on the map is a 90 degree right turn in real life. But they also heavily distort area. The equivalent or equal area projections are projections where the size of, say, continents are preserved. You can see that here in reality, Africa is about 14 times bigger than Greenland. And that looks true in this projection, whereas not so much in this one. But the shapes in this projection are wrong. So Greenland obviously isn't actually shaped like that. It's actually shaped like that, but that's much bigger than it is in reality. The other two categories are equidistant projections. This is where, in some way, shape or form distances preserved in the projection. In this example on the right here, it's from the center point of the map to anywhere else on the map distances preserved. If you tried to say measure distance from this point on the map to this point, any two points that didn't include the center point, it wouldn't be correct. But from the center point to any point outward, distance is preserved on that map. Shape and size are not preserved on this map. The last category is compromise style projections. Compromise essentially balances the distortions of shape, area, distance. Those are the three main map elements that we'll discuss. They kind of produce these visually appealing maps that are more true across categories, but don't have one specific map element that is 100% preserved. So you can't measure any metric properties on a map that compromises. So you can't measure area, distance, or shape, but it does produce a nice kind of balanced map. Sometimes, for example, the Mercator projection is a conformal cylindric projection. So oftentimes when a projection is described, it'll be described using the shape that it's being projected on to, as well as what map properties are being distorted and being preserved. I used to teach this as just a category called cylindrical conformal projections, but that's kind of confusing because in reality, if you go out there and look across a wide variety of projections, not all conformal projections are cylindrical, not all cylindrical projections are conformal. Most of the time they are, but not always. So I stopped teaching it like that. So for the purposes of this course, for midterm purposes, all I want you to understand is that you can project onto one of these three surfaces and then just understand what is being distorted and what is being preserved when we talk about a conformal projection, an equivalent or equal area projection, an equidistant projection, or compromised projections. Any questions about that? See? Okay. I'm going to play another video about projections. You don't often see a world map like this, but it's one of the best ways to highlight the oceans. There's no crooked way to put a wild plant on a flat map. Something is going to be distorted. Our top of hers appealed the growth in many different ways, and each has its own advantage. The famous Mercator projection from the 16th century was great for sea navigation, but Greenland looks bigger than Africa, even though Africa is 14 times larger. A German mathematician named Karl Mulleide created analytical projection. You can't use it for navigation, but it accurately compares land to areas. Call the oceans apart and land areas lie even flatter on the page, and are even more accurate in shape. For our map of the ocean floor, we use an interruptible lighting center from the Pacific and divide the land rather than the seas. The three main oceans are shown in their entirety with the least distortion possible. So the whole point of that is there are a wide variety of projections that can look a ton of different ways. If you're say mapping oceans, you'll use a projection that hopefully doesn't distort ocean areas that much. If you're mapping, say, North America, you'll probably try to pick a projection that doesn't distort North America as much, et cetera, et cetera. So one of the reoccurring kind of concepts that's talked about in a couple of those videos is Eurocentric mapping associated with the Mercator projection. And so this has kind of been shown again in a couple of those videos, and I pointed out in a couple of these images of the Mercator projection, you can expect a midterm question on this. Wow. There's a lot of heads just came up. So there's a couple of key concepts to understanding this. One is that is what Eurocentric attitudes mean as a whole, and that just means that it perpetuates this thought that Europe is more dominant relative to other countries. Now you can see here that Africa looks kind of not too much larger than a lot of these other countries close to it. In reality, all of those countries inside could fit inside of Africa. So that is one way that the Mercator projection has perpetuated this Eurocentric attitude, is that it distorts areas more heavily that are closer to the poles. So countries, continents that are high in the northern hemisphere, that are very far north look larger and larger and larger and larger, the farther north you get. That's why the Arctic, Baffin Island in Canada, northern Canada, Arctic, Greenland all look really, really, really big in that projection. So you can see here is because they are heavily distorted the further away you get from the equator. So on this projection in the Mercator projection, you can see again Greenland and Africa look almost the exact same size. In reality, if you use an equal area projection, you'll see that Greenland is much, much smaller than Africa. So the Mercator projection has been argued to perpetuate Eurocentricism in institutions, such as educational institutions, schools, et cetera, partly because it heavily distorts areas like Europe that are in the northern hemisphere to appear much, much larger. The other reason or the other feature of that map that is often argued to perpetuate Eurocentricism is where the prime meridian is. So the prime meridian, the center of the map, if we look back to the Mercator projection here, is Europe. Europe is typically what we think of as the prime meridian, it passes through, there's the prime meridian actually, right there, passes right through Greenwich, England, and that centers the map there. That's an arbitrary center. We don't have to center our map there, right? So there's other maps that have been created that say center the map over the Americas, over Asia. There's other maps where we kind of flip it upside down so that we get this completely different view of what the world looks like. The point is that the big take home from all of this is that map projections have the potential to influence our perception on the world and our perception of our surroundings. So it's important to critically think about what map projection we're using under certain circumstances and why. In terms of the midterm question, the key topics I want you to bring up if I ask you about how the Mercator projection has perpetuated Eurocentricism is A, that it heavily distorts the size of countries and continents as you get closer to the poles, making areas in the northern hemisphere, often more first world countries in Europe, Canada, et cetera, to appear much, much, much larger than they are in reality, which potentially perpetuates this sense that they're more dominant than other countries maybe in the southern hemisphere. The second point I want you to get at is where the Prime Meridian is. The Prime Meridian in the Mercator projection is centered on England or on Europe, which again makes Europe the very, very center of the map. That again potentially perpetuates the sense of Eurocentricism. Any questions about that? These are some additional readings and stuff about projections. They're optional. Don't have to read them. Any questions about projections, about how the Mercator projection influences your perception, has perpetuated Eurocentricism, et cetera, et cetera. You can definitely, definitely, expect a midterm question on that. You can trust me when I say, or just trust me. If I say it's going to be on the midterm, I'm not going to trick you. It's going to be on the midterm. Believe me. Any questions, clarifications, anything like that? All good? Okay. Now this is a brainstorming question that I want you guys to discuss with someone sitting close to you. In terms of measuring distances, today we have the metric system, the imperial system, we use meters, kilometers, inches, feet, whatever it might be. But we didn't always have these standards. So, way, way, way back, before you might pull out a meter stick, or your measuring stick, or your measuring tape, or whatever it might be, what do you think we used? People, I mean, what do you think people used to measure things? How would we measure something without a measuring tape, without a measuring stick, without a standard like that that we use worldwide in order to measure things? Yeah. Yeah. Hand span or wingspan, really good example using kind of parts of our body. Yeah? It seems like the imperial system of what a foot is literally just like roughly the size of a human foot. Yeah. Yeah. For sure. Yeah. Licks and knots. Yeah. How do those work? I'm not even sure. I don't know, but I heard the Caribbean is pretty old and they say clicks and... Okay. Yeah. Yeah. Yeah. Yeah. Okay. All of these kind of have a similar kind of concept, which is that it's related to some sort of standard that some people commonly do, right? In some way. Another one that I've seen is the furlong, this guy here, which is roughly the same thing. This guy here, which is roughly the distance a team of oxen could plow through a field. Kind of random, but the point is we used to always use thumb breath, hand breath, hand span, these kind of body measurements, fathom just a wingspan, cubit from your elbow to your fingertip to try and measure things, because we all have those things. The problem, obviously, is if, you know, some guy that's six, eight is standing next to me, he's going to have a much... Actually, I'm going to have a much larger wingspan than him. He's obviously going to have a much larger fathom, cubit, you know, hand breath, thumb breath, whatever than I am. So that wasn't a standard way to be able to measure things. Eventually, people wanted to be able to standardize measurements. So, here came the meter. Quick side note, the slides that are posted on canvas right now for this lecture don't have these two slides on it. Generally speaking, I'm not going to examine you guys on this, these two slides anyway, so it shouldn't really matter, but you'll notice the file name of the slides I posted for today ends in TMP. Anytime I post slides that end in TMP, it just stands for temporary. It's just because I didn't want to include a couple slides for you guys before I gave the lecture, because I wanted you to kind of genuinely brainstorm things and think of things on your own. It stands for TMP, temporary, because I'll upload these full slides as soon as the lecture is over. So, just a quick side note. Okay, so eventually came normalization, aka the meter. After the French Revolution, there was this new normalized decimal metric system which was proposed, and the meter, originally defined in 1793, was defined as one ten millionth of the distance from the equator to the north pole. That's how we kind of standardized what a meter was. From 1889 to the 60s, that standard was based on a platinum and iridium bar stored in the international bureau of weights and measures in Paris. The point is it was just this physical bar. That's how we kind of had our standard for what a meter was. It was still very close to this length, which was one ten millionth of the distance from the equator to the north pole, but to have our standardized safe measurement of what a meter was, we had this bar that we stored, and was like, this is what a meter is. Since 1983, it's been defined using the speed of light. The speed of light in a vacuum is constant, it never ever changes. So, we've said that a meter is the length of the path traveled by light in a vacuum during a time second interval, or during a time interval of about one three hundred millionth of a second. And that essentially is still based off of, we didn't just pick this, this, you know, three hundred millionth of a second completely randomly. That kind of approximates what one ten millionth of the distance from the equator to the north pole is. But nowadays, we often use these standards based on some physics that we have that exists that we know now in the world that give us a really, really precise accurate measurement and standardization of, in this case, what the length of a meter is. So, in general, the metric system, which uses the meter as its fundamental kind of base measurement, is an internationally agreed upon system of measurement. Most places in the world use it, other than, you know, the states and some other spots. What I want you to be able to do in terms of midterm questions, you'll probably see a, I mean you will see at least, well, not least, one question. You'll see one question that I'll ask you to convert measurements. So, I'll ask you maybe how many microns are in a meter, how many meters are in a kilometer, something along those lines. It's an open book, open internet test. So, if you don't get that question right, I'll be upset. It happens. It does, I get it. Sometimes you click the wrong thing or whatever it might be. But don't, you don't have to over complicate this. You don't have to memorize how to make conversions. You can Google it. We live in an age of information. You can find that pretty easily. But I will ask you about it on the midterm anyways, just to make sure that you know how to use Google. So, let me grab your attention again. We're getting through here. We're almost done. Don't worry. Well, getting close. So, until 2018, the kilogram was the last physical standard that we had in order to define what a measurement was. So, a kilo was defined based on this rock that was also held in the International Bureau of Waits and Measures. And it was this 143-old golf ball-sized metal cylinder. And it was our definition of a kilo. It was the last measure that was physically defined. A kilo is now defined using Planck's constant. Again, another kind of big fancy looking number. Planck's constant is just a constant value that helps describe the relationship between the amount of energy associated with a quantum of light, or a discrete package of light, and its frequency. You don't have to understand that. This isn't a physics class. We will talk a little bit about radiation and about some fundamentals of the electromagnetic spectrum in a coming lecture. But for this purpose, all you... The only reason I bring this up is to just note that the kilo was the last unit of measurement defined by a physical object. Most things now are defined using the speed of light, using some sort of relationship that explains the energy associated with something related to something else, related to something else, yada, yada. In this case, it's called Planck's constant. Initially, in the States, Thomas Jefferson rejected the metric system because he thought it was too French. Remember, the meter was originally a French kind of development. In 1866, the use of metric weights and measures became legal, but not standard in the U.S. And since then, many people have tried to standardize it. For many different arguments, have essentially tried to get the states to convert to the metric system. Probably won't ever happen. I mean, I don't really know how I feel about that. In today's day and age, it's really not that hard to convert between metric and imperial. For the most part, we all have Google. We can all convert things pretty easily. But historically, in the past, in earlier decades, this was actually a problem. This did actually cause issues. One example of that is the Mars climate orbiter, which was supposed to be the first weather satellite observing other planets, so not observing Earth, but observing other planets in our solar system. And in 1999, it burned up when entering into the Mars atmosphere because engineers failed to convert the units from imperial to metric. A lot of money wasted there. I got a quick video about that. Remember when NASA lost a spacecraft because it simultaneously used the empirical and metric measurements on the same mission? Mars climate orbit disappeared 15 years ago this month, and here's a very brief recap of exactly what went wrong. The Mars climate orbiter launched on September 11, 1998 on a mission to orbit Mars. This first interplanetary weather satellite was designed to gather data on Mars climate and also served as a pre-lilization for the Mars solar lander, a mission that launched the units later. But you can't just launch a spacecraft where Mars and trust that it's going to get where it's going. You have to monitor its products. The many-space-bound-talk reaction will keep them already properly, and now in each of these teams behind the interplanetary spacecraft, they constantly monitor the angle of the magnet and adjust trajectory to make sure it gets exactly where it gets to go. In the case of the Mars climate orbiter, monitoring its trajectory and angle of the magnet involved with these steps. First, data from the spacecraft was transferred to the Grand Fire Department's luxury. There was a cross- station fired. The resulting change in the launch of the US made west. One-space-off-work program on the spacecraft and one-space-off-work program on the ground. And here's where the problem went there. In terms of the juices done in the process, it's offered on the spacecraft and the software on the ground, for using two different units' measurements. The software on the spacecraft is measured in pulse, or the changes by cross-ters, in new cycles, accommodates that would measure the unit of the measurement. While the process is offered on the ground, it is the Imperial Panel seconds. And it was unfortunately the Grand Fire Department's data that scientists used to update its spacecraft trajectory. And because one-on-one of the forests was equal to 4.45 meetings, every adjustment was offered after a 4.45. The first spacecraft found that tens of millions of miles to the destination had a number of seemingly small errors of the new data. During the Mars climate orbiter's nine-month cruise to Mars, seven errors were introduced into the trajectory that meant it would have been a boost to the red planet. And today, it was one hundred and five miles closer to the Martian surface than expected. This turned out to be an unsurvowedly low altitude for its Mars encounter. With the spacecraft fired, its main engine would be well made in search of burn that was designed to put it into an elliptical orbit, nothing happening. NASA flossed contact quite abruptly with the spacecraft. So while we know the group caused a disruptment realm, we'll never know exactly what happened to the Mars climate orbiter. The loss of the Mars climate orbiter very sadly happened in space. Fun aside, that story, I'm not going to test you on the Mars orbiter. Just an example of why units of measurement, why conversions may actually matter sometimes. Okay, last topic to go over scale. It's a pretty quick topic. I wanted to briefly introduce that, introduce scale by just discussing the scale of the universe and the different tools that we have available to us to measure things that are very, very, very small and very, very, very large. The electron microscope is for the finest scale measurements. It kind of looks like this. It's a very expensive instrument and can help us see down to individual atoms. So this is a gold atom here. It works by accelerating electrons and using that to illuminate a target. Beyond that, don't ask me how it works because I don't really know. The point is, it is able to view things all the way down to atoms, something that we can't see with our naked eye. The optical microscope kind of looks like this. Maybe you've used it in other science classes at UBC. Maybe you've used it in high school or before university or something like that. But it just uses a bunch of lenses to create a 10 to 100 times zoom. And that can allow us sometimes to see individual cells. So these are chlorophyll cells under a microscope. But the point is, we're going from extremely, extremely, extremely small to a little bit bigger, but still requires us to have a special instrument to view it. Then we have a camera. A camera is kind of the standard instrument that we use to be able to observe things that we can see with our naked eye. Whether that's belief, a tree, a forest, an entire landscape, these are things that we can just see with our naked eye. And the scale of these is obviously completely different to the scale when we're looking at individual cells or individual atoms. And then lastly, we use a telescope often to view planets, galaxies, stars, things that are really, really, really big, but way, way, way far away. I'm not going to play the whole thing. This doesn't need to, I don't really need music for it. But just to give you a sense of that, I'm going to play maybe the first minute or two of this video that was created by IBM back in, I think, the 90s or so. And it zooms out of a picnic in Chicago, I believe. Each, what is it? Each 10 seconds or each second by a power of 10. And you'll see as it zooms out, you eventually see the whole planet and then it'll start going to the solar system and eventually galaxies and eventually further and further and further. It's a 10 minute video. I'll post it on canvas if you want to watch the whole thing. We don't have time to watch the whole thing right now. But I'll play the first minute just to give you a sense of, you know, just the scale of the universe, kind of how small we are relative to things that exist in our solar system, in our galaxy and beyond that. If I could have lost the sight, 100 meters wide, a distance of having around in 10 seconds, cars cloud behind me, cars live in a loss, the powerful reaches our soldiers leave. This square is a kilometer wide, 1,000 meters. The distance racing car can travel in 10 seconds. We see the great city of Lynchville. 10 to the 4 meters, 10 kilometers. The distance of super-s of clouds, the days went to the west. 10 to the 6th, 11 to 6 z-runs, a million meters. Soon the earth will show us its own sphere. We are able to see the whole earth now, just over a minute, along the journey. The Earth will finish over the distance, but those aircraft stars are so much farther away, they do not get up here to move. Our light extends at the true speed of light in one second half process until the order will move. Now we mark a small part of the path in which we learn to move about the sun. Now we go through paths with labor plans, Venus and Mars and Mercury. Entering our field of view is the growing center of our solar system, the sun. Followed by the mass of other planets, swimming live in the big orbits. That honor that belongs to Pluto. A fringe of a billion comets too faint to see, completes the solar system. 10 to the 14th. As the solar system shrinks to one bright point in distance, our sun is playing now only in one of the suners. Looking back from here, we know four southern constellations feel much as they appear from the forest out of the earth. This swears 10 to 16 meters, one right here, not yet out to the next star. Our last 10 second step toward us 10 white years further, the next will be 100. Our perspective changes so much in each step now, but even the background stars will appear to converge. At last we pass the bright star octaves and some stars will do. Normal but quite unfamiliar stars and clouds of gas surround us as a traversed Milky Way galaxy. Giant steps count us as the outsource accounts. And as we pull away we begin to see the great flat spiral facing us. The time and math in chose to leave the sun has blown us out of the galaxy around the course nearly per minute to this. The two little satellite dalts in our own are the clouds of the jungle. 10 to the 22nd time, a million white years. Okay, I'm going to stop there. It's a pretty cool video, it goes for 10 minutes, so it still goes quite a bit further. Really interesting if you want to watch the whole thing, I'll post it on canvas right under the slides for today. There's also this really cool website where you can essentially do the same thing, but you just control the scale and you can zoom in and out to get a sense of at what scale you can observe different phenomena that exist in our universe. It's pretty interesting. The point is the universe is very, very large and relatively speaking we are very, very small. Now, in terms of this course and why that's valuable for understanding Earth observation, for understanding how satellites work is really fundamentally about mapping and how we display scales on maps. So, a map scale is just defined as the ratio of the distance between two points on the map corresponding to the real distance of those two points on the ground. So, how far apart is this distance on the map relative to what that represents in real life? That's what a scale is. And on a map it can be defined using a graphic scale or a bar scale that looks something like this. It can be defined using a fractional scale or ratio scale where you just say one centimeter is equal to 50,000 centimeters or a verbal scale where you verbally say one centimeter on the map represents 500 meters on the ground, etc. The part of this that I want you to remember for midterm purposes, for testing purposes, is the last couple points on this slide, which is that a small scale map covers a large area with coarse detail. A large scale map covers a small area with great detail. So, it's a little bit counterintuitive. The larger scale map you have, the more zoomed in it's going to be. So, small scale maps are further zoomed out, have less detail. Larger scale maps or finer scale maps are more zoomed in and have more detail. You'll notice as you get larger and larger scale maps, the area or distance that is represented relatively between the map and real life is going to decrease. So, you start up here at about 95 million. You get down here to about 300,000 or 1 million, 300,000. The smaller that number gets, the larger and larger and larger scale your map is becoming. So, generally speaking, a worldwide map you might say is one to 40 million. So, one centimeter on this map represents 40 million centimeters in real life. A continental map may be one to 20 million, national map may be one to 10 million, a provincial, regional or state map may be one to 1 million, metropolitan area, one to 50,000, a city may be one to 30,000, a neighborhood may be one to 15,000, and a cadastral map which is just a map that's made for tax purposes. So, it needs to be really zoomed in so you can see each individual property maybe has a scale of 1 to 5,000 or 1 to 500. I don't expect you to be able to match large scale and small scale with a specific numerical value or specific numerical scale. What I do want you to be able to understand is if you're looking at a scale or looking at a map, understanding relatively which one is larger scale, which one is smaller scale. The only exception to that, which you'll see in the practice question that I have here, is maybe I'll ask one that's really obvious, which is if you have a map with a scale of 1 to 500, that's the largest scale that we've talked about. You can assume that that's a large scale map. If I do something that's really, really, really small scale, something like 1 to 40 million, you can assume that's a small scale. But generally speaking, what I want you to understand is these two definitions, what a small scale map is, what a large scale map is, and then be able to describe relatively if you're getting to larger scale maps or smaller scale maps by either looking at a given scale or just looking at two different maps or multiple different maps and comparing them. Okay, that is it for today. I'm going to give you five or so minutes to practice these questions that are up here with a neighbor by yourself, so go over them. These are questions that are very reflective of what you'll see on the midterm or final exam. Practice them. You don't have to stay if you don't want to. If you want to head out right now, go ahead. If you don't want to stay and go over these answers, then we'll do that in about five minutes. I'm trying to minor in this ein con, so I could not register. So I contacted the department, but they are not responding. Did you email me? No. Just email me. Are you going to send me a email? Yes. No problem. Hey. How's it going? If it's 1-500, is this large or small? Large. So if it's a large scale, so is this large? So this is what I was saying. I don't expect you to know on an absolute sense, which is large and small, but I do expect you to know which is larger and smaller relative to the others. So this is the smallest scale map that we looked at. The one that's furthest zoomed in the cadastral map is the largest scale map that we looked at. And then I expect you to understand relatively how would you describe the others. So if you... So it's one small scale, so it covers big areas? Exactly. It doesn't have details. Correct. Correct. And then one more thing. Um... Well, actually, I don't know. Oh, yeah, yeah. What does... What exactly is it? The prime meridian is essentially what describes the center of the map, or what describes zero degrees longitude. Equator. The equator, if you're talking about latitude, but if you're talking about longitude, then the prime meridian is perpendicular to the equator, because it represents longitude, not latitude. So, um, the latitude... Is this latitude? Uh, is what latitude? The prime meridian has nothing to do with latitude. The prime meridian has to do with longitude. Longitude would be this. Yeah, up and down, exactly. So when this is one zero... Oh, sorry, you're saying it's the prime meridian up and down. Um, I'm just asking you, like, what exactly this is, and where it's located. So, so in general, on most maps, it's located... The prime meridian is the very center of the map. It's the grad-a-cule, it's the grad-a-cule, or meridian that goes right through the middle of the map. Oh, and then... In most cases, it goes through Greenwich, England. So in this map, if you zoom in, you'll see it past that line right there, passes right through Greenwich, England right there. That's typically the prime meridian. And then, um, you know that these ones are smaller than the these ones for the... Yeah, it's like contour. What was it, contour? Like, I remember the name of the map. Uh, the, uh, conformal projection? The most common one that we use. That's, like, too deep. Yeah, the conformal projection? Yeah, yeah. So this one's bigger and then this is smaller, so you know... Yeah, areas, areas towards the poles in this kind of projection are more distorted than areas close to the equator. Oh, they mean... And appear much larger than they are in reality. Yeah, go for it. Hey. Question, is it okay? What's going on? I'm gonna go over them right now. Okay. Is it... Yeah, yeah, it's a large scale. Yeah. Yeah. Yeah. Yeah. Hey. Okay, can we go over it after I go over these questions? Yeah. Okay. All right, guys, if you're heading out, please head out. I'm gonna go over the answer to these. If you want to ask me a question, just take a seat and just hold off until right after I go over these. We'll have some time. I'm not going anywhere. But, uh... I just want to go over the answers to these guys. If you're heading out, please do so swiftly, briskly, quickly. However you want to describe it. If you're staying, that's great. You guys staying or... staying or leaving? You do whatever you want. Doesn't matter to me. If you're heading out, just head out. Okay. Cool. So, what type of distortion... I'm gonna crowdsource these, so just put up your hand if you want to answer. What type of distortion is minimized in a conformal projection? Yeah? Yeah, exactly. The angles are minimized in a conformal projection. Angles and shape in terms of describing distortions on maps will often use interchangeably, so you could say angles are preserved or shape is preserved. Either those I would accept. How, how can someone describe to me how map projections have potentially perpetuated your ascentrism? Yeah. The distortion in terms of number ones should be go higher up than the norm if the minuscule is the importance of countries in the group itself. Exactly. Okay. That's... Yeah? Yeah. Okay. That's okay. So, the other thing, maybe you can help me with this, is what projections specifically are we talking about here? Yeah. So, that one was like the Mercator projection. Yeah. And then the other thing that was mentioned was like how the crime already goes through Greenwich, England, which makes Europe look like the center of the world. Exactly. Exactly. Perfect. Okay. How many microns are in one meter? Nice. Good job. Good work. Killer. I don't actually know if that's right. I'm assuming you googled it and it's right. If I have a map scale of one to 500, would you consider that a smaller large scale map? Yeah. Large scale, exactly. And how was a meter defined originally, kind of historically, and how is it currently defined? Yeah. It was originally defined as like one ten millionth of the day. The distance from the north pole to the equator, but now it's defined as like the speed of the distance that light travels in one three hundred millionth of a second. Exactly. Yeah. I'll just repeat that just so that this guy picks it up. It was originally defined by one ten millionth of the distance from the north pole to the equator. It is now defined as the distance light travels in a vacuum in one approximately three hundred millionth of a second. It's actually two hundred ninety nine, whatever all those numbers were, but approximately one three hundred millionth of a second. Okay. Great. That's it for today, guys. I'll see you Tuesday in the other lecture hall. Wait, were you guys coming to ask a question? You're good? Okay. Okay. Yeah. No problem. Okay. Hey, how's it going? Good. Good. Good. Okay, everyone, let's get started. A quick announcement from, or just, I guess, announcement sort of from yesterday. Someone emailed me saying that they left their laptop, I believe, in the serves lecture hall. Anyone come across a laptop by chance? It was you? You found it? Okay. Never mind. We're all good. Okay, sweet. Don't really have any other announcements for today. Any questions about assignments, blog posts, course format, before I get started? Okay. Yeah, you got a question? I'll come home, or wherever. But there won't be a, we hopefully won't be, well, I tell UBC to not assign us a room because there's no in-person component to the final exam. Last semester they did, and it confused a couple students. If they assign us a room, don't go to it. Yeah, just don't go to it. There's no final in-person component. The final exam is online. You can write it from wherever you want, home, somewhere on campus. Doesn't matter. But it's online, fully online. Yeah? Sorry, guys. Can you quiet down? I can't hear the question. What was that? Midterms also online. Yeah. 100% online. Yeah. Both open book, open internet. Can you use everyone? Any other questions? Yeah? It's the midterm is 30 multiple choice and six short answer questions. The final exam is 40 multiple choice and six short answer questions. Yeah. I'll release a bunch more information about the midterm and final when we get closer as well. Yeah. Do you have a question back there? No? Okay. Yeah? You'll have practice questions for the midterm and final exam posted kind of two weeks or so before the exams. Yeah. And then you can also use the questions that are at the end of each set of lectures that I put up. Yeah? It's about 60% post midterm and 40% pre midterm. Yeah. So it's cumulative technically. It includes pre midterm stuff. But it's a little bit heavily weighted, more weighted on the post midterm stuff. Yeah? Is it what? Sorry? Time limited? Yeah. Midterm is two. No. Midterm is an hour and a half. Final is two hours. I'm pretty sure. Yeah. So they're both pretty short. Anything else? Okay. Sweet. Today we are talking about the history of positioning. Just a note about, I mean, just kind of the first couple of lectures that we go through in this course. They are a little bit. There's kind of a couple of fundamental baseline concepts that I'd like to get you guys. Kind of up to speed with at the start of the course that are a little bit. At least I think are a little bit less interesting and a little bit less fun. So this lecture is about the history of positioning. There's a little bit of, you know, important earth observation information sprinkled in here and here and there. But we kind of have to go through a couple weeks first about background info, context setting the scene before we start kind of diving into more technical information and content about earth observation, about satellites, about satellite data and their applications, et cetera, et cetera. So bear with me. I don't love lecturing these kind of earlier lectures just because they are a little bit more dull. But we'll answer the fun stuff. So today we're going to talk about the history of positioning. We're going to talk about why positioning is important today and why it was important historically. We'll talk about the concepts of positioning and how they've developed through time, how they've ultimately resulted in the technology that we use today. And then we'll talk about some of the principles behind space technology we currently use today and historically, again, how we've gotten to where we are today in terms of positioning. So a position fundamentally can be described in two ways. A relative position or an absolute position. A relative position just means that you're describing where you are, you're describing your position relative to other landmarks or features. So you describe your position relative to other things that exist in the world. An absolute position is just defined or described using coordinates of a point in space. So when we were talking about latitude and longitude yesterday, we're talking about absolute position. We're using coordinates to define a point in space. It's a form of absolute positioning. To obtain an absolute position, it generally requires quite sophisticated equipment and precise measurements. In the case of Google Maps or any kind of positioning service or app that you might have on your phone, on your computer, we generally use GPS and Global Navigation Satellite Systems, which use complex computations to determine your location on the surface of the earth. We're going to talk about how we've gotten there, how we were able to define our position before we had satellites to describe where we are. So if I gave you this kind of example, if I wanted you to describe where Jericho Beach Park is, you could describe it as, it's at 49 degrees latitude and minus 123 degrees longitude. That would be your absolute position. That's a coordinate, a set of coordinates in space. You could also describe it. We're not in the Forest Sciences Center at the moment, but if you were on UBC, you could say, to get to Jericho Beach Park or where Jericho Beach Park is located is about a couple kilometers down University Boulevard, and then you can hang a left on Blenka, and then go down 4th Avenue for a couple kilometers, and then you should see it on your left. I'm describing, in this case, where Jericho Beach Park is, using relative positioning. I'm just using landmarks that exist in Kitsilano in this case to describe where this park is. Now astronomy, navigation, surveying, geodesy were based on similar instruments and methods. Historically, the most important practical application of positioning was for sea navigation. This was often based on this concept called triangulation. Triangulation is based off this trigonomic rule, trigonometric rule, that if one side and two angles of a triangle are known, the remaining sides can be computed. You can do that by measuring angles, which is called triangulation, or distances, which is called trilatoration. For this lecture today, I don't want you to worry or really think at all about trilatoration. We're going to be discussing that either next lecture or the one after, I believe. That's the method that global navigation satellite systems like GPS use today to locate where you are on the surface of the earth. I want to talk today about triangulation, because that's kind of historically where we started in terms of determining our position. In this example that I have up here, if I am located here and I want to figure out how far away this star is for me, then I am just located in between two points, A and B, where I know the exact distance to each one of those points. Then if I just measure this angle here and this angle here, I can, using trigonometry, which I don't expect you to know. I'm not going to ask you to plug into trigonometric equations to solve for a solution like this. What I want you to mostly remember is just, if you know two points where exactly two points are in space, then you can determine relatively, however far, this third point might be using trigonometry and using triangulation. The kind of example I thought I'd maybe give to just illuminate this a little bit more, is can I get two people to come stand down here? You two? Sure. Yeah. Okay. So you stand right here, right there, and then you stand right over here. Okay. Perfect. Stairie there. So if I know exactly how far these two are from each other, if I know exactly, what's your name? Wayne. And Mohammed. If I know exactly how far apart Wayne and Mohammed are, then if I am anywhere else, if I am right here, say, for example, and they just measure an angle of where I am relative to where the other is, and then you get an angle for both of those spots, they can compute where I am. So if Mohammed's here and he says, okay, if I'm standing over there, I want to know what the angle is from here to where he is there, and then vice versa over here, if he says I want to know where the angle is to there from where he is here, and I'm standing over there, then we can essentially compute what the distance is between them or really any point between them to where I am over here. If I stand, wherever, if I stand way back here, fall in a part, if I stand way back here, he's got my angle there. You have this other angle here relative to that line that's drawn between them, then they can determine or I can determine how far away I am from anywhere along that line. Thanks guys. You can sit down. That's how it works fundamentally. That's all I really expect you to know how it works. And in terms of midterm, final exam questions, all I'm really going to ask you about triangulation is A, what it is used to map, and I'm going to talk about what that is in a moment. It's called a geodetic network. And then just that it uses angles. Triangulation uses angles. Trielaturation uses distances. You have a question? No, definitely don't need to know that. No. Okay. Historically, to measure angles, we just had essentially these fancy protractors. It was called a theodolite and it would just measure angles both vertically and horizontally at a very, very precise level of measurements. That's what we used historically to measure angles and then determine using triangulation a relative position in space. Now, just remember, triangulation is still a relative form of positioning. What they were used for eventually was to create these geodetic surveys. So the first geodetic survey was undertaken in France at the end of the 17th and early 18th century. By the end of the 19th century, there were major geodetic networks covering the US, Canada, India, Great Britain, large parts of Europe. Essentially, it was the creation of all of these geodetic points. And they are really the historical basis for mapping the Earth's surface. All they did was provide a number of fixed stations or fixed points whose relative and absolute positions were accurately established. And again, all that means is that if I have two points in space and I know in an absolute sense where those two points are positioned in space, then using triangulation based on measuring the angle between those two points and any other point that I choose, I can determine a position. I can figure out where I am or where someone else is. So essentially, it was just these little markers that kind of look like this. They're about the size of a plate or something. There was a lot of jail time associated if you tampered with them because they were really, really important, really valuable for positioning, for society operating essentially. But all it was was these little plates located all across the surface of the Earth and they just represented a known location, a location with absolute coordinates that you could then use to triangulate any other position because you know two positions for sure. And this is kind of what it would look like. So you got all your positions, all your dots of known position all over, and then any other position close by you can determine using triangulation. Okay, now that was for land, but often for sea navigation, you wouldn't have these points of no locations in the sea. So navigators had to figure out different ways to be able to determine their position as they were traveling on ships through the sea, etc. So they used, there were a couple methods that I'm going to talk about, and in a chronological order, they were piloted dead reckoning, celestial navigation, and the marine chronometer. The first two were forms of relative positioning. So piloted and dead reckoning were both forms of relative positioning. They essentially used landmarks to determine where you were in space. And then celestial navigation and the marine chronometer in combination was the first real form of absolute positioning, where you were able to determine what your coordinates were in space, essentially what your latitude and longitude were. And I'm going to talk about those individually here for a moment. So the first method of navigation used was piloted. This was before the 15th century or so, and you always had to be within sight of land. It was based on visual triangulation to known landmarks. So essentially how it would work is if I was, you know, again, if I was in my ship, I'm sailing, going across, and I see this chair here, you know, in reality, maybe it's some sort of mountain, some sort of big rock or something, and I see another one over there, and I know the exact point of where that chair, that mountain is in real life, and I know the exact point of where this other landmark is, then by determining the angle to each of those, I can determine where my position is relatively to those two known landmarks. Yeah? Alternatively, like if you know where you are and you know where one landmark is, you could triangulate the position of another landmark. Yep, totally. But in this case, the context that I'm talking about in this case is more about trying to figure out where you are relative to more so than where other things are, because in the case of, you know, sea navigation, back in the day you don't really have a lot of ways to tell, you know, there's nothing telling you on your phone or whatever exactly where you are in space. So for this specific form of positioning, this piloted, there wasn't really a need for accurate positioning, it was just to kind of let sailors know whether they were on a safe heading in a safe area, and it was generally helped by maps or an article charge. So you would have, you know, your map open, you would, you know, in the distance say, okay, I see that landmark over there, I can see it on my map, and then I see another one over here, okay, we know based on this map exactly where those two landmarks are, let's measure our angle to each of those, and then we can determine our position from there. The next was dead reckoning. Dead reckoning was the first method of navigation in the open sea, which just means that you didn't need to be next to, you know, a landform, you didn't have to be close to the shore, you didn't have to have land in sight, it was used from about the 15th to 17th century, and how it worked was you would calculate your position by using a previously determined position and advancing that position based upon estimated speed over time. All that means is, if I know exactly where I'm starting, if I start right here, and I say, okay, I'm going to head in this direction, I'm going to be going at a certain speed, I'm going to time how long I'm going in that direction for that amount of speed, then I stop, I know the heading that I had at the start that I've tried to ensure is the same the whole way through, I know the speed I was traveling, I know how long I was traveling that speed, based on that I can calculate my new position relative to the original position that I had. So that's how dead reckoning works. Back in the day, I believe they would determine speed based off of, it's called dead reckoning because they would determine their speed by throwing a log or something overboard the ship, and then see how fast that log would disappear as they were sailing away from it, that's how they would kind of estimate their speed, that's why it's called dead reckoning, because the log was dead or not moving. And it was pretty accurate at short distances, you can imagine if I'm standing here and I walk this way, and I time how long I go in this direction for five seconds or whatever, I'm probably going to get a pretty good estimate of where I am relative to where I started. If I do that same thing, but I'm walking for days and days and days and months and months, maybe even years, you can imagine I'm probably not going to get a very good estimate of where my new position is relative to where I started. So it was accurate at short distances, but over long distances you would get cumulative errors, which just means that the longer you traveled, the less accurate your estimate would be. Yeah? Yeah, it's partly because your speed varies, it's partly because it's hard to always stay on the exact same heading. It's hard to, essentially it's this concept of the more data you have, you know, the longer you're traveling, the more variation there is potentially. The more different speeds you could be at, the more different heading you could slightly have, the potentially less accurate your sense of how long you've been traveling in that direction might be, that kind of thing. Okay, eventually people realized dead reckoning had these errors associated with them over long voyages, so navigators tried to figure out other ways for these really, really long trips in order to get a sense of where they were and their positioning. Part of that solution lied in the sky, another part of that solution relied on time, and I'm going to talk about what those two were. For celestial navigation, in order to calculate latitude, you can use the stars. Now the stars in the night sky change for most of us on a daily or seasonal scale, and this kind of goes back to what we were talking about yesterday, with how the earth is rotating, and how it's revolving around the sun. If I am, say, you know, somewhere here in the United States or something, and my earth is spinning like this, and there are stars existing in the universe in every single direction, then you can imagine as it's spinning, I'm going to be pointing in a different direction the whole time. So for some time, if you know, if I'm, again, if I'm located wherever I was in the states here, and my earth is, and I'm located right here, and I'm rotating, I'm going to be looking at all the stars that are in this direction. But as I keep rotating, I'm going to be looking at all the stars that are in this direction. You obviously don't see stars when it's daytime, so wherever you are pointed on the earth when it's nighttime, that's the direction of the stars that you're going to be looking at, which also means, if I'm rotating this way, right, my sun is, if I say my sun is this chair right here, right, and I'm rotating like this on a daily basis, but on an annual basis, I'm also revolving all the way around the sun, and I'm over here, and that means now my daytime is going to be when I'm pointed this way, which means my nighttime is going to be when I'm pointed this way, all the stars that I see at night are all going to be the stars in this direction. When I revolve back around the sun, over here, my daytime now is going to be when I'm pointed this way, all the stars that I see at night are going to be the stars that are in this direction. So, that kind of creates a bit of a problem for navigating, for using the stars, because the stars that you see are always going to change on a daily basis, and are generally going to change on a seasonal basis. Now, there is a bit of an exception to that, and the way we generally use celestial navigation, which just means using the stars to figure out where you are, is by using the north star. So, to measure latitude, only latitude, not longitude, we can use the north star. The north star is a really special example, well, it's actually not that special of a star, but what is special about it is that it is located almost directly above the north pole of the earth. So, if a star is located directly above the north pole of the earth, that means as it's rotating, that north star is pretty much not going to move. It's always going to be in that same location directly above the earth. As I'm revolving around the sun, it's also not going to move in the sky just because of how far away it is, and the distance that we travel revolving the sun doesn't really make any difference in how far away or in the location of that north star relative to us on the planet. Does that make sense? Okay, sweet. So, essentially how that works then is if you determine in the northern hemisphere what your angle is, relative to the north star, you can determine what your latitude is. If you are directly on the north pole to get or to see the north star, you're going to look straight up. So, you're going to be at a degree of 90 degrees. 90 degrees, we know is the north pole. If you're at the equator and you're looking towards the north star, the north star is going to be right on the horizon. You're going to be looking straight out instead of straight up, which means you're going to be at zero degrees. Zero degrees latitude. Does that make sense? Sweet. Okay. So, the angle of the north star never changes. If you're in the southern hemisphere, you're not as lucky. There's no south star, so there's no star that is kind of perfectly over the north pole, or there's no star that is perfectly over the south pole. The way that the north star is pretty much perfectly over the north pole. There are some stars that are very close by to the south pole that are typically used and will still give you a pretty good sense or a pretty good estimate of your latitude, but the north star for navigating the northern hemisphere is pretty accurate. It's pretty solid for determining your latitude if you're in the northern hemisphere. Okay. Longitude is a lot harder and resulted in a lot of shipwrecks for a long, long time. No one could really figure out how to use the stars to measure longitude. And it turns out that you can't. You can't use the stars. You can't use celestial navigation to measure longitude. But what you can use is time. So, the key to calculating longitude is time. Whoops. The distance from a line of longitude to the prime meridian can actually be measured in hours and minutes. And that is because of the rotation of the earth. So, the earth revolves once on its axis every 24 hours. We know that there's 360 degrees of the earth because it's a full spear. 180 degrees longitude east, 180 longitude west. 360 degrees total. It revolves fully on its axis every 24 hours. That means every hour it revolves about 15 degrees. And every four minutes it revolves about 1 degrees. So, if you compare time between a known location and where you are standing, you can equate the difference in that time to your longitude value or to where you are, say, relative to the prime meridian or something like that. And that was done using this marine chronometer. She was developed by this guy named John Harrison. It was essentially just a really, really, really, really, really accurate clock. That's all it was. It's just a fancy clock. It requires that an observer knows exact green Greenwich mean time at the moment of observation, which denotes your zero degrees longitude. And then every four seconds of time error, the position measurement will be off by approximately one nautical mile, which is just to say the clock had to be really, really, really, really, really precise. Just had to have a really good sense of time. And how it works is kind of shown in this graphic here. So, if I start at zero degrees longitude at my prime meridian that goes through Greenwich, England, and it is 12 noon in Greenwich, England, I'm going to have two clocks on the marine chronometer, one that's going to keep the time in Greenwich, England, and one that's going to be changed to reflect the time of wherever I move to. The time that's changed, the time that I'm going to change myself is going to be based upon where the sun is in the sky. So, anywhere around the world, when the sun is at its highest point in the sky, that's generally noon. So, if I say start here, I got my two clocks, I travel, say, 30 degrees west, I then look at my, how did this one work in this case? I then look at my ship's chronometer, which is just the time that it is in the place where I am. So, I look up in the sky, I say, okay, the sun's going over, it's reached its highest point in the sky. It's exactly noon right now. I change one of my clocks to show that it's noon right now. I can then look at my other clock, which is showing what the time is in Greenwich, England. In this case, it shows 10 a.m. and because it's a two-hour difference, I can say two hours times 15 degrees is 30 degrees. I know I've traveled 30 degrees from the prime meridian. Does that make sense? You want me to go over that again? Yeah? Yeah? Sorry. Okay, I'll go over again. Okay. So, essentially, it's based on the marine chronometer. The key about this is that you have two clocks. One clock is always constant. It never changes. In this case, that's this one here that's representing, they call it local time, which is a bit confusing, but this local time clock is referring to the time in Greenwich, England. You're never changing that clock. That clock is always remaining the same. This other clock, the ship's chronometer, you are changing based off of when the sun is reaching its highest point in the sky. So, when the sun reaches its highest point in the sky, you change the clock on board the ship to say, okay, it's now noon. You never, ever, ever touch this other clock. So, when you change this clock to say, okay, I saw the sun just reach its highest point. I know it's noon. I'll change my clock to say that it's noon. You look at your other clock that's telling you exactly what time it is in Greenwich, England, and you see that there's a two-hour difference. We know that the Earth revolves 15 degrees every hour, so if there's a two-hour difference, you just say two hours times 15 degrees, that's a 30-degree difference in longitude. Yeah? It didn't look like any precise way. I'm like telling you it was noon, because I was just like, you know, to this hot ball there. No, you'd use the theodolite that I was showing before, so you'd have a really, really accurate measurement that would tell you the angle of the sun. Yeah. Yeah? Yeah. Yeah. Yeah. Yeah. Exactly. Okay. So it's changing, but we're not managing. Exactly. Yeah. Yeah. Yeah. Yeah. Yeah. Sorry, I can't hear you. Let me come up that way. Sorry. What's that? So, I want to put two clocks. Two clocks? Two records. The locus high and the nowadays two always records the top new cards. Yeah. Yeah. So one is showing, one clock here is showing wherever you started, and then one clock is showing wherever you've gotten to. So this clock here, in this case, 10 a.m. local time is referring to what time it is in Greenwich, and then this clock here is referring to what time it is locally onboard this ship. Does that make sense? Which one is referring to what? On board. On board? This one here. This one's referring to the time onboard the ship. This one's referring to the time back in Greenwich, England. So this clock here, and this clock here, the same clock, this clock here, and this clock here, the same clock, this clock, we're changing manually based on where the sun is located in the sky. This clock is changing, but not by us. It's just ticking through time. That makes sense? Yeah. So, guys, can you quiet down? I can't hear the questions. Can you repeat the part of like, can you say like two hours time 15 degrees? Like, what is that? Yeah, that's because we know that the earth revolves on its axis of rotation by 15 degrees every hour. So we know that if there's a change in two hours, then that's 15 degrees per hour that it's changed, two times 15, 30 degrees. That's like how it changed up to long to? Yeah. That's how we measure the longitude. As we say, essentially, we say what's the difference in time? And then if it's in hours, we just say multiply the amount of hours time difference by 30, that's the change in longitude that we have. And then that gives us what our longitude value is. Yeah. Yeah, that's 30 degrees? Yeah, exactly. Yeah. Yeah. Yeah. Why does every thought give the stars for a longitude? Because that goes back to what I was showing here. If we are around here in longitude, anywhere you are in longitude, at a certain, in a given day, for example, is pretty much all going to see the same stars if you're at the same latitude. Does that make sense? Yeah, because the stars that you see, depend on, or that the, okay, let me, let me. Pardon me? The angle of the stars would be different depending on what that is. Right. Exactly. The angle of the stars won't be different depending on what longitude you're at. Yeah. Yeah. Okay. Yeah. That you'd have to determine on your own. You'd have to know whether you're traveling east or west. But you would be able to do that with a compass. Yeah. Yeah. Yeah. What's your point of view like any map to calculate like, you know, distance along the tree? You might have to do number of hours difference times 30 degrees. That'll be all you have to do. Yeah. Sorry. Times 15 degrees. My bad. Times 15 degrees. Number of hours difference times 15 degrees. Yeah. Just out of curiosity, what if you're not? What if you're traveling diagonally? Would that still be one hour? So if you're traveling diagonally. If I started from this year's and I'm going about this year, because the wind that takes more time. We're measuring the time to remember, right? Yeah. So diagonally. Yeah. Exactly. If you're traveling diagonally, all you're measuring is the difference in, you know, the essentially angle of the sun from where you started to where you ended. So if you're traveling diagonally, the difference in the angle of the sun is only really going to matter because of the longitude. Does that make sense? So essentially, if you were traveling diagonally trying to figure out where your location was, you would use the stars, the north star, for example, to determine your latitude, and then you would use your marine chronometer, the difference in local time, to determine your longitude. Does that make sense? Okay. So, yeah. Sorry, I can't hear you. I mean, it could be minutes. It could be in hours. Doesn't really matter. Yeah. Exactly. Yeah. So it's one degree of longitude for every four minutes. Yeah. So I just did where my clicker go. So let's sit right there. One degree of longitude in four minutes or 15 degrees in an hour. Yeah. For a little bit of a minute, I'll make it three hours. Three minutes. A little bit of a minute. Yeah. Yeah. So that's just another way to denote latitude and longitude. They're not using, Google Maps is not using celestial navigation and a marine chronometer to determine your location. They're using a global navigation satellite system, like GPS, but they are giving you latitude and longitude. It's just degrees, minutes, seconds. It's just another unit of measurement that you can use instead of decimal degrees, which is kind of the example that's given here. So that's in decimal degrees. But degrees, minutes, seconds, which you'll often see on whatever other platform you're using to look at position is it's measuring the same thing. It's just a different way to denote the angular unit of degrees. That make sense? Just historically, that's how they traditionally showed latitude and longitude. It has nothing to do with the marine chronometer and using time difference to measure longitude. Degrees, minutes, seconds is just another way to measure longitude. It's how it works is in degrees, minutes, seconds, you have three numbers. The first one is degrees. The second one is minutes. There's 60 minutes in a degree. And then it's seconds. There's 60 seconds in a minute. It's literally just another way to show latitude and longitude, but it's showing the exact same thing. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Let's straight up. It's exactly perpendicular to where you are. At the highest point in the sky. Exactly. Right at the horizon. So because if I am located right on the equator and I have the north star that's located directly north of the north polar, directly up from the north pole, then if I'm on the equator, I can only see and it's straight up there. The angle that I'm at only allows me to see the north star right on the horizon. If I'm on the north pole, I'm looking just like straight up in the sky because I'm here and my north star is straight up that way. Whereas if I'm on the equator, the north pole or the north star is like that way. And because of the shape of the earth because it's an ellipsoid because it bulges at the center here. If I am or just because it's a sphere, if I'm below the equator looking north, you're not going to see the north star because it's going to be below the horizon. Yeah. Okay. Sweet. I am going to go on then. So essentially what you can expect in terms of talking about the marine chronometer and using a marine chronometer to measure longitude, I'll just give you questions if it were in a midterm or final exam setting about the difference between two local times. So I'll just say something like if, you know, I'm measuring a change in local time of four hours, what is that difference in longitude? Say four hours times 15 degrees, that would be your answer. Okay. Now, all we've really done today is instead of using stars and natural objects that exist in our sky to locate where we are, since about 1957, we've changed celestial navigation to be with satellites. And satellites are just these artificial objects that are in the sky. They represent a landmark that exists in the sky and we can use satellites to just locate where we are on the surface of the earth. You need at least four satellites to accurately determine where you are on the surface of the earth and we are going to talk exactly about how that is done next week. So for today, you can practice trying to answer these questions, try and practice them with someone sitting next to you or just by yourself. What is the difference between relative and absolute position? What is the difference of longitude represented by 12 hours difference in local time using a marine chronometer? Which methods of historical positioning use absolute and which use relative positioning and why is latitude easy to determine with celestial navigation at least in the northern hemisphere? I'll give you five or so minutes to practice those. We'll come back and answer those all together. If you don't want to be here for that, feel free to head out. Otherwise, we will go over those in a sec. How much difference does one hour represent? 15. 15? How much difference would 12 hours represent? 15. 15. 15. 15. 15. 15. 15. 15. 15. 15. 15. 15. 15. 15. 15. 15. 15. 15. 15. 15. 15. 20. 15. gradient circle of those results into the north star. I'm going to skip one of those results. If you look up to the North star, the angle that you look up to the north star at is equivalent to your latitude. If you are the equator, the north star is just over the horizon, which means you are looking straight out at 0 degrees. So, that means you are at 0 degrees latitude. If you are looking, if you run. Not if. Right? If you pull this, and I say that this is my north star, right? So it's directly north over the equator or over the north pole, right? So I'm directly over the north pole like this. If I'm positioned on the equator right here, and I'm looking at it this way, it's right over the horizon. That's the line. Oh, okay. Yeah. Okay. That makes sense. So, but then if you're here, for example, and then it's zero or... If you're here and you're looking straight up, perfectly off, right into the center of the sky, then you'd be at 90 degrees. So you're 90 degrees like this? 90 degrees north. Okay. Okay, that makes sense. Thank you. Okay. Yeah, no problem. Hey. I have a slide, so I was thinking of using the conometer in the local time, so... Is one held constant while the other is moving? Exactly. Yeah. And then the conometer's constant, so does it... Does the arrow not change? This one's showing... This one's a little bit confusing. So this one's showing that this one's saying your local time is established by the angle of the sun. So this one's saying that if you were here, then you would determine that it is 10 a.m. local time by looking at the angle of the sun. Okay. And your marine chronometer is saying noon, so that's a two hour difference. Makes sense. Okay. Thank you. Yeah, no problem. Hey. Actually, a little bit confused about second question. I'm going to represent by 12 hours, depending on the level of time. Do we use this equation? Mm-hmm. Right? Yep. So use this equation to basically answer. I have no question. So if a 15 degrees difference is one hour, then how much degrees difference would 12 hours be? Okay. Got it. Yeah. I also have a slide question about our assignments. I think with a question about Mount Logan. I'm not going to know the answer to it. You've got to talk to... I don't know. Oh, okay. You can ask me if you want, but I'm probably not going to know. So the location is really not exact. So maybe... So for example, we... Talking about a specific question? Yeah. I think we'll take a 15. Okay. Yeah, you definitely asked the TA. Okay. I'm not going to know. Thank you so much. Yeah. Sorry about that. It's okay. Hey. I'm not going to know where exactly you are. Yeah. So all these methods that you went over in class, weren't they like all relative to them? Because you are trying to know your location based on what other landmarks there are. Yeah. And then they were like, you know... Yeah. So the only difference in that is just like based off the definition of relative and absolute positioning. Absolute positioning allows you to determine an absolute position in space, which just means that you're able to determine coordinates. You're able to determine a latitude and longitude. And what about relative? And relative, you're maybe not able to determine a latitude and longitude value. You're not maybe able to determine exactly where your point is in space, but you can describe where you are relative to something with maybe a known location. Oh, okay. Then for example, this kind of like, not precise method would be relative. Yeah. And then this kind of like precise method would be... Yeah. Absolutely. Am I correct? Yeah. Yeah. And then... I can see where you're getting that because you could argue that for like, you know, for a pilotage, if you were looking at like a chart here or something, and you were using like two, you know, two known locations on this chart. Exactly. And then where you are. Yeah. And then using triangulation to figure out where you are relative to that. And then based on that, deriving what your absolute position is, I can see like how you could think like that's a bit confusing. But in general, we classify this as... As relative. As relative positioning. And then one more thing. Can you go over this? Yeah. Yeah. So the geodetic points are just these points that are... this is a nice diagram look at here. Geodetic points are each one of these points on this like map example that it shows here. And it's just a point that has either a known absolute or relative position. So the points that we know the coordinate, like exact coordinate of. Exactly. And then you can, if you can locate two points that you know the exact coordinate of, then you're able to locate any other point in close proximity to those using triangulation. So you'd be like, okay, this point here, and then another point here. And then you would know your low choice of wherever like... Exactly. Relative to those two. So I'm trying to go in. Exactly. Exactly. Thank you. Yeah. Thank you. Sorry. One second time. Yeah. What's up? I just wanted to know the sponsor. Yeah. Sure. Yeah. How does this like... Yeah. So you want me to just display it again? Yeah. So the... so all this is... all this is... all this is fundamentally is just based on a difference in time. Right? Notice the clocks here are both the same. The clocks here as you've traveled away from the start are two hour difference. All you got to do is two hours times 15 degrees. That's a 30 degree difference in longitude. Okay. But here we are taking the angle with respect to the sunlight for board the places. So here, yeah. So here you're saying, you're saying, okay, I'm going to set two of my clocks, both clocks, to exactly noon when the sun is at its highest. Then when I get here, I'm going to use the angle of the sun to determine the time of one of my clocks, and I'm going to leave my other clock untouched. It's going to keep continuing to track time, but I'm not going to adjust its time manually. And then based off the difference between those two clocks, you can determine how far in longitude you've traveled from the start. Here's no effect of the speed of the ship. Correct. Yeah, no effect of speed. No effect of speed, right? Yeah. Okay. So if we... you know, we can see the effect of the speed. It's just rotation speed of the Earth, right? I'm less speed. Exactly. Yeah. All that... all that... this is based off of the idea or the fact that the Earth spins about its axes like this. It does a full rotation every 24 hours, which means that every hour it's rotating 15 degrees. So ship speed is unrelated to this? Yeah. Yeah. No problem. Hey. Question about the assignment. So we... the assignment? Yeah. Okay. Where do we answer? Do we... There's a... there's like a canvas quiz on the submission page, so you'll put all your answers in through that. Okay. So we don't... like... So what if... say if we started it and then... Yeah. What if... is there a tap time limit? Nope. There's no time limit. What if we... we called... happened to... closed that page? Should save all your answers. I would maybe also save your answers in a Word document or something as well. Okay. Just in case. So I saw... as we don't hit the submission button, so... Yep. If you don't hit the submission button, it should just save your answers and it won't submit until you submit. Okay. Yeah. No problem. Hey guys, I'm just gonna go over these questions. Can I chat with you after I just review the answers to these? I mean... It will be a quick conversation. Okay. Oh, I was trying to like... I just heard from this conference. It's like full. Yeah. Did you email me? Yeah, I emailed you. Did you email me yesterday? Yeah. Okay. I haven't gone to those yet, but I will. Okay. Okay. Yeah. No problem. We're good. Okay. Cool. Thank you. Okay. Let's just go over the answers to these. Can someone tell me please, what is the difference between relative and absolute positioning? So maybe start with relative. What is... what is relative positioning? How do you describe relative positioning? Yeah. Yeah. Exactly. Uses landmarks and features to describe your location relative to those landmarks or features. What is absolute positioning? How do you describe your absolute position? Yeah. Yeah. Using coordinates of a point in space. Okay. What is the difference of longitude represented by 12 hours difference in local time using a marine chronometer? Yeah. 180 degrees. Exactly. 12 hours. 180 degrees. 12 hours times 15 degrees per hour. 180 degree difference. Which methods of historical positioning use absolute positioning and which use relative? So which two did we talk about that use absolute positioning? Yeah. Yeah. Yeah. Yeah. Yeah. And then one other as well for absolute positioning. Per me? That's okay. Yeah. Yeah. Exactly. The celestial navigation and the marine chronometer in combination essentially. Celestial navigation gives us our latitude. Marine chronometer gives us our longitude. So why is latitude easy relatively to determine with celestial navigation at least for the Northern Hemisphere? Yeah. You can use the North Star as light around. Yeah. And why can you use the North Star? Exactly. And what is your angle to the North Star represent? It's in the question. Yeah. Exactly. Yeah. Exactly. Okay. Sweet. That's it for today guys. I will see you Monday. Leanna has office hours for assignment one. She'll also be here next week on Monday and Tuesday to answer questions you might have in class as well. So if you can't make her office hours, she'll be here in class. We'll see you next week. Thanks. Have a good day. No wise words of Snoop Dogg? Are you recording them? I'll leave it, I won't see them out. You guys are on the radio. Hello, I'm on the radio. Hello, I'm on the radio. Yeah, that's right. I'm playing the dirty one. We can figure it out after. Just let them know. I do have my computer. Just let the nose fall into this. If you can give her a hint with that. If I be something to do with the files like already. It's been named the same or something like that. I just uploaded it from my... I'm here now. Okay, we're good? Cool, I can unblur. We're starting with... Yeah. So we're good to do these tips tomorrow, or you want to do them right after. Like switch them. What? What's going on there? Nevermind. No. Everything's all good with mine. How do you get the same? There's all that we can do. Okay. Okay. Oh, there. Should I make her just see where one of them? I clicked around. Oh, what do you... I clicked... Don't worry about it. I fucked it up. That was a good one. That was a good one. Don't mind Evan's vulgar language. I'm just sorry. We're getting started. Alright, everyone take a seat please. Evan's just gonna give you some reminders about what you're working on this week, and then I'm gonna hand it to Biena. She's gonna give you some tips and tricks for the assignment that you're working on this week. That's due on Thursday. If you have any questions for her, this is a good time to pose them. And then she'll head out, and then I'll give my lecture, and she'll be back tomorrow as well if you have further questions for her. So I'll start with Evan, and then we'll go to Liana, and then we'll go to me. Sweet. So pretty straightforward this week. There's assignment and blog post one due on Thursday at 11.59 pm. I've had a few questions about blog post one, where to get the actual like imagery, like the before and after. You're using Google mat earth pro in the assignment one, right? Yeah, that's a great spot to look because it'll have historical imagery, and you can flick it on and off. Other than that, things like Google earth engine, Google maps used to have the functionality. I don't know what happened to it. Yeah, so blog post one finds some images that are changed. And then blog post do is do next Thursday from same idea, Thursday, 11.59 pm. And then office hours, Leona's got two more this week, so tomorrow, 3 to 4 pm, and then Thursday, 10 am to 11 am. That's it for me. If you have any questions about syllabus content, let me know. If not, I'll hand it off to Liana. Any questions for Evan? These will also be uploaded on canvas. Okay. Cool. All right. Can you wait for me and go in the same bus? Okay. Hey guys, I'm Liana. Nice to meet you. I know I wasn't here when we did introductions. I was home, but I think I've met a couple of you in office hours. So I'm doing this assignment and I'm also doing assignment four. So you'll see me this week and then after your midterm. So just a couple tips for assignment one. I know there are some questions which are kind of confusing and that's because we kind of redid some of these assignments over the break. And I was trying to make them less confusing and I think, especially for one, I did the opposite and I made it more confusing. So for oh, so these are not the right slides. I updated them. They didn't take. So this will be updated. So I'm going to go through these tips and just clarify a couple things. I know there's been a lot of chat on the discussion board. So just overall when you're using Google Earth, use either the Google Chrome tab or download the Google Earth software. Someone was using a iPad app. I'm sorry if I'm calling you out right now as I'm not using a name, but I don't know how that works. And there was like a confusing, like one of the numbers wasn't showing up. So I would really, really recommend using either the Google Chrome tab on a computer or also the Google Earth software on a computer. If you don't have access to a computer wherever you live, you're free to use the library. If you really, really have no access to a computer at all, come to one of the office hours or message me and we can figure something out. Yeah. So please just use either the Chrome tab or the software. It's easier to use a mouse with like a scroll bar. So like I can't lift that up, but you know, like an actual mouse and not the mouse pad, but laptops. Make sure to turn on everything under the map style tab. That will really help you during this assignment. And if you are really, really new to Google Earth, there's the video which is posted on the assignment one tab and that kind of goes over just a bare bones introduction to how to use the Google Earth tab. So some question clarification and again, I have to go over this a bit. So question five is referring to the main feature. So when you turn on that 3D thing, like you can see boats or grass or something, but what we're really asking for is which of the features listed on canvas is now like popping out at you. And I changed this one a little bit so it should be a little more clear. For question seven who owns the land, there's a bunch of signs and labels nearby and it might take a little digging, but it's definitely on the Google Earth. It's in the area so you might have to click around a little bit, but it should be there. Question nine can be any type of equipment and it's going to count multiple answers. Question ten, if you saw this and thought, wait, why is it asking for two names when on canvas it only asks for one, this is the thing that this was from last year. So this question had a lot of confusion with it last year. We're only asking for one name. This needs to be changed in the assignment tips. So there's only one name for seawash rock that we're asking for. It's a squamish name. It can be found on Wikipedia, scroll down until you see name and it should be like right there. If you just look at like the first part and it says like nine pin rock or something, don't put that. That is wrong. That also was confusing last year. That is not the right one. We're looking for the squamish name. Okay. These are also the wrong office hours because these are from last year. So when you check office hours, look at ebb and slides that are posted today. Again, I know that I updated these slides but I think I posted the wrong one. So thanks to those of you who mentioned this in the discussion board, I'll fix this tonight. You can find the right zoom hours in the zoom tab on canvas and it will have like the link and the right time. So if you're confused about when the office hours are, it's under the zoom tab. I'm going to open this up for questions in a second but I'm going to go over question 15 because people are a bit confused about where we're looking. So in question 14, we have like a specific mountain in British Columbia and then question 15 asks you to put a negative sign in front of the north coordinate point and then we flip the hemisphere so we go from the north hemisphere to the south hemisphere and then question 15 is asking you what landmass is you see. You have to zoom out. I recognize that the first wording was confusing because it just drops you in the middle of the Pacific Ocean. So you zoom out, I'm going to zoom in. You zoom out until it's like right here. You zoom out until you get to see where it says camera kilometers. It goes to like, oh boy, this is, this is about it. Like, oh, 2000 kilometers. And yeah, just scroll around until you kind of see like what's down here a little bit, what's over here and then any answers, wow, where am I? This is, hi. Sorry. Okay. Just kind of stick towards, there should be four answers and this was updated in Canvas. There's four answers that are listed. Stick to like what's in the general vicinity of there. Like there's one that you, like where the camera goes to. There's one that might be close but it's much farther than the other four. This is, this question was somehow even more confusing last year and I tried to change it to be less confusing and that failed and this is that question. I just want to try to go on, I guess what I did is, I just zoomed all the way out and like, what that could do is center of there. So like, zoom all the way out to see a full flow without the LMA. Yeah, I think so. And then like if it's showing you more than four pick like the four that are closest to that point that Google takes you to. There was a question over here, I think. No? Okay. Any other questions for assignment one so far? Hello. There was a question on other stuff, Arctic Circle sign and then it asks the coordinates. Is that the coordinates of the Arctic Circle or the Arctic Circle sign? It's the Arctic Circle sign so when we go to the Arctic Circle sign. Yeah? That, Arctic. It's this one here, the Arctic Circle sign. And like, don't touch anything. So for all of these things that have like look down in the camera, so see there's like, this says camera, this has those coordinates, do not touch anything. Oh wait, sorry. Don't touch anything for the mountain when it asks for the height. I think that's question 10. But for this one, you do want to, what are you doing? Close this. You want to hit that middle part of, yeah, you want the compass to be facing north. So remember if you click the center of it, it'll just take you back the camera angle to north and then you look down here for what the coordinates are. But just like, there's a little bit of fudge points on that one because it changes a little bit. But you're looking for this, don't put it in the coordinate format where it says like blah, blah, blah, north, blah, blah, west. And that only be two numbers, like we're, oh, sorry. Decimal point numbers where it's like 60 point something. We want it to be 66 degrees sign, 30 apostrophe, 32 double apostrophe. What is that? Does that kind of make sense? You want to copy it without the mouse in there changing the subject. Because the mouse moves around the moment. Yeah, so try to just like hover above the compass. And about your hand arching points are not a point that's just the location of the switch. Yeah, for number 20 with the Antarctic one, you're not scrolling somewhere on Google Earth. It's kind of like applying what you've learned through the assignments to figure out what that would be. And it's similar to what you do in question 15 to get from the northern hemisphere to the southern hemisphere. Any other questions? Hello. Or the mountain one, like the elevation changes depending on where your cursor is on the mountain. And because like the camera rotates, elevation is constantly changing. So I just like put my cursor on like the very tip of the mountain, the peak where like I tried my best. If I'm like one or two minutes off, that's not going to be a big deal. No, yeah. There's like a standard deviation, so like plus or minus however many meters, which is factored into canvas. So if you're in like within 10 meters or something, it'll be fine. Was there another question? Hi. I have a question where we want to see the spring shot of an image. How like is there one search image that you guys are in for? Or are they like around that area? Yeah, it's around that area. It's like in the assignment, it's kind of outline like the path you should be on. And so you should be able to see like from the path across the lake to the buildings, but it's really not like that's not that exact image. It's very flexible. Canvas marks the assignment, but I go through the assignment to mark the open ended questions. And if there's one, we're like, like say you're doing the equator and you write the equator, but canvas says that's wrong because you have the twice. I go in and I mark it and say no, that's fine. So it's a lot more flexible than what canvas might look like. Hello? Is that the one where it says to use the Wikipedia? Yeah. So use the number that's listed in like the Wikipedia thing on Google Earth. The paragraph. Hello? Yeah, sorry. This is question 15, what I was doing down here. So when you're, nope, sorry, that's not it. That's not it. What? What's question seven? No, yeah, but I don't know what it is. Sorry. I'm going to move down here super quick and then, oh, do you have it? Okay. Right. So for question 15, zoom out to the number that it says on your canvas quiz and pick like the four countries or land masses that are closest to that point. Hi. I'm not an approximate. Put exact, but there are fudge points. So if you get like a couple meters off, it should be fine. Reminder, I have office hours tomorrow at three. So if you have any more questions, email me, use your discussion board. A lot of these have been covered on the discussion board already. So make sure you're checking that before coming to office hours. Cool. Thanks guys. See you guys later. Thank you. Leanna, you'll be back tomorrow. Leanna will come back tomorrow if you do have any last minute questions and then she also has office hours Thursday morning and the assignments do Thursday night. So if you have any last last minute questions, you can ask her then. Okay. So today with me, we are talking about space and orbits. So we're going to talk about defining space, talk about what space is. We're going to talk about what makes an orbit and how an orbit works. We'll then talk about different types of orbits and their applications, essentially what kind of satellites we have in different orbits. And then we'll talk a little bit about space junk, some of the issues around space junk, some of the solutions we have for space junk. Did I mention, have I told you guys already that I went over my student surveys from last term? Have I had that conversation with you guys already? So one of the most common complaint that I had or critique or whatever was that I sometimes talk too fast, which I know I do, so I'm trying to work on that. If I'm ever talking too fast or you want me to go over something again or you're just noticing that I'm going a bit fast and you'd like the pace to slow down, just put your hand up and just let me know. Sometimes I just get in the zone and talk a bit fast. If that makes sense. Cool. Okay. So first thing we're going to talk about is how do you define space and how far away is space? We know that or at least it seems to us when we're standing on the surface of the Earth, we look up to space. When it's nighttime, we just kind of see this black abyss. But there isn't this straight line that exists through the sky that defines the difference between our atmosphere, the Earth's atmosphere and space. So we're going to talk a little bit about how that's defined. Generally speaking, most satellites are at least 400 to 500 kilometers above the surface of the Earth. As we increase altitude up, we get lower and lower air pressure, which just means that there's lower and lower air density. There's less air molecules closely together as we just travel further and further away from the surface of the Earth. We can see that here. This just shows us on the right kind of a depiction and illustration of what those air particles or air molecules would look like. You can see close to the surface of the Earth here, there's lots of them, including oxygen, which we use to breathe. And then as we get further and further away from the surface of the Earth, there's slowly less and less and less air molecules. Now for international laws and treaties and governance, we kind of have to have a political definition for where space starts. And that's because generally speaking, countries have what they define as their own airspace, which they kind of have governance and control over. But beyond a certain altitude, it's considered space and it's governed by kind of very loose international agreements and laws. So what we have is something called the Carmen line. That's this line here. It's just an imaginary line. You don't actually see an orange line in space. But essentially what it is is this arbitrary boundary between the atmosphere and space that's recognized by the FAI, which is just the Federation, Aeronautique International. It's just this international organization that keeps records related to aeronautics, which just means related to space. And for governance, because they all use this Carmen line, essentially a country's airspace typically in international politics is considered up to about 100 kilometers where the Carmen line is from the surface of the Earth. The problem with that definition is that we know air molecules and thus the Earth's atmosphere extend way, way, way, way beyond the Carmen line. So we have particles all the way up here. And like I said, there's no distinct boundary or line drawn between the atmosphere and space. So in reality, the atmosphere actually extends way, way, way up from the surface beyond the Carmen line about 480 kilometers above the surface of the Earth or at 480 kilometers altitude. And in that general range of 400 to 500 kilometers-ish, that's when the, that's when essentially space kind of scientifically begins. That's when air pressure approaches a vacuum. A vacuum just means an area that's devoid of any matter. Air molecules are a type of matter. And as we get further and further away from the surface of the Earth, there's less and less and less air molecules eventually approaching what's called a vacuum or we consider space. It's called space because there's a lot of space there. There's no matter there. There's nothing actually existing in that space. So there is really no distinct boundary that we can say holds the difference between the atmosphere and space. But we roughly say that the transition between the atmosphere and space occurs at about 400 to 500 kilometers. One of the reasons this Carmen line is really useful for international and, you know, country governance and that kind of thing is because airplanes don't really ever travel above 100 kilometers altitude and pretty much all satellites that exist are well above the Carmen line. So things that we just naturally think about to be in space and things that we naturally think about to not be in space like planes are generally on either side of this line. But in reality, space doesn't really start until you're about 400 to 500 kilometers above the surface of the Earth and it's just this general transition. Now, orbits are what holds satellites. It's what holds these, you know, artificial objects that humans have created that we've launched into space that are now just traveling around the Earth. And the orbits came from a couple of concepts that you can think of to try to have wrap your head around it. So, first of all, an orbit is just a curved path around a celestial object and that path is created from gravitational attraction between the two objects. If you imagine, in this case, throwing a javelin, you throw a javelin really, really far kind of in a straight or slightly up angle, the javelin is going to travel really, really far. It's going to go straight and then eventually it's going to fall back towards Earth, right? And that's because you've thrown it, you've put power, momentum into it, so it's going to travel that direction, but eventually gravity is going to pull it back down to the surface of the Earth. The same with, if you shot a gun, but that bullet's going to travel way, way further. It has way more, you know, power or thrust associated with it than if you threw a javelin. So that bullet's going to travel, travel, travel, travel really, really far, but then eventually gravity is just going to pull it towards the Earth. An orbit is really no different. An orbit ultimately is the effect of something continuously falling towards the Earth. Now there was this famous Newton-Cannibal experiment where Newton kind of deduced that orbits were possible, and his theoretical experiment, he didn't actually do this, but he theorized that with significant thrust, enough power forward, and enough altitude or lift, so you're kind of high enough above the surface of the Earth, a cannonball in his case could maintain a circular orbit around the Earth. Which just meant that if you had enough power or thrust behind a cannonball, and it was high enough up in the air or in the atmosphere, or, you know, in the case of satellite orbits in space, if it was high enough above the surface of Earth and had enough power behind it, it could continuously fall towards the Earth in such a way that it would just keep going. Over and over and over, just keep falling towards the Earth, and that is what creates an orbit. So an orbit ultimately, which I've noted here, is just the effect of something continuously falling towards the Earth. So satellites orbiting the Earth have just been given enough thrust, enough momentum in a certain direction, and are high enough above the surface of the Earth that they can just continuously fall towards the Earth and just keep circling it. Any questions about that? About how an orbit works? Yeah? So over the long long term it always does fall above, correct? It always comes back. Not if you get it at just the right spot. There are, and I'll talk about that in a moment briefly, there are, you know, when satellites are actually in orbit, they will sometimes deviate from their orbital path, which needs to be corrected sometimes. Generally speaking, theoretically, if you put the right-sized object at the right direction with the right amount of thrust and the right amount of altitude, you could theoretically get it just right to where it's always going to be orbiting, and it's always kind of infinitely falling towards Earth. Yeah, yeah. Have they tried to send a cannonball into orbit? Not, no, not literally. I mean, they've reproduced it in the sense that satellites orbit Earth, right? So that's essentially the same thing that Newton was theorizing, but now we just have these kind of spacecraft in the case of satellite orbit. Earth that are doing just this, they're continuously falling towards Earth. We should reproduce it. Yeah, we should set a cannonball in space. I can be out for that. Yeah. Are there all satellites in these free spots that are always falling, or are they just like deviate a little bit, and sometimes it's a thousand-year uniform? Sorry, I said it again. Are all satellites in these free spots that they're always falling down? Yeah. Or some of them are like mostly somewhere deviated, or something. A hundred years you're going to fall down. Yeah, most of them have a slight deviation, but are always, generally in the case of satellites, they're always corrected. So there's never deviation in the sense that they're going to deviate and actually fall towards Earth, which kind of ruins the question that I was hoping to ask you guys. But in general, in reality, there's other forces that could act upon them, that could potentially force them to deviate their orbit. But the theoretical sense of it is still that if you are high enough above the surface of the Earth with enough power moving forward at just the right direction, you could theoretically infinitely orbit the planet. Yeah. If most satellites are in the same number of kilometers above the Earth, then wouldn't they crash into each other? Or is the Earth just so big that satellites are so small and comparison-scale that it doesn't really happen? We're going to get there in lecture. We'll talk about that. But generally speaking, satellites can orbit anywhere from about 400 kilometers above the Earth to 35,000 kilometers above the Earth. So there's a wide range. 400 to 500 is just kind of the minimum. Yeah. Yeah. Okay. So there's four kind of orbits that we talk about in this course. Low Earth orbits, close elliptical orbits, far elliptical orbits, and geostationary orbits. This is the classic, you'll get asked about these on the midterm. Many, not many, but students every year, Google this answer, and Google will tell you a different way to categorize orbits. So if you Google the answer to this question on the midterm, you're probably going to get it wrong. You need to make sure that you're using the classifications that we use in class. So the first is the low Earth orbit. And each of these orbits we just defined, essentially based off of how far above the surface of the Earth they are, so just what their altitude is. So the first is the low Earth orbit. The low Earth orbit travels at a speed of about, in the case of the International Space Station, about 26,000, 27,000 kilometers per hour. So that's really, really fast. They are about 400, the International Space Station is about 400 kilometers above the surface of the Earth. And in general, its applications are Earth observation, which just means that there are sensors, cameras, and actually video cameras, which we'll talk about later in the course, that are onboard the International Space Station, that are used to collect imagery and videos of the surface of the Earth. It's also used for human space flights, so we know that the International Space Station is home to astronauts for certain periods of time. What's nice about the low Earth orbit for something like the International Space Station is it is the lowest orbit, which just means that satellites that are in low Earth orbit are the closest to the surface of the Earth of satellites that we have. So for something like the International Space Station, that's really nice if we're trying to send astronauts up to the space station. If they were way, way, way further up, if I had an International Space Station that was 800 kilometers above the surface of the Earth instead of 400, that would just be twice as far for me to travel to get to it, which is just a lot of fuel and a lot of money. So the International Space Station is essentially the lowest orbiting, or one of the lowest orbiting satellites that we have. The International Space Station is also pretty much the only satellite that we talk about that's relevant to this course in low Earth orbit. And so it's used for, like I said, Earth observation, human space flight, as well as microgravity experiments. So there's a lot of experiments that go on on board the International Space Station to test what things like cell growth and chemical reactions and stuff like that would look like in a setting where there's little to no gravity. Now, although the International Space Station and in general, you know, satellites that are in low Earth orbit, they're close to the Earth because that's valuable for something like human space flight. But that also makes satellites in low Earth orbit the least stable. So the low Earth orbit is actually the least stable type of orbit. It's very susceptible to atmospheric drag more than any other kind of orbit, which means that it is subsequently subject to orbital decay. Now that just means that the International Space Station is kind of right at that transition boundary of the atmosphere and space. And that definition is just based off of where air molecules get so sparse that it's transitioning from the atmosphere into a vacuum. But there are still air molecules and particles that are there, which means that the International Space Station is still occasionally hitting some air molecules and some air particles that are suspended in that part of the atmosphere or in that part of the transition between the atmosphere to space. And that's what creates this thing called atmospheric drag. Essentially, the International Space Station is traveling so, so, so fast that even though these air molecules are very, very sparse and very, very small, the friction associated with them can be quite impactful and essentially the friction between air molecules that are suspended in that part of the upper atmosphere where space is kind of starting to begin, force the International Space Station to kind of lose momentum, lose power because of that friction. And that results in orbital decay, which just means that the International Space Station kind of gets pulled down to Earth a little bit. Now, again, I've kind of already given this question away a little bit, but if the International Space Station is in this really unstable orbit, right, an orbit that kind of is very, is well known for pulling things back down towards Earth, and we know that in general an orbit is just defined as continuously falling towards the surface of the Earth. And I wanted to ask you guys the question, will the International Space Station eventually be forced to Earth? I feel like I've kind of answered that question already maybe. So, extra question, if, you know, the International Space Station, as I've kind of alluded to, is never actually going to fall down to Earth. Why would that be? What would allow it to stay in orbit despite it being in this really unstable orbit that could potentially drag it down to Earth a little bit? Hold your question. I'm going to get you guys to discuss. So brainstorm a little bit. With someone sitting next to you, take a couple minutes. I'll give you three, four, five minutes, something like that. Brainstorm it for a moment, and then we'll come back and talk about it. I guess that's just fascinating. Okay, I got you guys attention again. So, what do you guys think? Anyone have any ideas? So, first of all, can someone just tell me, is the International Space Station eventually going to hit the surface of the Earth? You think so? Right, okay. And so, how do you think, so we know it's not going to fall to Earth, essentially. So, how would it stay in orbit? Yeah. So, essentially, if you look at, this is just a graph of the altitude of the International Space Station through time. And all of these little quick increases here are essentially orbital corrections. That's a little thruster going off on the International Space Station because the astronauts on board, or Houston, has said, we've deviated a little bit off the orbital path that we want to be on. Let's just correct ourselves a little bit, thrust a little bit away from the surface of the Earth, and that'll correct us. Okay, and you can kind of see, right, you can see the orbital decay because of the atmospheric drag going on. That's kind of this slow decrease, each of these like slow kind of sections where it's pulling the International Space Station back towards the surface of the Earth. What about these big drops here? So, there's one kind of big quick drop here, and here and here. What do you think those? That's not atmospheric drag. It doesn't work that quickly. So, what do you think that could be? Yeah. Is that the space station actually doing the reverse and moving downwards to avoid something? That's a really good guess. Why? And that is what it's doing. It's moving closer to the surface of the Earth. Why would it want to do that? Yeah. It has to like dock with the ship that brings in the new round of astronauts. Yeah, exactly. Yeah. So, there's astronauts that are flying occasionally from Earth up to the space station, and it is way cheaper and quicker for them if the International Space Station just says, okay, let's just like, we're in orbit. We're not actually using much fuel thrusting us forward. We've already gained the momentum that we need. We're just going around. If we move ourselves a little bit closer to the surface of the Earth, then the spacecraft that's bringing up the astronauts doesn't have to travel as far, doesn't have to use as much fuel. So, yeah, that's exactly what's going on there. So, from about, you know, 400 or so 500 kilometers above the surface of the Earth, from the International Space Station, this is kind of what Earth looks like. So, this International Space Station, like I said, in this case is traveling at about 26,000, 27,000 kilometers per hour. So, it's going really, really, really fast. And can someone tell me just kind of a pop quiz from one of our past lectures? Based off this little map that you can see up here, in the northern hemisphere is it summer or winter right now? In the northern hemisphere is it summer or winter? I guess that kind of gives it away because it has the actual date right there. So, didn't think that one fully through. But, the idea is what I was trying to get you to look at, and what I'll just point out myself is, you can see here, this depiction here is showing you whether the International Space Station is traveling through darkness or traveling through sunlight or traveling when the Earth is pointed towards the sun. And what you can see here is that Antarctica, kind of close to the south pole, is in sunlight, has direct sunlight for what it appears like most of the day, maybe almost 24 hours, whereas it looks like there's portions up here at very, very northern latitudes where the sun might not be reaching at all at any point during the day. So you know based on that that the southern part of the hemisphere must be tilted right now towards the sun. If that makes sense. Okay, so next we have, that was the low Earth orbit. The only satellite that we talk about that's in low Earth orbit is really the International Space Station. The next time of orbit that we talk about is the close elliptical orbit. So the close elliptical orbit starts at an altitude just above the low Earth orbit at about 600 kilometers all the way up to about 2000 kilometers. And depending on the satellite, it'll be traveling at a slightly different speed. Generally speaking because this is a much more wider range. The further you are away from the surface of the Earth, if you're a satellite and you're orbiting the Earth, the further away you are, the slower that you're generally going. So the fastest satellites are typically the ones closest to Earth. The slower ones are typically the ones further from Earth. This one is an exception. This one's, you know, I put up here 27,000 kilometers per hour. That's about the speed that the Landsat satellite travels at which we'll talk about a lot throughout this course. So that's kind of an exception to the rule. But generally speaking, whether it's the close elliptical orbit or a different kind of orbit, the further you are from the surface of the Earth, the slower you're going to be moving. So the close elliptical orbit is about 600 to 2000 kilometers altitude and its applications, the main applications of satellites in that orbit are really just Earth observation satellites. Satellites like Landsat, MODIS, which we'll talk about in depth throughout this course. But for midterm purposes, you can just say, if I ask you what are the applications of a close elliptical orbit, you can just say Earth observation. Okay, next, yeah. What I just said, if I were to ask you on the midterm exam, what are the applications of satellites in close elliptical orbit? You would just say Earth observation. Yeah. Okay, next we have the far elliptical orbit. So the far elliptical orbit travels mostly at about 14,000 kilometers per hour. You can see that's substantially slower than satellites in the close elliptical orbit. It's at an altitude of about 20,000 kilometers above the surface of the Earth, so much, much higher than satellites in the close elliptical orbit. And really the only satellites that we talk about that are in this orbit are satellites that are part of global navigation satellite systems or GPS. That's what we'll be talking about tomorrow. But that's essentially the satellites that are able to determine your position. These are the satellites that when you pop out Google Maps and you see where your location is on Google Maps, it's these satellites telling you exactly where you are. Lastly, we have geostationary orbits. Geostationary orbits travel at the slowest speed at about 10,000 kilometers per hour. Their altitude is generally around 35,000 kilometers per, sorry, 35,000 kilometers above the surface of the Earth. They have a couple of different applications. The main application we'll talk about of geostationary orbits in this course is weather satellites. What's really unique about the geostationary orbit is satellites and geostationary orbit are fixed above one position or one side of the Earth, which means that they are essentially always looking at one single portion of the Earth. They always have the exact same target. They're always looking at the exact same view of the surface of the Earth. You can see that here. The satellites here are just orbiting such that as the Earth rotates, they're just orbiting right with it just so that they're always looking at the exact same point of the surface of the Earth. That makes him really useful for understanding weather or monitoring weather because we often want to know what weather patterns are at a very, very fine temporal scale, which just means we want to know what weather is doing all the time. We want to know what it's going to be like this afternoon, what it's going to be like right now, etc. These geostationary orbits are really, really good for that because they're always looking at the same spot on the Earth. For example, we have the GOES satellite. That's an example of a weather satellite. It is a geostationary satellite. It has positions all around the surface of the Earth, but for North America, there's a couple of satellites that just look right at North America, right at Canada and the States, and can give us year-round around-the-clock information on what weather is doing at that point. Or if that question is doing, you need to be able to know that is detailed or is it mainly just like what's on the slide? Just what's on the slide. Yeah. In terms of knowing what, for the midterm, for the final exam, what you need to know about each of these orbits, just roughly how we classify them, which is their altitude, what kind of altitude they're at, and then just each of their applications. So for geostationary, we talk about weather satellites, for far elliptical, just GNSS satellites, for close elliptical Earth observation satellites, and for low Earth orbits, saying either Earth observation, human space flight, microgravity experiments. Those would just be the applications. Yeah. And is the reason you can't see the poles because you wouldn't be able to orbit with the Earth? Yeah, you can't. You couldn't have a geostationary orbit that is looking at the poles. You are able to see poles with certain satellites, mostly with close elliptical satellites that Earth observation satellites are in. You can see here in this little graphic, this little video, these satellites are actually going over the poles. So those are the ones you would use to be imaging the poles, but the geostationary satellites are typically quite limited to mid latitude areas. They can often still view pretty far up either north or south of the equator because they're so far away, they have such a wide angle of view, but generally they're pretty much situated right above the equator. Yeah. Okay. Any questions about orbits, about each of the orbits we just talked about, their applications, how we classify them, anything like that? Sweet. Okay. So we know that we have all these satellites, they're in all these different kinds of orbits. This is kind of going back to a question that I had earlier. Now this is kind of a scale depiction of if you just blew up the size of each of those satellites relative to, you know, Earth, and you were trying to visualize and look at all of the satellites that we are tracking that are currently orbiting the Earth in some way. Shape or form, this is kind of what it would look like. So you can see here there is a ton of stuff, a ton of artificial objects, satellites in this case, that are orbiting the Earth. You can zoom in, you can see there's lots and lots and lots and lots. Obviously, you know, you can think of this would be a bit of a problem. And that's where this idea of space junk comes in. So we've had these growth of satellite programs over the years, kind of from the 60s when the space age started. We've had all these different countries and companies launching satellites into orbit. And we've never really had a particularly well regulated way to control who's launching what into where, because space is space. We don't really have, you know, agreements or laws to govern what can be done in space. And so through time, and you can see that here going from the 60s all the way up to 2009, we've slowly accumulated more and more and more tracked artificial objects that are orbiting the Earth that we've launched. And what's really interesting and pretty crazy now is that more than 95% of these tracked objects, which just means that objects that we've launched into space, that we are still tracking so we know where they are, a lot of them are just debris, which just means that they're not actually useful to us. They're not taking images of the surface of the Earth. They're not actually doing anything for Earth observation remote sensing. They're literally just junk. And that has its risks associated with it. So it's difficult to launch new satellites because you put all this money into a satellite. And now there's the potential that if you launch this satellite, it could get hit by old ones and by other junk. You might be, you know, wasting all this time and money to send the satellite into space just for it to be hit by some piece of junk and maybe malfunction. It's also now more dangerous and difficult for space exploration by people. There's this higher risk of potentially getting hit by some sort of space debris if you're in a spacecraft or if you're on the International Space Station. So scientists have predicted that close encounters between satellites and debris are going to rise by about 50% in 10 years and by about 250% by 2059. And that's kind of the foundation of this thing called the Kessler Effect. And the Kessler Effect is just about exponential growth. It's the idea that as we increase space junk that's orbiting Earth, then there's going to be more collisions. Because there's going to be more collisions, more smaller pieces of space junk are going to break off and now be going all kinds of directions. And now those have potential to collide with something else. And it's this exponential growth of problems, of collisions that are going to be associated with space junk. Now, the most current research hasn't suggested that we have currently right now reached the point of that happening where we've reached the point of the start of exponential growth, this Kessler Effect of space debris. But there are studies to suggest that we're potentially coming upon that within the next 20 to 30 years and that this next decade has a lot of influence over what the next couple of decades are going to look like in terms of space junk and safety for astronauts and that kind of thing. So this is just an example of an image from an astronaut on the International Space Station. They kind of had a chip to windshield here that was from something I believe about the size of a softball that hit their window, that hit their window. I mean, if I were an astronaut on the International Space Station and something came, hit my window and this, you know, I saw this little chip, I mean, I wouldn't go to space in the first place because I'm not brave enough, frankly. But, you know, if I was up there and I saw this little chip created by something that was flying through space and hitting the side of my window, I'm pretty scared. So there's this sense of increased danger for astronauts associated with space junk. This is an example from the Hubble Telescope, which is a really famous telescope that's actually orbiting the Earth and it's orbiting the Earth so that it can take images from space of things deeper into space, so not of Earth, but because it's orbiting in space and surrounded by essentially darkness. It's much easier for it to take images of things further away that are much more sensitive to the light because it's just surrounded by darkness as opposed to being kind of polluted by the light that we have here on Earth. That's kind of a side. It doesn't really matter. The point is it's this really expensive, well-known telescope that's gotten us lots of really, really valuable imagery for exploring space. And all of these are little individual dents that have occurred from space junk hitting it. So there's tens and tens, dozens and dozens and dozens. This is just a little section of the Hubble telescope in reality. It's huge. So you can imagine there's probably hundreds of little dents and impacts that have been created between space junk and just this one telescope. The Sentinel-1 satellite is a pretty new satellite. It's only been orbiting the Earth for about 10 years. You can see here it's already accrued some damage from running into some space debris. Okay, so I kind of wanted to. That was a little bit doomy and gloomy. We got all this junk that's in space. What can we actually do about it? There's three kind of key things that we can actually do about it. Two of them will talk in detail. One is more of a kind of a side. So what can we do? We can create debris reduction programs. We can actively go out and launch satellites or launch some sort of spacecraft to go out and try to remove space junk. We can also try to develop technologies for new coming satellites so that when they get launched into space they can kind of remove themselves from being space junk. And generally all of these things kind of a prerequisite maybe to being able to fund and have support for those first two things is being able to implement stronger international agreements. Like I said, the governance of space is very, very, very fuzzy. So it's stronger international agreements and support from governments. There'd be this sense hopefully that we'd have more funding, more support to go out and try and reduce the space junk problem. But the two that I'm going to talk about in general and the two that I would want you to use as an answer if I were going to ask you on a midterm or final exam, hint hint, wink, wink, nudge, nudge, are either debris reduction programs or developing new technologies to avoid oncoming space debris. Now, debris reduction programs are typically involving sending satellites into space that might use a harpoon or a net capture to actually get rid of those kinds of space junk or a new satellite that can be launched. And then maybe once its lifespan has been reached, then it kind of deploys a big boom that creates a bunch of atmospheric drag and pulls it back to the atmosphere where it burns up and kind of removes itself from space junk. So I'm going to play a video that's about three or four minutes long that kind of has some graphics depicting how these debris reduction programs might look or how these new satellites might look that would have a way to remove themselves from space junk. And then I'll talk again in detail about kind of what's going on and how it's working. I'm going to talk about it, you'll see what I want you to know. In a while, I'm reminded on web art phones, communications, such as these net and electronic monitoring systems that I'd like to know if crucial to our modern way of life. Perhaps unsurprisingly, we'd ever remind the services for a more satellite that's been popping up in launch around the area. But this is a technical problem, there's now some junk in space. This space junk comes in all shapes and sizes, and in the two years' black of paint, bright, rigid, dead satellites that no longer work. Even sections of all broadcast the past video space junk problem. Some of these are estimated that there's already over 70,000 tons of junk in space, and it's increasing. The majority of useful satellites today launch into NOAA at orbit, which is ending up to around 2,000 kilometers above the area. Here, there is the greatest waste of collision. The insorality. Well, in reality, a huge pilot like that is unlikely, however, satellite collisions have a burden space, such as the original commercial free collision in 2009. Scientists are now exploring the best ways to minimize and remove space junk in order to combat the problem. There are two methods. Be sure future satellites are able to get rid of themselves so they don't contribute to the population of junk. There are two actively long permissions to what they do with a capture space junk. There are a room to bring mission with a whilst first mission to demonstrate capturing technologies that could deal with space junk. The first experiment is net capture. A small cube-set will be ejected, which happens on a fishing drone. This will be fine in place of construction. The inflatable structure helps the cubes out to deal with quicker. Getting the net and the platform correctly alive with the junk with a capture system doesn't miss is a big challenge. In full mission, the net will have a tether life to pull the junk back down to Earth. The second experiment is half moon capture. Here, at a point of time it is used to demonstrate the use of a half moon, the capture space junk. The third experiment is a fishing-based navigation and will also use an ejector cube-set. But simply, in order to satellite to run in Earth's face, camera and light-art technology is needed for movement. Light-art is a measurement technology that uses a laser to eliminate the cube-set and read back information such as position and orientation. The final experiment is the drag-sale. In order to satellite the disfoset of themselves, future missions may have such drag-sales attached. By deploying the drag-sale, using an inflatable booth, drag the time to satellite and hit fast-spoon the outer elements of the Earth that was shared. This added drag will turn the satellite to Earth faster, where it will burn up in the atmosphere. Here, we show the burn-up for the main platform. The extreme temperature to your imagery will cause the platform to maintain the burn-up. If we fail to clean up our space environment, more collisions are going to keep occurring. Intentionally, when the whole satellite is placed unusable, we'll damage the virtual silences that we use on any basis. The paid-out for space jump is proven to ensure the satellite's use space with future generations to enjoy. Okay, quick. One quick clarification just so you don't get confused. In that video, they classify low-Earth orbits as anything up to 2,000 km altitude. In this class, we do low-Earth orbits at about 400 km altitude and then close elliptical from 600 km up to about 2,000 km altitude. Okay, but just a quick kind of review or extra explanation about how these methods or systems would work. So the first is debris reduction programs, which means that you launch a satellite out or some sort of spacecraft that wouldn't have a person in it. It would just go by itself. You would actively launch this satellite and it would be able to harpoon, send out a harpoon, and grab some junk and pierce it and then pull it back into itself. So if I was this satellite that was sent out to get all the space junk, I'd be going through space. I would go, booo, booo, booo, booo. I would send out all these harpoons and I'd punch a bunch of stuff and then I'd reel it back towards me. And then once I have a bunch of space junk on me, once I've kind of reached my max capacity, then I would just increase the thruster on me to send myself back towards the atmosphere. And then because of all of the air molecules in the atmosphere, when I travel back towards Earth and hit the atmosphere, I'm traveling at such a such a fast speed that the friction between all the air molecules and myself will cause me to just burn up. So I just burn up upon re-entering the atmosphere. All of these methods in one way or another involve pushing the space junk back towards the atmosphere so that it's burned up as it hits the atmosphere. Just a quick note. The other method is... Would I say the second one first? Oh, yeah, sorry. So the other method is, as opposed to, you know, going out, if I'm finding that satellite, harpooning a bunch of things and then pulling it into me and then sending myself back down towards the atmosphere, I might also send out a harpoon that hits the satellite or the space junk, but that has a little thruster on it. So it just kind of individually sends that piece of space junk back down to the atmosphere where it burns up when it hits the atmosphere. Yeah. Is that a theory of the burning of the junk and that atmosphere? Yeah, great question. So up until recent years, the general consensus was no that, you know, it's so... The size of the atmosphere relative to the amount of space junk that we have or the amount of satellites that we have that would be re-entering the atmosphere and burning up that it probably wouldn't pose a problem. But there's been new research that's come out in the past couple of years that has suggested that it could be a problem. One of those being associated with the fact that most of these satellites or pieces of space junk have high amounts of aluminum in them and that if they come down, if we have tons and tons and tons of satellite debris that's coming towards the atmosphere and burning up when it hits the atmosphere, that there might actually be this increased level of non-natural or human-caused aluminum in our upper atmosphere and that could potentially cause problems with our ozone and things like that. Like I said, the consensus was for many, many, many years that it wasn't a problem, but because of the proposed ideas from SpaceX and other companies to have these massive, massive mega constellations of satellites, where they're sending up thousands and thousands and thousands of satellites all the time, just a ton of them and replacing them really quickly, just constantly sending out more and more satellites, the idea is that or the thought is that because now we're going to potentially have so many satellites that are going to be sent into orbit, that there's going to be so many much more that are going to be coming and hitting the atmosphere and burning up, that it could potentially have an impact. And to be honest, the impact based off the research that's out there right now isn't particularly clear. There hasn't really been enough research or experiments to know exactly what the byproducts of those chemical reactions could be in the atmosphere. So it's kind of unknown, but there's a lot of research that suggests, you know, or questions, should we kind of allow this and be okay with this, given we don't really know what could happen if we have potentially all this extra stuff in our atmosphere? Do you have a question? Was there a question here? Okay. So the two kind of examples here are both harpoon based examples, one where you harpoon out to a bunch of space junk, pull it in and then go down to the atmosphere with all that space junk, or you send a harpoon out that leaves the kind of, you know, main spacecraft that's out there to remove the junk, and it hits some space junk, has a little thruster on it, and then it just individually thrust that space junk down to the atmosphere. The other example is a net and capture system where the satellite goes out to remove the space junk, it identifies some bigger pieces of space junk, it sends this massive net essentially to encapsulate the space junk, and then has this tether, which is essentially just this long rope that kind of looks like the fin of a tadpole or something that just kind of is leading off the back of the net, and essentially some crazy physics go on that I do not understand that occur in the upper atmosphere where that tether on the back of the net is able to create enough orbital decay to pull that satellite back towards the atmosphere, and again, it burns up when it hits the atmosphere. The one other example, which is not about sending out a satellite to actively remove junk, but to design new satellites that are launched to be able to remove themselves from space is that they're just able to go out, and once they've reached their lifespan, deploy this big boom, and that boom creates enough atmospheric drag, creates enough friction with the air molecules that are out there that it can kind of drag that satellite back towards Earth, hit the atmosphere, burns up when it hits the atmosphere. Any questions about any of those methods? You can definitely expect a midterm question on it. Yeah? Yeah, exactly. So I would specify the question probably along the lines of, give me one example of how we can actively remove space junk. Give me one example of how we can design new satellites to reduce space junk so that they don't become space junk. Does that make sense? Any other questions? Sweet. Okay, so I often got this question the past couple times I was teaching this course. Why do we not spend, send all our garbage to space? There is, for example, this literally massive container of garbage that was dropped or dumped from the International Space Station. It's got 2.9 tons of used nickel and hydrogen batteries, and it's currently orbiting the Earth at about 4.8 miles per second. Eventually it's going to get pulled into Earth's atmosphere where it's going to, you know, quote unquote, safely burn up in the atmosphere with no byproducts or other effects. Supposedly. Anyways, the point is that it's going to come down, it's going to hit the atmosphere, it's going to burn up. So why wouldn't we just do that? We know we have a bit of a garbage problem on Earth on the planet. Why wouldn't we just send all of our garbage into space and then allow it to fall back into the atmosphere as it reenters the atmosphere it would burn up? It would get rid of a bunch of garbage for us. So the problem with that is that the world makes about 2.6 trillion pounds of garbage per year. That would take about 168 million rocket launches to get all that garbage into space just for one year, which would cost about 33 quadrillion US dollars for one year, which that number literally looks like this. I've never seen a number that big before until I looked this up, to be honest, but this is about 1500 times the annual GDP of the states. So beyond it just being, you know, a problem because of all the fuel you would burn and all the greenhouse gases that you admit is also just ridiculously ridiculously expensive. So, unfortunately, we can't send all of our garbage to space. Okay, that is pretty much all I have for today. I'm going to give you guys about 5 minutes to try and brainstorm the answers to these questions. Where did the space begin? What is the altitude of each orbit type? What are the common applications of each orbit type? Why is space chunk a problem? How can we fix it? If you want to stay and go over the answers, you are welcome. If you don't, you want to head out. Please do so swiftly and I will see you tomorrow. Okay. Nice. You guys are my favorites. Stick around. Talk with me more. Nice. Okay, cool. We can go over these. I'll get you out of here quick. So where does space begin? It's kind of a trick question. We talked about some different definitions. So maybe we can kind of mention all of those. But if I were to just ask you where does space begin, what would you say? Pardon me? The carbon line. The carbon line? Yeah, so that's the political boundary that we kind of have established for where space begins. What's kind of our more scientific sense of where space begins? Yeah. 480 to 500 kilometers. Yeah. Somewhere in the range of 400 to 500 kilometers. And why is it about there? You can answer someone else can answer if you want. Because that's when the air particles kind of fade out. Yeah. So it's like a vacuum at that point. Exactly. That's where about 400 to 500 kilometers altitude. That's where air molecules get so sparse that it essentially is approaching a vacuum. There's so little air molecules. There's so little matter that we're starting to get into what's considered a vacuum, which is space. Okay. What is the altitude of each orbit? What is the altitude of low earth orbit? Yeah. 400 kilometers. What about close elliptical orbit? Yeah. 600 to 2000 kilometers. And what about far elliptical orbit? Sure. Yeah. Do it again. 20,000, what's the day each of the orbit? Beautiful. We're on a roll. You can kind of rough those. You don't need to give me the exacts. Each individual satellite in that orbit will kind of vary a bit. I just picked one specific satellite and threw it up. But in that general range of 21,000 kilometers or so. And then what about geostationary orbits? Yeah. About 35,000 kilometers altitude. Sweet. What are the common applications of each orbit? So what kind of satellites and what maybe are they used for are in the low earth orbit? Yeah. First observation, human space flight and microgravity. Exactly. So earth observation, human space flight, and microgravity experiments are the common applications of low earth orbit. What about close elliptical orbit? Oh yeah. Keep it going. Earth observation? Yeah. Far elliptical orbit. Sure. Why not? Yeah. Genesets. Global navigation satellite systems or positioning satellite systems. Those are the satellites that tell us where we're located on the surface of the earth. We'll talk about those in detail tomorrow. And then lastly, geostationary satellites, common application of those. Weather satellites. Exactly. Okay. Last question. We can answer in two parts. Why is space junk a problem? What's the issue with space junk? Why are we concerned about it? Yeah. Big part to launch new satellites can also improve for more space. Yeah. They might get hit by older. Exactly. Slate. Bit more expansion on the first point. Why is it difficult to launch new satellites? Yeah. Expensive. It's expensive, yeah. It's expensive. And what's the kind of... Can you elaborate a little bit more? Okay. Same. Same. Yeah. Yeah. Yeah. Exactly. So they're expensive. They're valuable. They're hard to launch. If we're launching them, there's this increased risk now of collision with space junk that's out there. Okay. So how can we fix it? What are the ways that we can fix space junk? Yeah. Send out the things that goes out in the park booms all the space junk and then breaks it into the atmosphere to burn up. Or use a net to grab it all and then put it into the atmosphere to burn up. Yeah. So there's new satellites that we're launching put a sale on it so they can eventually slow down and then burn up. Yeah. Exactly. So we got... Yeah. Also stronger international agreements. Yeah. Exactly. The stronger international agreements is kind of more around what we can do to support or get more support for those methods that are actually physically going out and removing space junk. So if I were to ask you on a midterm or final setting, what are the solutions to actively go out and remove space junk, then it would be either the harpoon or the net and catapult. If I were to ask you what are the ways that we can design future satellites to not create so much space junk, that would be designing new satellites that have a boom and inflatable boom that inflates when the satellite has reached its maximum lifespan. It's not like it drags back into earth. I'll just add the harpoon and the net and catapult. The net is kind of pretty simple to understand just that it's a net that's shot out from this satellite. It covers the space junk and then there's this tether. Essentially the net itself creates the drag that pulls that space junk back down to the atmosphere. The harpoon, there's kind of two specific harpoon methods. One of them being you kind of harpoon out and pull all the space junk back into that original spacecraft or satellite and then it sends itself back towards the atmosphere or you send out a harpoon that completely detaches from where it's sent from and it just has a little thruster that's on it, that when it hits the space junk, it then just thrusts that individual piece of space junk back towards the atmosphere. Does that make sense? Makes sense? Yeah? Sorry, study again? The inflatable thing? So yeah, so for new satellites, if I were to ask you how you could design new satellites to limit or reduce space junk, new satellites would be designed so that they have this inflatable boom. So it's this sail essentially that inflates when the satellite has reached its lifespan and then that creates drag, creates friction with air particles that just pulls it back down to the atmosphere where it burns up. Yeah, that makes sense? See? Any other questions at all? See? Alright, thanks guys, see you tomorrow. How's the week on? It's going good. How are we doing? Pretty good. Not too shabby. How's the week on? Oh, it's good? Yeah. Yeah, yeah. Everything's going smooth. It's in here actually. I've been used to this lecture all. I felt really big on the first day. Now it feels pretty normal. Oh, time to get. Okay, let me start here. Never use your mic. Oh, right. Hi, guys. I'm going to get started here. I just wanted to introduce you to Tristan. He's one of your TAs for your next assignment and the assignment after that. He's going to say hello. He'll be out of here. Yeah. Just a quick hello. My name is Tristan. I'm your TA for assignment two and three. I am a PhD student in the integrated remote sensing studio in the department of forestry. So assignment two is going to be going up this week before Thursday along with my office hours. So. Yeah. Awesome. Thanks, Tristan. Thanks a lot. All right. So. Oh, Leanna wanted me to go over something, which was just that the assignment submissions. So the way you answer all your questions for the assignment is through a canvas quiz. And she said there'd been some confusion about that. So that just means that, you know, the submission page for the assignment for assignment one in this case is just a canvas quiz. There's no timer on it. It'll just close when the assignment is due, which is at midnight on Thursday. But the quiz is how you actually submit your answers to the question. So you can open the quiz. You can work on answering the questions as you work on them. The quiz will save your answers. You should just be able to close it and then reopen it and it should save all your answers. And saying that, don't trust canvas if you are just hoping that canvas will save your answers if you're trying to save your work and then keep working on it another time. I would just back up your answers somewhere. Just have a word document or something where you can just put the answers to your questions as well. Just in case if you reopen canvas, it doesn't save your work. But it should. Yeah. Yeah. Yeah. So the policy, I believe, is 20% late deduction per day. So it's open for five days after it's due because five days after it's due, that'd be minus 100% so you would get zero anyways. So we accept them, but they just get docked late marks. Any questions about that? And how you submit answers to your questions for the assignments. Okay. Sweet. So today we are talking about satellite navigation systems and global satellite navigation systems. We're going to be talking about how they work, what information you need from space, from global navigation satellites in order to figure out where your location is on the earth. We'll talk about how they actually go away and do this. And then we'll talk about some different kinds of GNSS, the difference between differential and kinematic GNSS. And I'll just end on some applications of GNSS. So I always start this lecture by just asking you if you have used GPS today, if you Google Maps something or you looked at your location on Google Maps, then sure enough you've used GPS already some point today. We know we have these car GPS's that are sometimes built into our cars. We have these really fancy ones that scientists will take out into the field to get really accurate measurements of where they are or of points that maybe they're surveying. And we also know that maybe if you're on a plane, planes are using GPS or GNSS to navigate where they are through three dimensional space. And as we talked about in our history of positioning lecture, it's not new for humans to be looking at things in the sky in order to figure out where we are located on the surface of the earth. The only new thing since the early 60s, late 50s, is that instead of looking to natural objects that are way distant in the sky like stars, like the north star for example, instead we just look to these artificial objects, these satellites that we've launched into the sky and are in orbit around the earth. Navigation satellites or navigation satellite systems are like orbiting landmarks. The landmark or the position of that landmark is known, in this case a satellite. If we know exactly where the position of that satellite is, then you can determine what your position is using at least four satellites. And I'll talk about in detail how that's actually done. I just want to make sure I'm recording here. I'm paranoid now. I somehow accidentally turned it off one of my other classes. So history of, yeah, question? What does it mean by if the orbit is on a public satellite, we're doing the location thing themselves. So that'll become clear throughout this lecture. I'm going to talk about that in detail. So maybe just hold off on that for now. That's OK. OK. So history of GPS, the global positioning system, GPS is just an American global navigation satellite system. So GPS is called, stands for global positioning system. It's the American, the United States developed version of GNSS. So GNSS global navigation satellite system refers to all the satellite systems that exist. GPS is a specific GNSS that was developed by the American. So GPS was developed by the US military and about started getting developed around the 1970s, became operational with 24 satellites in orbit in 1995. And then many people started using it from 1996 onwards and became a civilian asset, which just means that pedestrians, people started using it on a relatively common basis from 2000 on. There's three segments to how a global navigation satellite system works. And I just will quickly go over what I stated on the last slide because you can definitely expect a midterm question about it. I will want you to know what the difference is between GPS and GNSS. GPS, again, just the American version of a GNSS. There's many global navigation satellite systems out there. I'll mention what the other ones are in a moment. GPS is happens to be the one that's most familiar to us because it was developed in America. So any GNSS system essentially works the same way. There's three segments to it. The space segment, the control segment, and the user segment. Satellites are communicating with control stations. So with the control segment, with a two-way signal, which just means that satellites are sending information down to the control segment, and those control segments are sending information and commands back to the satellite. So there's a two-way signal going on there. The user segment, which is just you, that's you holding your phone or your GPS device, whatever it might be your receiver, it's just receiving information from the satellites. So you are never with your phone or whatever GPS device you're using. You're never sending information to the satellite. The satellite is simply sending information to you. Now like I mentioned, GPS is the American GNSS system. There's always a minimum of 24 satellites across six different orbits, and from any point on Earth, at least four satellites are typically visible for the receiver. So like I mentioned, GPS is the American GNSS. There's also the Russian GNSS, GLONASS, the Chinese, Badu, the American GPS, and the European Galileo system. So these are all different global navigation satellite systems. So when you hear oftentimes GPS and satellite systems being used interchangeably, they're not exactly the same thing. Each of these, the GLONASS, GPS, Badu, and Galileo, are a different GNSS system. This graphic is a little bit outdated. I just put it up there to kind of give you a sense that there are different ones that exist. There are much more than 24 GPS satellites in orbit now. There's probably about 32, 34 that are operating as we speak. Generally speaking, most GPS devices, most receivers, whether that's your phone or some other fancy GPS device you have, is able to use not just GPS satellites, but also satellites from GLONASS, from Galileo, from Badu. So that gives you a wide variety of satellites that you could potentially get a signal from in order to determine your position. GPS and all the other GNSS systems are always launching new satellites into space. They all have finite life spans. So as ones get old and stop working, we launch new ones into space, which again, starts contributing to that space junk issue we talked about yesterday. These GPS3 and GPS3F satellites, these are the newest ones. As they become newer, they just have longer life spans. They get more accurate, et cetera, et cetera. So that's an example of the space segment. The space segment are the actual satellites that are in space. We know that they are all in far elliptical orbit. The user segment, that's you. So that's just your receiver. That's your phone. That's your trimble receiver. Whatever it is that you are using to determine your position, that's what the user segment is. The space segment, the user segment, then we have the control segment. And the control segment is this entire system as a whole. So you can see here we got a master control station and a backup master control station. That's essentially where people are sitting in offices, collecting all the data that ground stations are receiving from satellites, running a bunch of analysis, figuring out how the satellites are performing, whether any of them need to be adjusted in their orbit, et cetera. You then have the ground antennas which are collecting that information. These monitor stations are collecting the information from the satellites. And then these ground antennas are sending information back to the satellites. So an example might be this monitor station receives information about a satellite's orbit. It comes then to the master control station, the control station says, okay, we've received this information about the orbit. It looks like maybe this satellite has deviated from its orbit a little bit where it's supposed to be. Let's send a command via these ground antennas back to the satellite and say, thrust yourself a little bit to the left just to get you back onto the orbit that you're supposed to be. That's what the control segment does. So it's just for controlling, monitoring the satellites, that kind of thing. Any questions about those? About those three segments? What they do? What their importance is? So control segment, user segment, space segment. So how do we actually determine the location of an object or of an entity or of ourselves using these satellites? It uses a method, a kind of mathematical principle called trilatoration. Now we talked about the other week, I guess. Wasn't yesterday I don't think? No, the other week, we talked about triangulation which uses angles fundamentally to figure out position. Trilatoration uses distances. And this is an example of how that would work. If we want to say, okay, I'm a city. I don't know which city I am or I don't know, yeah, I don't know which city I am. I'm looking at this map, right? But I know that as a city I am 175 kilometers from Amsterdam, 320 kilometers from London and 185 kilometers from Luxembourg. So I know that I am a given distance from these three cities. Which city am I? How can I figure out where I'm located? So if we say, okay, well we know we're 175 kilometers from Amsterdam, it's the first set here. If we draw this little circle that has a radius of 175 kilometers around Amsterdam, then we know, okay, we have to be located somewhere along this dotted line. We're somewhere along this circle because we know that we are 175 kilometers from Amsterdam. When we say, okay, we also know we're 185 kilometers from Luxembourg city. So that means that we also have to be somewhere along this line. Now if we know that we're 185 kilometers from Luxembourg and we are 175 kilometers from Amsterdam, that means that we have to either be right here where these two lines intersect or right here where these two lines intersect. So we're probably either Brussels or Dusseldorf, we're probably absolutely butchering that pronunciation. But we know that we're one of those two cities. If we add one more city in, we say, okay, we also know that we're 320 kilometers from London, then we can narrow it down and say, okay, we're definitely Brussels here. Because that's where each of those lines overlap. That's the only point that we can see on this map that is 175 kilometers from Amsterdam, 320 kilometers from London, and 185 kilometers from Luxembourg. So we know it has to be here. That's essentially how satellites work. They know exactly where they are in the sky based off of what orbit they're in and their orbital parameters. And then your receiver says, okay, if I can figure out the distance to three of these satellites, then I can determine exactly where I am. This is exactly how it works. So you use this trilatoration rather than triangulation. In this case, the global navigation satellite systems use the speed of light to calculate distances. So they're not just, you know, they're not just a measuring tape that's determining how far you are from each satellite, for me, the satellites are sending signals down to your receiver, and then your receiver is determining based off of knowing that those signals were sent at the speed of light using radio waves. It then determines, okay, how long did it take for me to receive that signal, based off that how far away did that signal come from? How far away is that satellite? And by combining those distance measurements for at least three satellites, but generally speaking, if I were asked in an exam how many you need, you would always say four, and that's because three satellites are able to determine the position of something, but four are needed to get an accurate and verified position. So you always have a fourth satellite to essentially say, yep, it looks like the measurement that we've gotten from the three satellites is accurate. And that's just because you need three to essentially determine where your position is using trilatoration, using this method I've gone over here, and then you add in a fourth just to say, yep, that's definitely correct. Generally speaking, that's just the minimum, the more satellites that are communicating with the receiver, so the more satellites the receiver can be in contact with, the better accuracy your calculated position is going to be. So more satellites are always really better. Now in reality that process is in a few steps. So radio waves send from the satellite down to your receiver, down to your phone, down to whatever it might be, and then your receiver downloads the almanac, downloads the ephemeris, downloads the GPS date and time, which just means the date and time associated with that satellite, and then it measures the change in time to at least four satellites to determine how far each satellite is away, and based off that calculates a position for exactly where you are on the earth. Now I'm going to go into detail about how exactly it uses time and the speed of light to calculate distance to each of the four satellites, but if I were to ask you, what are the steps for a receiver to ultimately go away and determine position, it would be first download the almanac, then download the ephemeris, then download date and time, that's all sent together via these radio waves, and then based off those measurements, I'd measure my change in time to at least four satellites to determine the range, how far they are, and then based off that be able to calculate a position. So those are the steps. Now the ephemeris and the almanac each contain different kinds of information, and there's a reason that they're downloaded. The ephemeris, generally speaking, contains detailed information on dates, time, satellite accuracy and health, orbital parameters, clock correction coefficients, essentially a bunch of very, very fine scale information. The ephemeris often tells the receiver exactly where that satellite ought to be in orbit. So the ephemeris gives really detailed information about where that satellite is and about the orbit that it's in. The almanac just essentially contains less accurate information than compared to the ephemeris, it's valid for about 90 days, and the main purpose of the almanac is just to speed up the time it takes a receiver to find other satellites. So once a receiver comes in contact with one satellite, it'll download the almanac from that satellite, and then that makes it easier for it to find other nearby satellites that are orbiting. When it downloads the ephemeris, it's then able to determine where that satellite essentially is above the surface of the Earth, where it's at in its orbit, what orbit it's actually in. Because of that, a receiver can always work without the almanac because the almanac's purpose is just to help it find other satellites, but it always needs the ephemeris data because the ephemeris is what actually tells you or tells the receiver where those satellites are in orbit and what the health of them are, whether they're operating properly or not. Okay, so that's the ephemeris and the almanac. Any questions about the ephemeris and almanac? They're a bit of an abstract topic, they're a little bit hard to wrap your head around. I don't expect you to know in detail each specific kind of information that's associated with them, but just generally speaking what they are and what they do, the almanac just being coarse information about the satellite, allowing that receiver to connect to other satellites that are nearby quicker. The ephemeris contains the really, really detailed information about the orbit that satellite it's in and its health and et cetera. That makes sense? Yeah, yeah. Where is it being downloaded? Onto the receiver. So if you're looking at your location on your phone, it'd be onto your phone in that case. Yeah, yeah. The almanac has coarse information about the satellite's health and just general information about it. The purpose of the almanac is that it helps the receiver find other satellites that are nearby in orbit, that are nearby in the sky. The receiver has to find at least four satellites in order to be able to calculate position. So once it finds one satellite, downloads the almanac from that satellite, then it's easier for that receiver to find other satellites that are nearby. Yeah. Yeah. Yeah. Yeah. Yes. So that has to do with something called assisted GPS, which we'll actually talk about towards the end of the lecture. Okay. Sweet. So the almanac, the ephemeris are downloaded. But how do we actually measure change in time? How do we actually calculate how far a receiver is from, say, four satellites that are somewhere above it in the sky? That's by comparing these transmitted PRN codes. PRN code stands for a pseudo random number code. And essentially, your receiver receives this PRN code as transmitted via radio waves. And that PRN code comes with a date in time that it was transmitted from the satellite. Then the receiver marks a date in time when it receives that PRN code, based off the difference in time from when the satellite transmits the code to when the receiver receives the code, we can then calculate how far away that satellite is. All of these codes are traveling at the speed of light because they are being transmitted via radio waves. So we know that they're all traveling at a standard speed. So therefore, if we determine, okay, it was transmitted and then received the difference between the time that that code was transmitted and then that code was received, we can just multiply that by the speed of light. That tells us how far away that satellite is from the receiver. Okay. I'll just, I'll maybe go over that one more time. So we send out the pseudo random number from the satellite, it's sent out with a date in time. The difference between when that pseudo random number is sent from the satellite to when it's received by the receiver is then multiplied by the speed of light that gives us the distance how far away the satellite is from the receiver. You do that with four different satellites. That means now the receiver knows the exact distance to four different satellites in the sky. Based off that using trilatoration, it can determine exactly where it is on the surface of the earth. Does that make sense? Yeah? So we use trilatoration based on the location of the satellites or the cities like you sure of the satellites. So because the satellites have very precise orbital parameters, we essentially have the satellites as these known landmarks that have their position known. And so we're kind of using the receiver and the information about where those satellites are in the sky to determine where we are. Does that make sense? So any questions about this? I know it's a bit, can be a bit confusing. Okay. So that's fundamentally two different kind of concepts but they're very linked, right? So here is the steps of what the receiver is actually downloading and then calculating and then how it actually measures this change in time based off of downloading the GPS date and time is kind of done with this method here with the PRN codes which are transmitted and then have a specific date and time that they're transmitted. The receiver marks that specific date and time where it receives it. That change in time is then calculated, that's multiplied by the speed of light, that tells us how far away the satellite is from the receiver. Yeah. One of the PRN codes is that it gets received and then it can tell time difference, right? That's what the PRN codes call it. Yeah. Yeah. Well, the PRN code is what tells you the time difference. Yeah. Okay. So because of that, because essentially we're basing these measurements on distance off of clocks, essentially off of the change in time, we have to have super, super, super accurate clocks. The signal travels about a meter in three billionths of a second, which is just the speed of light. So satellite clocks are off by about just one millionth of a second. Our position would be off by potentially three thousand meters. Because of that, satellites use something called an atomic clock, which is essentially by most standards the most accurate clock or measurement of time that we have available to us. Atomic clocks work by monitoring the vibrations of an atom, essentially. That's why they're called atomic clocks. I don't expect you to know how that works, but it's kind of a fun fact, I think. Okay. Kind of just to summarize a couple of those concepts that I just talked about and how GPS or GNSS determines our location. I just got a video that we're going to watch. It's about five minutes long. Video then also goes into a little bit about some of the concepts I'm going to talk about next. Things like differential GPS. So I don't expect you to necessarily know what's going on when it starts talking about differential GPS and some of the other concepts in the second half of the video. But it serves as a good introduction because I'm going to go over them in detail. So it's good watch. So we'll watch here. In December, I finished with the video first one on hand. And the European Union got its own satellite innovations. It joins the American GPS and the Russian developments in the current return system available around the globe. And when it did cost more time, when it was expected, it also increases your reliance on other systems. The navigation systems have long been on path or everyday lands. They talk about logistics, the orchestrations, and the name of the econ to take you anywhere in the world. All just one to do the nearest sounds. By looking at the history of these systems, it's clear that the motivation behind them was not to take you to your nice cappuccino, but instead, what always pushes technology forward. Or, essentially, if it's cold. When the Soviet Union launched putting one into space in London, in December, they became the first nation to eliminate that light into orbit. That, of course, annoyed the Americans, but more quality, it marked the beginning of settlements. A very low space that constantly orders to Europe and can use for communication or spying or navigation. This is especially important if you're a things that really shouldn't know where they're going. GPS first became a original in 1978, and the luck of the Russian donors was one of the two major satellite navigation systems for multiple techniques. So we opened up in order to challenge Japanese security owners to do that on the console over systems. These two pairs are set like the rest of the point blocks of the radio and so on, constantly sending signals back to the raw parts and hence the current location of the precise sound and signal sound. And you know these four pairs go over the field light, it's still in the 50 and 100 milliseconds of the average year. And the QPS is the easiest to all of these signals, but because some satellites are further away from the others, they're signals that work different times. So when satellite A and B says that a signal is exactly the correct point, but the signal from satellite A is the first, then you have to be closer to satellite A and B. QPS is the right to be made under precise sound signals and calculate how much closer satellite A is because the satellite is also set under current position. You don't want to know where you are and where there are two satellites, but there you are, exactly. And at the first satellite you can get to all of them in two dimensional space, or on the map, and for the fourth you can also calculate the animation. And with a system like that you can pinpoint your left position on Earth with an accuracy of 21 million, but it wasn't always this accurate. So when GPS was first developed, it was supposed to provide a complicated accuracy of 100 meters, but some are accidentally in the LTP-5.5.5. For a system available to everyone, a little bit of a problem to be ready to accurate, so they implemented selectable available, which added a random offset to the public signal, but still became accuracy of an encrypted signal, which only the military could be. So now public institutions have to work with a signal that was significantly less accurate. So that led to the development of differential GPS, which includes accuracy by unique reference points. Take the land map, some place you know the exact problem of the signal. Take a GPS to see one top, and you can always compare an actual location with a location to receive a practical answer, and apply that to friends to all of the other signals nearby. But the unofficial offset is not the only thing that affects accuracy. GPS supplies broadcast location, but slight locations will always be avoided. And rather atomic blocks are really precise. Over light span of multiple decades, the blocks grow almost slightly wrong. Location and accuracy in clock room are the same amount of local, and can be easily compensated. But public systems may be from a satellite to the ground. They have to go through the atmosphere. Atmospheric disorders are much harder to deal with. Because they are highly local, depending on where you are on the signal at the travel program, and the atmosphere itself changes all the time. So generally a single offset is enough, which is why they are none of these stages, all are not developed. The issue is how to get these corrections to the rescue of them. One way is to simply broadcast them locally. The US coast was just set, and from the late 1990s at most coast and American waterway recovery. But that still led to that part of the continental US with all key truth accuracy, which brings us back to satellite. To improve the navigation in aviation, the FAA and others work on the system to broadcast these differential information by a satellite, to make them available all around the US. By now they are most of the transport to gas systems, and they live in a different part of the world, both restaurant and satellite space. All of these systems end up making space more accurate, than you can ever be in its own. More accurate even if you ask what the e-without artificial intelligence, which may select an availability resource. There was an executive order for building. It was finally turned off to the gate counter. That is why the European Union Launch Challenge have a system of its own, where no one else can deliver space. So the next time you navigate Starbucks, remember all the lonely satellites about you, always to take the work of time and no one to ever discuss. Okay, so historically we've kind of, as most things happen with technology, have gotten receivers that have gotten more and more and more and more accurate through time, and have become smaller and smaller and have become, again, through time less expensive and less expensive. The first GPS receiver from 1977 looked like this, so there's this massive unit where you were just kind of, you know, just sit here and wait for all the calculations to happen. Today we have, you know, microchips that are in our phones that are essentially GPS receivers, maybe as small as the fingertip. We also have GPS receivers that are much, much, much more accurate than something like this, where we have these big antennas like that. They're much more expensive. They're often just used in industry, environmental applications, whatever that might be. Generally speaking, as you get, go kind of from the spectrum of left to right here, you're getting more accurate GPS measurements, but each receiver is costing more and more, so obviously you have your phone here, it's the least accurate, cost the least, you have the Garmin and the Trimble GPS, and then you have here a Trimble GPS that just has this attached antenna to it. So the Trimble GPS's are the most expensive, they get up to around $3,000 Canadian dollars, and then these antennas that you can buy to go with them are usually another 3,000 to 5,000 Canadian dollars. So your phone, generally speaking, has an accuracy of about 3 to 15 meters, 95% of the time, and we say 95% of the time, just because you need a clear view of the sky in order to get the typical accuracy associated with the GPS on your phone. Accuracy in general is influenced by the number and position of satellites that your receiver is able to connect with in the sky, atmospheric effects, obstructions like trees and buildings, just how good, how high quality your receiver is, as well as whether or not you're using any kind of post-processing corrections. I'll talk about each of those in a bit more detail in a second here. The maximum accuracy you can generally get with a GPS is from those very expensive Trimble receivers like this that have these antennas attached to them, and then also use a form of differential GPS. And again, I'll explain what differential GPS is in a moment, but the point of this slide is just to tell you that the maximum accuracy, generally speaking, that we can get from GPS measurements, horizontally is about 10 millimeters, which is just kind of your X-Y coordinate, and then vertically is about 12 millimeters, which is essentially just telling you your elevation or your Z coordinate. So generally speaking, highest accuracy we can get about 10 to 20 millimeters using GPS technologies that we have available to us today. But that's really good. That's 0.1 to 0.2 centimeters. That's super, super good accuracy, considering we're just using these flying objects through the sky in order to determine our position. Now, there's lots of sources of error with GPS. There's lots of reasons why you might not get a very accurate GPS measurement. One of them is that radio waves can't pass through some objects, things like buildings, trees, mountains. Radio waves can bounce off of some of these objects sometimes, and then still kind of reach your receiver. And this results in something called multipath errors, where the actual time it's taking to get to the receiver doesn't reflect how far away that satellite actually is. That's called a multipath error. It's generated by obstructions, landmarks, buildings, big mountains, trees, those kinds of things. Generally speaking, if you're using your GPS, you want to get the best measurement possible. You want to try and remain in the open. Try and avoid buildings, tall trees. You want to try and take several measurements through time over and over and over again, and average them. That's going to give you your most accurate measurement, and then you want to also be patient. Sometimes when you pop open your phone, it doesn't, can't figure out exactly where you are right away. That's typically just because the receiver is looking for some satellites that can send it a signal. And as it's looking for those satellites, it's slowly connecting to them and getting a better and better accuracy in terms of determining your position. One of the ways that we measure how accurate your satellite-based position is, is with something called dilution of position or a dump. It's a measure of the geometry of the visible GPS constellations. So you can definitely expect a midterm question or file question about dilution of precision. And essentially what it is, is a measure of the geometry of the visible satellites. So this is a good dilution of precision. This is an example of a good one. Because all of the satellites are spread out over the horizon. They're not all bunched together. This should be an example of a poor dilution of precision, because all of the satellites are very, very, very close together. We get more accurate GPS measurements when our satellites that we're using to determine our position are spread out more throughout the horizon or throughout the sky. So again, that might look like this. You got here a good dilution of precision. Your satellites are, in one sense, spread out horizontally. They're not too close together horizontally. They're also kind of spread out vertically. You got some that are further away from the surface of the earth than the others. That will give you a really nice dilution of precision. This is an example of, again, a bad dilution of precision. All these satellites are kind of bunched together both horizontally and vertically. There's not a lot of diversity in terms of where they're located above your position on the earth. If you were to be standing right here. So that's an example of a bad dilution of precision. Generally speaking, you can break down the dilution of precision into a couple different measurements. You have your position dilution of precision, which is just you adding your vertical and horizontal dilution of precision. And the vertical and horizontal dilution of precision is kind of what I just mentioned, which is that a horizontal dilution of precision measures horizontally. How far the satellites are spread out and your vertical dilution of precision measures vertically. How far your satellites are spread out. Braves people see here these two satellites vertically are kind of at the same point. These two satellites vertically are kind of at the same point. Whereas this one, there's one lower bit higher, bit higher, bit higher. So that's a nice vertical dilution of precision. Then you got your time dilution of precision. Your time dilution of precision is just an estimate of clock errors associated with the satellites or maybe with the ephemeris. And then add all of that together and you get an overall dilution of precision, which gives you a measurement of the accuracy of your positioning. Generally speaking, how dilution of precision values work and you'll look at these and one of the assignments coming up, a lower dilution of precision, so a lower value is a good dilution of precision. So when you look at dilution of precision, the higher the value is, the more it's diluting your precision, i.e. the less accurate your value is. So a very good dilution of precision is a smaller number. Now in general, to get a good dilution of precision when you're going out and making measurements, you want to have high quality mission planning, which just means that before you go out and measure your GPS locations, you want to ensure that you're going out at a time and a point where there's gonna be lots of satellites scattered all throughout the sky that are gonna give you a nice accuracy. And so that's where mission planning comes in. You can use websites like this to look at where satellites are estimated to be over your point or where you're going out approximately to measure GPS points. And then you can look at a graph like this and say, okay, throughout the day at a given point, when is the dilution of precision going to be the best? When's it going to be the worst? And you'll do this, like I said, in one of the assignments coming up. In general, if we look at all the different kinds of errors that we can get, we can have GPS receiver errors, which just means that maybe we have a lower quality receiver. Our phone, for example, is a much lower quality receiver than a trimble, a really expensive GPS receiver. We might look at clock errors associated with either the satellite or our phone. We might look at ephemeris errors, tropospheric delays, ionospheric delays. These are both just different parts of the atmosphere that result in potential refraction of the radio signals that are being transmitted from the satellites to your phone and might actually slow down the signals or force them to go in a different direction. That can cause some errors as well. And then multipath errors, which we already talked about, if the radio waves are coming down and bouncing off of buildings and things like that. The biggest issue generally, or the largest source of error, is coming from the ionosphere. And the ionosphere is this part of the atmosphere that ionicizes, which just means that all of the molecules there become ions, which means that there's all of these floating electrons that are in this part of the atmosphere. It's often influenced based off how much solar radiation is coming from the sun, which just means that it can change on a daily basis. During kind of high noon or the middle of the day, when there's lots of sunlight coming from the sun, the ionosphere effect is very, very strong because there's tons of these free electrons floating around in that part of the atmosphere. Whereas at night, it's not so much of an issue because there isn't this solar radiation coming down and interacting with the ionosphere to create all these electrons that influence the radio waves being transmitted from the satellites coming down to your receiver. It's a lot of words, I know. It's not really that important in terms of midterm final exam purposes. All I expect you to know are the different kinds of errors that you can get with a GPS position and which one has the largest impact, which you can see right from this graph is the ionosphere. Okay, any questions about where we're at so far? About GPS errors? Anything like that? Okay, sweet. Now, the next couple things I'm gonna talk about are the different ways that we have come up with in order to improve standard GPS measurements or standard GPS positioning services. One of them is called differential correction or differential GNSS or differential GPS. And it's used to increase the accuracy of GPS location by taking a base station with a known location and using that to compute corrections for wherever you're measuring location that's maybe somewhere nearby. So, this kind of as an example here, I am the receiver here. I'm looking to find my position and I have connected to a bunch of satellites in order to find my position. But I have a reference station, which just means that I have some sort of nearby building or in this case they have a little rover here, some sort of other receiver that's gone ahead and established its position. Then I can say, okay, based off of knowing exactly where this is, I can actually correct the measurements that I'm getting and get an even more accurate sense of my location. On the other hand, I'll go over that again. I know I went over it quick, but that's just cause I'm gonna go over it again here in a second. We also have RTK or real time kinematic GPS. RTK is just a type of differential GPS. It's kind of a newer version. It works a bit more accurately and it works a bit more faster. It's called real time kinematic because it provides corrections in near real time, but it works the exact same way. You have your rover or your receiver here. You are trying to determine your position. You have this other rover that has a known position as well. You use that to then transmit corrections to wherever you are trying to get a position for. So if you compare them, differential GPS, it still uses satellites just like any GNSS. It uses a base station with a known location. Oftentimes in the case of differential GPS, this is one of the key differences. It uses a permanent location. So something like what was mentioned in the video we just watched, it said that there's different differential GPS systems that are set up kind of across the world. That's often true. Sometimes it's like a building or something somewhere that has a very well known and established accurate position and it's able to transmit corrections to wherever you're measuring position based off its well known position. Relatively speaking, differential GPS provides less correctional information than real time kinematic GPS. And it's transmitted a little bit slower. It can be transmitted in near real time sometimes, but often it requires post processing, which just means that you go out there, you measure your point in the field or wherever you're working, and then you go home to your office later, you pop that point into your computer, you download the corrections for that date and time from the differential GPS system, and you're able to calculate a nice, even more accurate position using differential GPS. On the other hand, we have this RTK GPS, also uses satellites just like any other GPS or GNSS. It has again a base station with a known location, just like differential GPS. Generally speaking though, the key difference here is that RTK uses a mobile station, which just means that if you're going out in the field, typically you take this extra rover with you, set it up close to where you're going out to measure a bunch of points or a bunch of positions, and you let it establish a nice, accurate position for exactly where it is, and then you go and walk around, so you're actually still pretty close to where that rover is and able to transmit real-time corrections to exactly where you're measuring. It also just generally transmits more correctional information than differential GPS, it just has a newer algorithm, and again, it generally transmits it a bit faster, typically always in real-time, which just means right away, instantaneously. So, differential GPS generally looks like this, where we have our rover, we're going out to determine positions using this tool right here, using this instrument, we're going out getting information from satellites, determining our position, but then there's this base station often a building with differential GPS, something permanent that has a known location, and it's transmitting correctional information to us, so that we get an even more accurate sense of position or more accurate measurement. Real-time or RTK, on the other hand, you typically have your going out with your rover, this is where you're measuring GPS, where you're moving around, getting a sense of measurements from positioning, and you have brought out with you some sort of base station that you set up, you leave it in one place, you let it measure an accurate position for exactly where it is, and then you leave it, you walk around with your receiver, and then in real-time, that rover that you've set up is transmitting correctional information to you, as you are collecting or measuring points, using GPS, using whatever GNSS satellites are available to you. Yeah? This is RTK, you can use satellites as a connection. No, use the same exact satellites, same number of satellites. Yeah, nothing's different about the use of satellites between the two. Because the diagram says that the fly satellites could look like RTK and four-visidables for a different company. Or you're asking the number of satellites. So generally speaking, just because RTK is a more accurate, it generally is a little bit more accurate, so it might use an extra satellite just to have an extra verified position, but I don't think it honestly matters too much. Yeah, I wouldn't ask you about it anyways. Yeah? So basically, we have like, oh, not satellite, but you, you use differential GPS, but it's a field limit. Generally speaking, today, most civilian GPS systems do use some form of differential GPS. Yeah, but none of them really use RTK GPS because that would involve going out to exactly wherever you're doing your measurements and setting up your own rover. Setting up your own kind of base station. Does that make sense? Yeah, okay. Any other questions about the two? Just about how they work in general? No? Okay, sweet. So assisted GPS, there was a question about this kind of earlier about using cellular networks and Wi-Fi and things like that to get better GPS measurements. That's kind of what this is about. So assisted GPS or AGPS is when a receiver uses cellular networks to essentially establish a more accurate or quicker position. So AGPS typically just improves startup performance by using cellular network towers that pass information about the location of relevant satellites to the receiver. So in general, AGPS or assisted GPS is just the use of a bunch of a network of cellular towers that are able to just quickly send information to your phone or whatever GPS receiver about satellites that might be nearby in orbit that you can connect to. So often times you might notice, I think someone asked about it, but if you have your cellular data on, for example, or you have just your network, your cellular network on doesn't have to be data, but you'll get a better, a quicker measurement of your GPS location than say, if you are trying to determine your location on your phone in airplane mode. So you can actually determine the location on your phone in airplane mode because GPS, your positioning, isn't using cellular networks. It's using global navigation satellite systems, which are completely different. But if you turn your network on, then it can use those network towers to transmit extra information about where satellites are so that you can connect to those quicker and get a quicker measurement or quicker location. Does that answer the question that was earlier? Yeah, kind of. So that's what assisted GPS is, it uses cellular networks. Dead Reckoning is another form of kind of correctional GPS and it just uses, it calculates your position by using a previously determined position and then estimating based off of if you're moving, what speed you're moving at, what direction you're moving at, where exactly you might be. So you'll notice sometimes if you are using GPS in your car and you're about to go through a tunnel, as soon as you're inside the tunnel, it kind of has a much tougher time figuring out exactly where you are, because it's maybe not connected to those satellites anymore, but it'll still move along. It'll still kind of roughly tell you where you are inside the tunnel and that's because it's using dead Reckoning. So it's just kind of calculating, okay, based off of where you were when you entered the tunnel or right before you entered the tunnel, the speed you were traveling at and the direction you were traveling at, it'll continue to estimate where you are when you're inside that tunnel. Does that make sense? Let's see. The last kind of correctional information that we talk about is indoor positioning systems and we don't talk about them very much because they are still relatively new, but indoor positioning systems use things like Wi-Fi or Bluetooth, typically to send signals out to your phone in order to detect where they are, kind of an enclosed indoor space. Again, not super, super commonly used, but something that's on the forefront in terms of being able to better measure and determine your position indoors. Because generally speaking, it's really hard to determine your position indoors because it's hard to get information from satellites. It's hard for those satellites to transmit their signals to your receiver if you're standing inside. So that's where these indoor positioning systems come in, typically based off of Wi-Fi or things like Bluetooth. And often how they work, again, like I said, is just your phone is kind of like an RFID, if you will. It's being able, it can be detected by sensors that are set up, often Bluetooth sensors, that are set up around a building and are just transmitting signals outwards throughout the whole building. It can detect where your phone is and then kind of use to determine your location in the building based off that. Does that make sense? Any questions about that? It's a bit complicated. I don't honestly know that much about it. Yeah. Does it just like follow-off the satellite signal? Pardon me? Does it just like follow-off the satellite signal? It wouldn't. You mean the... So in this case, the indoor positioning systems, the signals that I'm talking about are coming from someone going in and setting up these little devices inside the building that are transmitting their own signals. So the satellite's going to be sending? Exactly. It's completely separate from satellites sending their information. So you'd go in, you'd have someone come in, they would set up these devices kind of all over the building and then they would program those devices to know exactly where they are. So if those devices know exactly where they are and then they transmit out these Bluetooth signals, they can detect where your phone is and then based off knowing where they are trying to determine where you are in the building. So completely separate from global navigation satellite systems. Yeah. Any other questions? Okay. I got some... Oh, last slide here. Generally speaking, there's a wide, wide, wide variety of applications for GPS and satellite navigation systems. We obviously use them in transportation and navigation, whether it's on your bike, in your car, in a plane. We use them for lots of mapping and surveying applications. We use them in agriculture, aviation, like I mentioned, environmental applications. Oftentimes with public safety and disaster relief, as well as in recreation and wildlife research. So it's a really, really fundamental component to a lot of different kinds of industry, a lot of different kinds of research. And now hopefully, you know, kind of how it works. So the last thing I have is a couple of questions for you to practice, either with a neighbor by yourself. So I'm going to, like I always do, give you about five minutes to practice these questions with someone. If you want to head out, now you're welcome to do so. Leana is going to be here in about five minutes. So if you have questions about assignment one, you're welcome to stay and ask her. She'll be here in literally a couple minutes. But if you don't want to do that and you don't want to go over the answers to these, then you are welcome to head out. And I'll see you guys next week. Hey. This is how we use TMS as like interchangeably with TPS. Yeah. Okay. Well, I use the terms interchangeably because in practice, they're often used interchangeably. But what I tried to do at the start of lecture was lay out how they are different. So they are technically different things. But we typically just in North America refer to GNSS as GPS because it's just the most well-known GNSS system here. Do you have a preference for like in the midterm or the final? No, you can generally use them interchangeably unless I am asking you specifically what the difference is between them. Yeah. Hey. I'm so confused about IPS. Like you mentioned how Bluetooth is and why is someone basically someone goes into the room and says that the system. But what Bluetooth, how can you set up a system? For example, if I got my phone and I have Bluetooth in there I can throw drops off from a Mac to my iPad. So, I'm very confused there. So, how it would work is there would be a bunch of sensors, devices that are set up all throughout a building and that transmit a Bluetooth signal out from them to then try and locate something like your phone. But if I'm not in the building, I can see these Bluetooth though. Sure. So. But if you're not in the building, then you don't have sensors that are specifically designed to transmit out signals in order to find your location. Right? Like if you're not in the building, you're not using indoor positioning systems. If you're not in a building, you're using GPS or GNS. But I can soon use Bluetooth if I'm outside of it. Right. But it's not being used for positioning services. Right? Like you can use Bluetooth out, you can use Bluetooth anywhere for different purposes. Right? But when you are sending, air dropping something from your laptop to your phone, your laptop isn't sending out, isn't sending out Bluetooth signals in order to determine where your phone is. Oh, right. It's just sending out signals in order to transfer something from your phone to your laptop or vice versa. Oh, right. Yes. So, they're basically like going to a signal station everywhere around now. So, we can actually just use like Bluetooth to find out location. I mean, not in reality, no. Like, I don't think that this building, for example, has a bunch of Bluetooth sensors that are set up all around. That's kind of when I was talking about indoor positioning systems, that's something that can be installed in a building and has been used before. But it's not something standard. It's not something that you would commonly see. Right. Yeah. Yeah, no problem. Hey. Something real quick. Yeah. It's like a big branch and the GPS is one of the- Exactly. It's specifically the American GNSS system. GNSS means global navigation satellite systems. Yeah. Yeah. And can you quickly, like, if you have time- Yep. Where I go over the difference between the RTK and the differential GPS? Yeah. Yeah. So, the key difference is they both have these second diagram that's a bit better, I think. They both have a base station that has a known location associated with it. The main differences are A, differential GPS. The base station is typically permanent. It's a building or something like that. Mm-hmm. Whereas RTK, typically the base station is mobile. So you can move it to wherever you're going out to take your measurements. And then the differential GPS, or I should say maybe the RTK GPS sends more correctional information. So it's just a newer algorithm. And it transmits faster than differential GPS. How come this has more accuracy than if this is mobile? Because I feel like this is permanent, so it should be more accurate. Right. That's a good- I can understand why you would think that. The main difference is the correctional information you be sending from a base station in RTK is going to be much more localized. Because you're taking it to wherever you're doing your measurements. So you're never getting that far away from it. Whereas differential GPS, you might have a base station that say in Vancouver, but you might be doing measurements up in northern BC. And the other reason it's a bit more accurate is just because it has a newer algorithm. A newer algorithm. It just is better essentially. And then you have differential GNSS. So maybe I was thinking this is GPS. Yeah, so I use differential GPS. I use the terms GNSS GPS kind of more or less interchangeably. Throughout lecture. Because that's how the terms are often used in practice. And that's fine for you to use the terms interchangeably. The only setting in a midterm or final where you shouldn't use them interchangeably is if I'm specifically asking you, which I probably will, what the difference between GNSS and GPS is. Oh, okay. But other than that, you could interchange. Totally. Yeah. Yeah. Thank you. Yeah, no problem. We actually got one more. Yeah. The GNSS. Yeah. Yeah. Yeah. So for amorous, can we understand amorous from like kind of schedule for a kind of calendar or satellite? The almanac or the ephemeris? Almanacs. The almanac? Yeah, almanac. The almanac? A schedule. Kind of. I mean, the ephemeris has the more detailed information about orbital parameters. So it really has the more important information about exactly where a specific satellite is. The almanac, all you need to, the best way to understand an almanac is just providing coarse, general information about the health of a given satellite and then allows the receiver to connect to other satellites quicker. That's really the fundamental use of it. So basically, it is your use in tandem and ephemeris is basically the only thing that we need and almanac speeds up the operator, the time in which, you know, basically speeds up the operation of the ephemeris. Exactly. Oh. Yeah. Doesn't literally speed up the operation of the ephemeris, but it speeds up the ability to connect to other satellites in order to download their ephemeris. Oh. If that makes sense. Is the information like there between a satellite? Generally speaking, yes and no, the information sharing between satellites is when satellites transmit information down to the ground stations and then the ground stations analyze all the information that they're getting from different satellites and then maybe transmit info or commands back to the satellites. But the satellites aren't typically going from satellite to satellite. Sure. Yeah. Yeah. No problem. Does that make sense? It speeds up the operation of the ephemeris, but it's because the almanac allows it to find other satellites nearby and this in turn increases the efficiency at which the ephemeris operates. Yes, but you're talking about the ephemeris as if it's, I don't know, as if it's a tool. The ephemeris is just a set of information. Very similar to the almanac and that is just a set of information. The ephemeris isn't actually doing, it's not performing the calculations in order to find the location of where you are. So can we understand that ephemeris is actually more detailed than the almanac? Yeah, the ephemeris is more detailed than the almanac and the ephemeris provides, specifically what the ephemeris provides that's important is orbital parameters. So it tells you exactly what orbit that satellite is in, which in turn tells you exactly where that satellite is in the sky. Then when you use the pseudo random number code to determine how far that satellite is from you, you can then, based off that determine an absolute position because you know exactly where that satellite is in the sky because you've downloaded its ephemeris. Does that make more sense? Yeah, yeah, yeah, yeah, yeah. The ephemeris is, like the information can be gauged through the GNSS. So like the GNSS gets both of these pieces of information. Well, so your receiver does. Your receiver. Because the GNSS is an overarching term for the control segment, the user segment, and the space segment, right? So the GNSS is the whole system as a whole. Your user segment is your receiver. Your space segment is the satellites. And your control segment is those control stations on the ground that are monitoring their performance. Does that make sense? Right, yeah. We can talk again in a sec if you like. I'm going to go over these questions. Yeah, no problem. OK, guys, we're going to go over these questions here. So. What side of the woodward building are we on here? You guys know? Like cardinal direction east northwest? North side? Really north side? Leanna, your TA, is lost right now. Somewhere in this building, she can't find us. She just said, where the hell is this room? So I'm going to say north side of the building and hope that she finds it. I said, yeah, that's what I told her. She knows it's room two. But I'm guessing she's never been. This is a funny thing about grad students. I guess I did my undergrad here, so I have a slightly better sense of where things are. She's doing grad school here. And she's in the forestry building. She's literally probably never been in any other building ever. So this is probably kind of overwhelming for her. Hopefully she finds it. OK, so how does a receiver find your position? We can break that into two questions. What is the method or principle used? What's the mathematical? She said, everything is labeled with G's. What does that mean? Are those like on the same level as these big lecture halls? They're in the basement. Oh. G's are this floor. G's are this floor. So they're just like the small rooms. Oh, you made it. Nice. OK, let's go over these real quick. So what is, first of all, is to find your position for a receiver to find your position. First of all, what is the method or principle used? What's the mathematical principle that we talked about that satellites and your receiver use in order to find your position? Tri-lateral. Tri-lateral, exactly. And then what are the steps required in order to find your location? What do you download and then what's calculated? Yeah. Is the almanac eight members? Yeah. And then the date and time. Yeah. And then what do we do? Yeah. We calculate the change in time. Yeah. Yeah. And then we determine the range based on that change in time. And then we calculate our location. Nice. Good job. What is the accuracy of GNSS positioning? That's kind of a trick question. I wouldn't actually ask you that, you know, loose or not specific of a question in an exam setting. Because you can really break it into two kind of things that we've covered. What is generally speaking the accuracy of GPS on your phone? Yeah. Yeah. About three to fifteen meters. And what's about the maximum accuracy that we can get with a GPS device? Yeah. Yeah. Yeah. Yeah. For the horizontal edge. Yeah. Exactly. About ten to twenty millimeters. Perfect. What does DOP stand for? Yeah. And what does it measure? I just bought the definition. That's okay. It's a measure of the geometry. It's a measure of the geometry. Exactly. It's a measure of the geometry of the satellites that you're using to determine your position. A good dilution of precision is when all of the satellites are spread out across the sky. A poor dilution of precision is when they're all really, really close together. Okay. And what can someone try to explain to me what RTK GPS is? Please. Don't make me beg. Come on. Yeah. Yeah. Yeah. Yeah. Exactly. Yeah. Exactly. So it's a type of differential GPS uses a mobile base station to correct your positional information in real time. It produces or uses a newer, better algorithm than differential GPS. And it transmits correctional information faster. And uses, like we said, that mobile rover. Okay. Awesome. Do you have slides or anything you want to go over? You just. Okay. Do you want to go over them? Yeah. I'm going to go over some tips and then. Okay. Sounds good. Sorry. I forgot my flash drive again. That's okay. Oh my gosh. Yeah. Sorry you got lost. Sorry. I was asking people, like, how to describe to you where it is. If you imagine your grandmother is trying to find this classroom. Okay. One, two, seven. Modules. Sorry. It's in assignments. I didn't. I think lecture six is labeled lecture five also. So I didn't know if that was today's. So it's here. One more. Yeah. That one. That one. Okay. There you go. What's your mic? All right. Hi guys. Sorry. If you're heading out, please do so, specifically, just so you're not distracting anyone that's saying please, thank you. Cool. Thanks for trying to help me get here. I appreciate that. I have a terrible sense of direction for a geographer. So clarifications to assignment one. I have a couple of questions that I've been getting in the Zoom office hours. So we're going to go over that. And then I'll open it up for just like general Q&A. And then if you want to ask like more specific questions after, you can just come down and ask me some questions. So what am I doing here? I'm just scrolling. Okay. First of all, please make sure that you're using either like a desktop computer or a laptop for like either the Chrome browser or the Google Earth software. If you're using an iPad, it doesn't show that camera view like number, which is needed for one of the questions. So like that's something that people have been running into some problems with. If you don't have access to a desktop computer or a laptop, the library has some. We really recommend going to the library. If not, if you really can't get there, come see me, come to office hours and like we'll figure something out. I can let you drive. I can share my screen and let you like, you know, work that on my computer, but you know, we can't just be like, here's the answer. Anyway, remember that it's really, really good practice to try to complete the assignment. If you're not already trying to complete the assignment in Canvas, like alongside with the quiz, it's really good practice to have the quiz open because sometimes there will be multiple choice answers or like a format to the answer that you wouldn't know if you're just doing the assignment. So just like if you're confused about something, make sure you can check the, that you're checking the Canvas quiz at the same time. You can enter the quiz and leave the quiz at any point and it'll save everything. But if you submit it, that's your submission. So like you can start the quiz, leave the quiz and your answers will be saved. You can come back to the quiz, but if you submit the quiz, that's it. So like for this assignment, just let us know if that's something you accidentally did and like we'll talk about it, but just remember that. And then just like a general to pay attention to the directions in the assignment. So some of the questions we've been getting for question five, which is when the camera is tilted like this, which of the following now appears in much greater detail? And the thing we're looking for is that much greater detail because you can kind of see some of these answers if you're, I mean, closer up. We're assuming that you're answering this question right after going through the steps to get here. So like you should have this view that's up on the screen right now, sort of like this. If you have to zoom out a little bit to see all of those things and you're not just like, you know, really zoomed in Vancouver, there should be one object that is much more obvious than the others in this 3D view. So make sure that you're in 3D view and kind of have something similar to what's on the screen. Like that's what you're looking at. Any questions about that? Cool. For question eight, which is please take a screenshot from this path facing south and uploaded onto canvas. So on the assignment PDF, it has that blue line with the arrow pointing to it. And that's like the path that you should drop your little person icon onto and then like you can use your compass to face south. And so you should be facing this down here, which in the assignment PDF, it describes as the buildings of the West End. If you end up like over here or over here for your screenshot, it's fine. It's okay. There's no like one exact screenshot that we're looking for. And if you are like five degrees off, then we're going to take points off. This is a very, very flexible question. We're just looking for a screenshot in the vicinity of Lost Lagoon in Stanley Park. Cool. Cool. Question ten, what is the squamish name for seawash rock? And remember this is the one where in the original tips, it said two names. We changed that from last year. So there's only one name in that's in the tips PDF, which is on canvas under assignments and also under your lecture five module. So if you, there were a couple questions about like typing that into canvas. If you can't spell, there's like a little symbol kind of letter. If you can't spell that on your own, just copy paste it from Wikipedia. And that should be kosher. And then for question fourteen, so this is a really important one to pay attention to. So it says according to the camera view in the lower right hand corner, how tall is this mountain? And the thing to pay attention to is camera view. And that is what is outlined in red in this part of the, in the lower right hand side near your little compass thing, your person guy. That's, it's that camera view that you're looking at for question fourteen. There's like another number over here. And that is not what we're looking for, for question fourteen. Some people are writing this number, but we're looking for this and it says camera view in the assignment PDF. I'm sorry I keep saying camera view. I just really want to clarify that because that's been tripping people up. And also please note that this is not the number we're looking for for question fourteen. This is just an example. Cool? Cool. This one is also really, really confusing. And this is my fault and I tried to make this easier and again it didn't work as well. So now we're on the opposite side of the world. Opposite side of the world. Which of the land masses can you see from this view? Select all of the ply. On canvas it's been changed so that it's edited where it says zoom out to like two thousand kilometers. Some people are still only, so like when you type in the coordinates it takes you to this random dot in the middle of the Pacific Ocean. Some people even when they zoom out to two thousand kilometers are still getting a random dot in the middle of the Pacific Ocean and like there's nothing you can't see anything. That's fine. You're not doing anything wrong. Just kind of like pan around a little bit. It's okay if you're not exactly two thousand kilometers. Like the camera's not exactly two thousand kilometers away. It's okay if you need to move your cursor. Just what we're looking for is which of the four or there's six answers listed on canvas. Which of those four can like our closest to this dot. Which is the coordinate that we tell you to put in. Yeah, that's right. Cool. I think I got lost somewhere. Does that make sense? Are there any questions about question 15? Amazing. What is for question 19? What is the latitude of the Arctic Circle sign, Arctic Circle, in degrees, orient the compass to north and use the coordinates found on Google Earth? Oh, sorry. It's been six o'clock and I know you guys are tired too. Twenty-three says to type in the Arctic Circle sign into Google Earth. This is where you're getting the location of the Arctic Circle. You need to type in the Arctic Circle sign to get the coordinates for the Arctic Circle. And when you do that, try to keep your mouse in the middle of the compass so you're not getting a slightly different answer than what your peers are getting. If it's like.3 degrees off or it's not really, really like small number, like say it's night, someone's getting 66, 40, 18 and you're getting 66, 40, 19. That's okay. There's like a little plus or minus thing in canvas. That is fine. That's a really, really infinitesimal amount of distance in reality. And you should be writing the coordinates like this example. Again, this is just an example and not the real answer that we're looking for. It'll say like 66 degrees sign, 27 apostrophe, 34 double apostrophe, which is actually feet and wait, no, minutes and degrees, minutes, seconds. Thank you, Professor. Yeah. And this is on canvas. So when you like open canvas, it has like blank degree, blank minute, blank second. Hello. It says, go over. After the, whatever that quote, sorry, there's like north. Don't put north. We just want the numbers for this. And if you're doing question 20 and you decide, hey, if I want to go to the opposite side of the world to Antarctica, maybe I'll put a negative sign in front of it. We, if you want to put a negative sign in front of a coordinate point, just put it in front of the degree sign, just 66. So if that's how you decide you want to say now we're in the southern hemisphere, it's just in front of 66. Does that make sense? Hello. Just to put, let's, I'll use two points, the comp of the current and the comp of the cancer. If you want to get from the same coordinate and comp of the cancer, then like the, I get, comp of the true or comp of the cancer, you would just change the sign from the person. Right. It's very similar to what you do in question get to this coordinate point in question 15, where we go from Mount Logan concept. Hi. I know yesterday we talked about the Dominion building question. Yes, the Dominion building. Sorry. I'm not sure the, I remember you said like, oh look at that next service, not like the possible meaning for any building. Like it doesn't say it's height there, like click on it, and there's a height that's will provide the ninth, like, like, like, Wikipedia and then it's an article there's a height. Yeah. In the details there's a height so there's a three different height so I don't know if there's any. There are two heights listed in Wikipedia. Either of those two is fine because Wikipedia is super weird for having two heights. You can't have that. But no one decided to tell Wikipedia that. So there's two answers. Which one is that? I forget what number that is. But there's two answers to the Dominion building one and either one you put in will be fine. But yeah, don't use like the number in Google Earth itself. Use the Wikipedia one. Cool. Yeah. Some of the questions is that, oh, just with the number for the height. But I don't remember exactly what some of them don't exactly. We only want the number. Yeah. It's easier if you don't put like a minute or meters or degree sign and usually we'll have like, well, sometimes with meters we don't have it but like for the degrees we'll have that. But it's better to err on the side of just putting the numbers. Okay. Yep. I think, yeah, I have one more. So that's it for what I'm covering for questions. But please just remember again, you can exit the Canvas quiz and come back. But once you submit it, you've submitted. Please remember to check the discussion board before sending an email or coming to office hours about something just because someone else might have had the question that you're asking. And it might just be right there. And I do just want to point out for this assignment and for future assignments, it's not really the point of office hours to come and check every single answer with us. We can't tell you if you're right exactly. We can only guide you towards the right answer if you're confused. So if you come to office hours and asking, is this right? Is this right? It's not quite what we're looking for because we want you to understand how to do this on your own. So thank you so much. Please go eat dinner or something. Thanks, guys. You just come up. Yeah. Thank you. All right, everyone, take a seat. I'm just going to give it to Evan. He's just going to give you some reminders for the week. I'm going to give it to Tristan quickly to introduce your next Assignment, and then we will get going for the day. Sweet. All right, everyone. Pretty straightforward stuff for me again this week. Last Thursday you have blog post 2, blog post 2, an Assignment 2, do. Thursday at 11.59 pm. And then next week there's blog post 3. That's February 9th. The midterms coming up the week after that. I'll have information on the midterms and like a whole slide deck with all the format and everything next week. We can go over it then. Also, if you're super concerned, you can email me in advance. I'll give you that all that information via text. Other than that, Tristan's got office hours. They're listed up here. These are also online in the Zoom section. Other than that, I'm going to hand it off to Tristan. He's going to talk a little bit about Assignment 2. I will not text here, yeah. Email me. Hey everyone, just a few brief introductory notes about Assignment 2. On the Canvas page there's a little introductory video. It walks you through all the different websites that you can be using for this assignment. It's pretty simple. The websites are mainly used to compare different map projections as a way to explore the different compromises that you have to make by using different projections. There's another website that lets you compare distance measurements to rum lines versus great circle distance. All of that is explained in this introductory video. My office hours are posted here in the Zoom tab in Canvas. The first one is tomorrow at 10 a.m. For questions, you can use the Assignment 2 discussion board. I'm going to be checking that every day. Or you can send me an email. I'll get back to you within 24 hours. But probably sooner I'm going to try to check that at least a few times a day. Okay, good luck with the assignment. Anyone got any questions for Tristan? That's the start of the assignment? Probably the last. No? Yeah? No, nothing is available from a site. Just like your comment. It'll be released with the Mark's first assignment one, which usually takes around two weeks to overtake a little bit. So somewhere around there. Yeah, almost done with assignment, but I just had to, like, there's some where it's like, is there any distortion, shape or size of this? Only very, very subtle. If there's a slight deviation, not preserving the shape or size. A good way is to look at the ArcGIS website, which gives you the official property. It gives you which parameters are distorted or not. If you want to email me the question, I can give you a more specific answer to your question as well. Yeah, for sure. Okay. Any other questions? No. Okay. Awesome. Thanks, man. All right. We have this lecture. I actually forget. Let me take a look real quick. We have this lecture, one more lecture, two more lectures after today, and then that will be the last lecture that's included on the midterm. So this is the third last lecture that's included on the midterm. And then we'll do one lecture that won't be included on the midterm, and then we'll do a review lecture, and then you'll have your midterm, and then we will get into post midterm content. But today we are talking about the history of the earth from space and Canada's role. There's three of them observing there. I was going to say the history of the earth from space. That sounds snarly. So we're talking about the history of earth observation. So it's a little bit, again, more history, similar to what we've talked about over the past, kind of a couple of weeks, couple of lectures. But today kind of focusing a little bit more on specifically earth observation, different earth observing systems, and a little bit of Canada's role specifically in earth observation, and some of the earth observation systems that Canada has that are analyzing data, collecting data, et cetera, et cetera. So today we're going to talk about history of earth observation. We're going to talk about four major earth observing systems in space. The four that we talk about, I'll mention this again when we get there, but we'll talk about them in a lot more detail after the midterm. So this kind of introduction to the major earth observing systems, Landsat, MODIS, ISAT, and WorldView. It's kind of just to introduce you guys, familiarize you guys with those earth observation systems, and then we'll talk about them and their applications a lot more post midterm. I'll make it clear as I talk what it is that you need to know for the midterm. And then we'll talk kind of lastly about Canada's contributions to earth observation from space and some of the key roles that Canada has played in those systems. So starting off, we're talking about earth observation from space. We're talking about essentially the first cameras that were launched into space to take photos of the earth. Now initially these were just cameras fixed to unmanned rockets. So these rockets that would just launch into space, kind of orbit the earth a little bit, take a couple photos. The first one, the first photo taken from space was from an American-owned V2 rocket. And these images here show New Mexico and the Gulf of California. It didn't get very high up. These rockets were still pretty new technologies back in the late 40s. The American V2 rocket was actually originally a German-owned rocket. It turned out that after World War II the Germans were much more advanced than where they had gotten with their rocket technologies than the Soviet Union than America. So naturally, after the World War II, America got their hands on some of these rockets and started experimenting around with their applications, started launching them into space, etc. So this was the first rocket that was launched that took images from space. So that's an important midterm question, maybe, possibly, who knows, probably will be, when was the first image taken from space or what decade at least was taken in the 40s, specifically in 1946? This was specifically the very first image taken of earth from space. Again, by that V2 rocket, and you can see here, you know, you can see maybe kind of depict a little bit of the curvature of the earth, but again, not much there in terms of detail, but very monumental in terms of the history of earth observation from space. Now, following the successful launch of rockets with a camera, attached to them, systems specifically designed for cameras were developed by the military. So the corona, the argon, and the lanyard programs were three of the first programs that were designed specifically to create these satellites that their main application was to go and take images of the earth from space. The corona one is probably the one that, you know, is most famous and that will talk about the most. So this is an image from corona of the pentacon back in the 60s. And, you know, back then, there was no remote transmittance of data or anything like that. These rockets would get launched into the sky. They'd kind of look like this. They'd get into probably approximately a low earth orbit. And they'd take photos. They'd take several photos on a roll of film. Once that film was full, it would be deployed from the rocket. It would get sent kind of back down to earth separately from, you know, what was now the satellite that was orbiting the earth. And there would be a parachute that would deploy. And it would just be kind of falling towards the earth. And then, you know, in this case corona, all of these were American systems. So the Americans would have jets or whatever that would go fly around, grab these falling parcels that had parachutes. They were just floating down to earth. They'd pick them up from the air, bring them back and analyze them. Now, sometimes those, you know, parcels of film would reach all the way to the ground, but there wasn't necessarily a good way to, you know, find them or to know where they fell on the ground. So sometimes the American government would just put out a really large reward and say, you know, we know it fell somewhere in South America or somewhere outside of our jurisdiction. We'll give someone a bunch of money if they return it to us. Yeah, yeah, yeah. The point is the film had to be deployed from the satellite, to come down to the earth or to the atmosphere and be floating in the air, and they had to go out and manually retrieve the film themselves in order to get those images. So that's how kind of the first true satellite programs worked. Now, we've come a long way from, you know, these first images of the earth that we got from space. This is a high resolution image from a satellite called Quickbird, which is one of the highest resolution satellite systems that we have available to us today for Earth observation. You can obviously see in comparison to this, we got a ton, ton more detail, much higher spatial resolution, much clearer image, et cetera, et cetera. So what is an Earth observing system? They are these essentially continuous data streams that provide observations of the earth. They're for monitoring a wide variety of things, but mainly the major systems of the earth, including the atmosphere, the biosphere, the cryosphere, the lithosphere, whatever it is on the surface of the earth, these systems help us monitor it by collecting data, mostly about the reflectance properties of those services. We'll talk a bit more about what that means in coming lectures, but essentially these earth observing systems just collect data about the earth from space. And part of their immense value is just having a really nice repeat, stable, and consistent measurement for different features of the earth. We can't track things like climate change and the effects of climate change if we don't have data that kind of explains to us what has happened in the past, what is currently happening, and then allows us to predict what might happen in the future. So my question to you is we know that from what I just kind of mentioned, back to these first satellite programs, corona, argon, land yard, these were all for military purposes, so they were never really for any civilian or research purposes. What do you think the first environmental application of images from space were? Why are you in brainstorm with someone sitting close to you? Brainstorm for a couple minutes, come back and I want to hear you guys' ideas of what you think the first, specifically environmental application of images from space might have been. Go ahead. Severnance. Any ideas? Anyone want to tell me what they thought of? The whole ozone layer? Yeah, good idea. Deforestation maybe? Yeah, another good idea? Hurricanes? Yeah, good idea. Anyone else? Sorry? I'm strong in all of the matches. Okay, yeah, yeah. Weather monitoring. Okay, yeah, yeah, yeah. Weather monitoring? Anyone else? Anyone else? Yeah? Sorry, say that again. Like terrain? Facilities? Yeah. Public terrain? Yeah. Things like that. Terrain facilities. Things like that. Yeah? Forest fires maybe? Forest fires, yeah. Melting ice caps? Melting ice caps, yeah. All really good ideas. Anyone else? Any ideas? Yeah? We're going to crops? Yeah, good idea. All really good ideas. All things that today, there's been some application of Earth observation data for. The first one, the first kind of environmental application of images from space was indeed weather, which I know somebody said. So the first earth observing systems in space, specifically for environmental applications, was weather satellites. So there was always this practical need to monitor weather patterns, predict our weather, understand storms, so that we could plan for crops, things like that. The first televised picture from space was this image here, which is aboard the Tyros weather satellite from 1960. And again, you can compare that to something that we have today, a nice really detailed image from, in this case, the GOES, series of satellites, or the geostationary operational environmental satellite. These are satellites that are in geostationary orbit, and so they provide this 24-7 coverage for a hemispherical side of the earth, which just means that they provide 24-7 coverage for the target or the portion of the earth that they're looking at. I'm getting confused between all my courses. We've talked about orbits, right? Yeah. So you guys remember what a geostationary orbit is? So it's orbiting at the same point, it's always looking at the same point on the surface of the earth. You can imagine for something like weather that's super, super useful, because that allows us to continuously track weather through time so that we can get really, really good estimates of weather forecasting, really good tracking of historical weather, something that's immensely valuable to us as a society. Stitch kind of all these images, these high-resolution images together, and we can get these really, really amazing HD videos, in this case, of something like a hurricane, images that, you know, back in the 40s, 50s, 60s, we kind of never really imagined we could get this far with. So we're not really going to talk that much about the GOES series of satellites, which is the kind of series of satellites I just mentioned, the weather satellites. In general throughout this course, we talk about four major earth observation programs, or four major satellite systems. One is MODIS, and I say in brackets, on board Terra and Aqua, because Terra and Aqua are kind of a brother, sister satellite. They're two different satellites, one's named Terra, one's named Aqua. That's the actual name of the satellite. Both of them have a MODIS sensor on board, which is the exact same sensor, or camera, if you will. But MODIS, on board the Terra and Aqua satellites, that's one satellite program we're going to talk about a lot throughout this course. Landsat is another one that we're going to talk about a lot throughout this course, and then Worldview and ISAT will talk about a bit, not in as quite much detail as Landsat and MODIS, but we will talk about them quite a bit. So MODIS is kind of our coarser resolution satellite. It takes images of the largest areas. Landsat is kind of a moderate resolution satellite. It takes pictures of regional areas, and then Worldview is kind of the highest resolution satellite system that we'll talk about in this course, and it takes satellite images with spatial resolutions or pixel sizes all the way down to 30 centimeters, which is really, really, really small, extremely detailed images and data. And then lastly, we'll talk about ISAT, which uses LIDAR. All I really expect you to know for midterm purposes about ISAT is what it images, and just that it uses LIDAR, we'll talk about what LIDAR is in a lot more detail later in the course. But I'm going to go kind of a brief overview of each of those now, and again, kind of highlight what I want you to know for the midterm, because we are going to talk about each of these satellite systems in much, much more detail following the midterm. How's my pace doing? Am I going too fast for anyone? Thumbs up. Blank faces. Sweet. Okay. Cool. So the first that we talk about is MODIS, which stands for moderate resolution imaging spectrometer. It's onboard the Terra and Aqua satellites. That's a really important distinction that a lot of students get confused. So there's two satellites, Terra and Aqua. They're pretty much the exact same. They both have a MODIS sensor onboard them. So if you say MODIS, you're really referring to both Terra and Aqua, which is fine. We'll mostly refer to that satellite system as MODIS. MODIS has 250 to 500 meter pixels for land research and 1,000 meter pixels for ocean and atmospheric research. And it has a one to two day return period, which just means that every one to two days MODIS is able to take an image of the entire Earth and subsequently of the same point on the surface of the Earth. So every one to two days MODIS is able to image, take an entire image of all of the Earth, which also means that every one to two days it will return back to the same spot on the surface of the Earth to re-image that same target or same area. Yeah. So we'll talk about what a pixel is in further or in coming lectures. So don't worry about it too much right now, not for the midterm at least. Essentially all it is is every image that exists, including satellite images, are made up of pixels, which are just kind of little squares that have some sort of color associated with them. So if we zoomed into this image I have up on the screen here, if we zoomed in and zoomed in and zoomed in, eventually you'd see pixels, right? So with a satellite, when it is taking an image of an area on the surface of the Earth, it has a given pixel size, which means that it has a given square area where it's going to measure the amount of reflectance of a certain type of electromagnetic radiation for a certain area, in this case for about 250 to 500 meters or 1000 meters. We'll get into what that means in a lot more detail in coming lectures. Essentially what it is is the smaller pixel size you get with a satellite image, the more detail you're able to make out in that image. So 250 to 500 meters or 1000 meter pixel size is pretty coarse. We don't get a ton of information from the MODIS satellite in terms of detail. But what is really amazing about the MODIS data is this one to two day return period, the ability to get an entire image for all of the surface of the Earth every one to two days. So for midterm purposes, all I expect you to know for the MODIS system is that it's onboard the Terra and Aqua satellites. It collects daily imagery and it represents a coarse scale of spatial information. So you're not going to get very zoomed in high resolution, high detail images of areas with MODIS. But what you do get is a very fine scale temporal level of information, which just means that every one to two days, every single day you're able to get information or data about a given point on the surface of the Earth, which is really, really really valuable in its own sense. Any questions about MODIS? Yeah? Is what depending on the orbit? Yeah, yeah. So the return period is partially dependent on the orbit, partially dependent on something called the SWAP width. Again, we will talk about that in a lot of detail in coming lectures. So I'm not going to go into detail about it right now, but we'll go over it in about three or four lectures. Okay. So when you look at Google Earth, when you're looking at an image of the entire globe, generally speaking, these are put together by MODIS imagery. Again, because MODIS just gives us that coarse level of spatial detail, allows us to get really nice images of the entire surface of the Earth. Okay. The next system that we talk about is Landsat. Landsat has a 30 meter pixel, so smaller than the 250 meter to 1000 meter pixel that we get with MODIS. That means that it allows us to get a little bit, relatively speaking, a little bit higher level of detail in those images. It has a 16 day return period as opposed to a one to two day return period that MODIS has. And it's a series of about eight satellites comprised of four different sensors. And again, we'll talk a bit about what those different sensors are and how they work in coming lectures. But what I want you to know for the midterm is essentially that Landsat is the longest collection of satellite imagery that we have through time. Landsat won the first Landsat satellite, launched in 1972, and operated until 1978. And so that means that for Landsat, we have imagery, we have data, dating all the way back to 1972. So that's 40 plus years of data that we have all from the same satellite. Now, the most recent satellite was Landsat 9, launched in 2021. You'll see, where did I say here? I said it's a series of eight satellites, but you'll say, well, what the heck Chris looks like there's nine. The reason I say eight here is because Landsat 6 here actually never made it to orbit. So it actually was never able to collect any data. It was launched, upon it's launched, it kind of failed, never made it to orbit, never collected any satellite imagery. So we actually have eight satellites from Landsat that were able to give us data starting from Landsat 1, then Landsat 2, 3, 4, 5, and then Landsat 7, 8, and 9. So if I were to ask you what the most recent Landsat satellite is, say Landsat 9, the two most recent satellites are Landsat 8 and Landsat 9. And the first Landsat satellite, Landsat 1, was launched in 1972. So the data set for Landsat goes back all the way to 1972. Might see a midterm question on it. Who knows? Okay. In terms of just kind of a summary, what I want you to remember about Landsat is the oldest program. So it dates back to 1972. The thematic mapper, which is kind of the modern era sensor on the Landsat satellites that we use in most applications today, dates back to 1982 and 84. I don't expect you to remember that part. Just know that the Landsat satellite system or data set dates all the way back to 1972. Just that it's a moderate scale of spatial information, and that allows for a fine to moderate level of temporal information. So 30 meter pixels, 16 day return time. Don't expect you to memorize those specific numbers. Just that Landsat is a moderate scale of spatial information and gives us a fine to moderate level of temporal information. Okay. Any questions about Landsat? We always say in this course, there's one thing. If you learn nothing from this course, you decide to just leave with one single thing. Just remember what Landsat is. This course is kind of a Landsat course, more or less. But we'll talk about Landsat a lot more in detail. We'll talk about its applications a lot more. But if you leave this course with one thing, remember what Landsat is. Any questions? Yeah? What is the what, sorry? Fine to moderate temporal info. So that just refers to how often we get an image of the entire surface of the Earth with that satellite system. So 16 days in this case means that every 16 days we get an image of the entire surface of the Earth. So fine to moderate temporal information just means that in terms of how frequent we can get information or how frequent we can get data of a certain area it's kind of fine to moderate. It's not quite coarse. It's not quite fine. An example of a data set that provides us a fine temporal resolution or a fine level of temporal information would be the modus satellite. That gives us that one to two day return period. Yeah? Well, like considering that it's revolving around the Earth, when it gets like a full image of the Earth, it's not all taken at the same time where it's taken over the course of a day or a full day. Yeah, yeah. So it's taken, it's taken in this case, it's taken 16 days. Oh, right. Yeah. Yeah. Okay. But for modus it only takes one to two days. All right. So when you're on Google Earth then because on Google Earth you can zoom all the way into like very specific areas. Is that through modus? No. Once you're, once you, with something like Google Earth where you have satellite imagery, kind of at different scales depending on how far you're zooming in. Is this all of them? Yeah, essentially. So as you zoom in, they will kind of overlay finer and finer spatial resolution satellite imagery. Wow. It's just like stitched altogether. Yeah. Yeah. Yeah. Yeah. Correct. Correct. Yeah. Yeah. Yeah. So 35 is moderate. What's an example of a non-expression? Fine. Fine spatial resolution. That would be something, again, we will define those terms, how we talk, you know, we'll define the exact pixel size of what classifies a low versus moderate versus high resolution sensor in the resolutions lecture, which is about three lectures from now, I think. Generally speaking, high resolution sensors are anything that's low versus high resolution or anything that's lower than a meter. So in that case, that transitions nicely into this, which is the worldview constellation of satellites. This is by the worldview satellites are owned by this company called Digital Globe. It's one of the largest and most well-known private satellite companies, which has launched six high spatial resolution satellites. And these would kind of classify as your high spatial resolution satellites. In some cases, they have pixels with a size all the way down to 30 centimeters. So literally, you can see things that are all the way down to about that big, which is pretty crazy. Note, though, that with these high spatial resolution satellites, generally speaking, they're all private, which just means that you all have to pay an exorbitant amount of money for a license in order to use any of the high spatial resolution data you might get from worldview or from other high resolution satellites. For Landsat and MODIS, they are both US-owned and they are both generally speaking open source, which just means that any of the MODIS and Landsat data, anybody can access completely for free. High resolution satellites like worldview cost a lot of money. So, finest scale spatial information you can get from worldview and is private. These kind of summary slides, after I talk a little bit in detail about each of these systems, these quick summary slides are really all I want you to remember about those satellites for the midterm purpose. So, you know, that is this one, MODIS, Daily Imagery, core scale spatial information, Landsat, oh, this one, LANDSAT, oldest program, moderate scale spatial information, and this one, worldview, find a scale spatial information, private. Anyone recognize this image? Was that? Yeah, exactly. That's the Suez Canal when it got stuck. Yeah, fun stuff. Yeah, yeah, it's from worldview. Yeah, yeah. Yeah, someone probably paid for this image. I mean, in certain cases, you know, worldview is a private company, so they kind of have access to their own data. They might have said, oh, this is like a really, you know, crazy photo. It'll get us a bunch of public attention. Yada, yada, yada. So, they might have just posted it themselves and just said, look at this. Yeah, our data is awesome kind of thing. So, are there like rules and regulations for them? Like the government needed theirs to be like something or maybe forced to? That's a great question. Generally speaking, you know, because we'll talk about that kind of a topic we'll cover later in the course, there aren't a ton of, with things like Landsat and with MODIS, where they're kind of moderate to core spatial resolution. There's international agreements whereby it's legal for images to be taken all around the earth and they're open source so anyone can access them. With these high resolution satellites, it becomes a bit more tricky like you're saying. So, there's lots of sensitive areas where information isn't let out to the public. So, company like worldview has to have agreements with different governments, et cetera, in order to kind of tackle that issue. Generally speaking, if the US government or whoever wanted satellite imagery from worldview, they would just pay for it. And like, I don't know which country worldview is from, but it's not the country. So, if the states was at war with a country that was using worldview, could they stop worldview from licensing that information? Maybe, I mean, I think it would be difficult, probably, I would guess, but, you know, it's one of those issues that's tough because we're talking about sensitive areas that are in different political jurisdictions, but space itself and satellite imagery itself is still relatively unregulated despite kind of the really high detail information and images you can get from it. So, I'm honestly not sure up top my head. Yeah. Like, out of this relevant, what would it just be a lot easier if the US government just nationalized worldview and like NASA bought it or something? Yes, that would be easier. I'm not sure exactly, because the big obstacle to that is they don't want to pay millions and millions and millions and millions and millions of dollars to buy this company, right? Like, that's still the issue there, because, you know, that's still taxpayer money. And this is still a, you know, a private enterprise. So, you know, technically, yes, there is kind of this, you know, thought to that it's better off the high. The thing to note about, I guess, in general the high spatial resolution data, these images that give you really, really high amounts of detail, they're still really new, and so they don't necessarily give you the highest quality level data. They give you, you know, really high spatial information, but one of the nicest things about Landsat and MODIS is just how high quality those data sets are and how high quality the imagery is. I've worked with some high spatial resolution imagery before, and it's riddled with errors and noise and all kinds of stuff that Landsat and MODIS has just kind of perfected and doesn't really have as much of an issue with. So, it's not quite a perfect product yet, I guess, is what I'm saying. Yeah. So, that's a part of it, too. Any other questions? Sweet. Okay. The last program that we'll talk about is the ISAP program, which just stands for ice cloud and land elevation satellite. They really just decided to omit the L from their acronym. I guess ISAP sounds better. It was originally designed for imaging ice, so they wanted to call it ISAP, fair enough. It has a 70 meter footprint, so that's related to something called LIDAR, which, again, we haven't talked about yet, so I don't expect you to know what that means, but it essentially is this space-borne laser-ranging system, which just means that it travels over the surface of the Earth and it pulses down lasers and individual kind of beams, and then those laser beams reflect off the surface of the Earth, and ISAP is able to determine how far away that target is from where the satellite is, and by doing that can build up kind of 3D topographical models of what the surface of the Earth looks like. We'll talk about LIDAR and how ISAP works and how LIDAR works in general and much, much, much more detail later in the course. So don't worry about it too much for now. All I want you to remember is that ISAP uses LIDAR. That's it. We'll talk about what LIDAR is later, and for now, just know that ISAP is mostly used to image or to monitor ice clouds and measure elevation. Okay. Like I said, we've talked about, I kind of threw around a lot of terminology there, that might be new to you, that maybe we haven't talked about yet. For example, you know, MODIS having different pixel sizes, just what pixels are in general. Talked about ISAP, the footprint that it has. Talked about LIDAR. Again, just hold off on worrying about those topics too much. We're going to talk about them in a lot more detail in about three lectures and then also just in a lot more detail throughout the course. So, hopefully I've made it pretty clear what you need to know about those systems for the mid-term. It's not too much. Just kind of trying to introduce you to them. How much they cost, just know that MODIS and LIDAR is free, just know that world view is expensive, it's not free, and then just kind of generally the level of spatial and temporal information you can get from each. That's it. Okay. Now, we are going to transition to Canada's role in Earth observing systems. So, Canada, as a country, is very large, it's very vast, has a lot of resources. It contains about 7% of the Earth's renewable fresh water. And so, there is this underlying need to monitor resources and to monitor them efficiently, repeatedly and constantly to make sure that we are conserving them, managing them properly, managing them sustainably and responsibly and things of that nature. So, we will talk about four kind of five different contributions to Earth observation from space that Canada has contributed. One being astronauts, so we will talk about Chris Field and Roberta Bondar. And then we will talk about radar sat, which is a radar satellite that is owned and operated by Canada. We will talk about the Canada arm and we will talk about Earth cast. So, Chris Hadfield took over the International Space Station in 2012. He became the first Canadian to command the International Space Station ever, came back to Canada and about or back from space in 2013. And he took, as we learned, if you guys remember, back to our introduction lecture with that video from Don Pettit, one of the astronauts' jobs is to take photography and take images from the International Space Station of space of Earth, et cetera. So, he took a ton of images, over 45,000 while he was in space. He was the first Canadian to walk in space. He actually, in 2001, helped install the Canada Arm II to the International Space Station. We will talk about what the Canada Arm II was in a second. There was also Dr. Roberta Bondar. She was Canada's first female astronaut. She was also the first neuroscientist to be launched into space. Her main research focus was the health sciences, so a lot of the research she did was studying the effects of microgravity on the human body, which is essentially just the study of when you're in space, when you're in the International Space Station, there obviously isn't much gravity going on. They call it microgravity because technically there's a tiny little bit because gravity is what's holding the International Space Station in its orbit close to Earth. So, they call it microgravity, but she was just studying the effects of microgravity on the human body. Then we had the Canada Arm I and Canada Arm II, which we can often see on our bills. Canada Arm I was a Canada Arm, was this robotic arm that was put on space shuttles, so they started being put on space shuttles in 81. Canada Arm II was a second iteration of the first Canada Arm, and it was mounted permanently directly on the International Space Station. So, there were several Canada Arm I's, each of them were kind of just attached to different space shuttles. There's only one Canada Arm II, and it was permanently mounted on the International Space Station. So, in 1969, Canada had this agreement with NASA to contribute to the space shuttle mission, and it just agreed to deliver, create, and kind of produce these robotic arms called the Canada Arm, the first one was delivered in 1981. There were five Canada Arms, in this case I'm referring to just Canada Arm I's, so there were five Canada Arm I's that were delivered, each costed around $100 million Canadian, and the last Canada Arm I left the space station in 2011. So, literally how it worked was these robotic arms were just mounted onto a space shuttle. That space shuttle would be launched from the surface of the Earth out to the International Space Station. It would dock on the International Space Station, and then they could use the robotic arm, the Canada Arm I, that was attached to the space shuttle to kind of do maintenance or other things onboard or outside the International Space Station. But that just meant that every time the Canada Arm I came to the International Space Station, it would have to leave eventually, that space shuttle that it was mounted to would eventually leave, travel back to the surface of the Earth, and thus so would the Canada Arm that was mounted to it. So eventually they said, alright, well we want a Canada Arm that's permanently mounted on the International Space Station, and that was Canada Arm II. So, Canada Arm II was a second generation arm mounted on the space station permanently in 2001. So, there were several Canada Arm I's. These were all fixed to different space shuttles. The last Canada Arm I left the space station in 2011, but there was only one Canada Arm II, and it's been permanently mounted on the International Space Station since 2001. Make sense? Any questions? Anybody? Okay. The next kind of contribution we're going to talk about is radar sat. So radar sat is a, there's been a variety of radar sat launched, but it is this radar-based satellite system that is designed to measure and monitor ice, winds, oil pollution, ships, identifying track, disasters, and monitor ecosystems. So radar sat one and two were the first radar sat that were launched. The first operational civilian radar satellite was launched in November 1995. That was radar sat one, and then radar sat two was launched in December of 2007. It used a sea-band radar. Don't expect you to know what that is. We'll talk about it later in the course, but essentially sea-band just represents the specific wavelength sizes of radio or microwaves that the radar sat was using in order to image the Earth. Again, don't worry about that. We'll talk about it in a lot more detail. It had kind of a moderate to find spatial resolution of about eight to a hundred meters. And what's super valuable, what was probably the most valuable thing about radar sat, was that it was able to see through clouds. So no matter what the weather conditions were, no matter if it was day or night, whatever it was, radar sat was always able to collect data and collect images of the surface of the Earth. So this is what an image of Canada looks like derived from radar sat, radar sat one in this case. You can see kind of a lot of the ice sheets and things like that that are up near the Arctic, a pure lot brighter, as well as kind of a lot of the mountains areas that have a lot of snow and that kind of thing. But that's what a radar sat image looks like. One of the really valuable things about radar sat, and actually what it was the first satellite to do, was create the best map that we have had available to us of the Antarctic. So this is what the Antarctic looks like. And this was derived from radar sat one, and it was the first instrument that was able to go out and collect a kind of full, pretty detailed map of what Antarctica looked like. So that's kind of what it looked like there. Okay, this is a video just kind of about radar sat, some of its applications. In December 2007, Canada's Earth observations underway, radar sat two, was launched into space. Keep a hold of standing near at all times, day or night, through any other conditions. This is relatively acquired to an 30,000 in just a year. These images are used by research centers, private industries, and government departments across the country and around the world. Information may provide these for advanced applications. From the open monitor pushing activities on our coast to increasing our agriculture's profitability and sustainability. Radar sat two technology, who used to monitor one's lightness, along strategic transportation and energy corridors, fitting our country in use to protect critical infrastructure. We have a deal of this data and we have a real time. It finishes our offices to help coordinate our strategic playground, following our disasters, making radar sat two essential to lower bikes, communities, and environment areas to make. Radar sat two collects critical information on the road to what an extensive area is, helping ships navigate a city that you can make in Mars, and enabling modern communities to find safer routes, professions, and health conditions. And space technologies developed to establish various solutions to your challenges. The Canadian Space Agency is committed to fostering innovation with benefit of all Canadians. The Canadian Space Agency, thinking outside the road. Okay, so that was kind of about radar sat two. There's a third radar sat that is going to be launched soon, or was already launched, actually, sorry. And it is called the radar stack constellation. So it is, instead of one single satellite, it is three identical smaller satellites that altogether have, each one has a slightly finer resolution, finer spatial resolution, than those radar sat one and radar sat two satellites. But it's three identical satellites. They're all kind of staggered in orbit, so they allow us to take more images at a greater frequency. And they were launched by SpaceX from California in February of 2019. This is a video of just them getting launched. They love anything space. They love the dramatic music, so we're going to have to get used to that. So those are the satellites right there. You'll see each of them get detached. So there's the first one going, right there. And the second one. And the third one. So that's the radar stack constellation. It's three identical satellites all kind of staggered around Earth. The last application that we're going to talk about is Earthcast. So Earthcast has launched a video camera that operates on the International Space Station. It's the first high res HD video from space. And the camera allows tracking of objects on the Earth's surface. So things like cars, boats, things that are moving. It's actually a Vancouver based Canadian Earth observation company, but it's now called Earth Daily. So the video that I have about it says Earthcast, but it's called Earth Daily now after they went bankrupt. That's what it's called. It's just a fancy word for they went bankrupt and someone bailed them out. But they're not called Earth Daily, but really, really cool kind of videos that they were able to collect of the service of the Earth. So you can see all the cars and stuff moving. There's some boats. There's a lot of boats. What's the, anyone remember what's the space movie, name is Space Movie with Matthew McConney? Every single video has an interstellar kind of soundtrack. That's about space or about satellites or whatever it is. But it is really cool. The data it's able to collect. So one of the things that I noted there, I don't know if you guys caught that, but it's only able to look over a certain area and kind of monitor it, collect video data for about a minute. And that's because the space station is moving, right? So this kind of shows how that works. So the space station is kind of traveling over the surface of the Earth. It's got its video camera, the Earth cast. It's kind of pointed down towards the surface of the Earth. And it gets angled. So it'll kind of point at a target, boom down here, and then just consistently look at that target for a little bit. And that's what it would actually look like, the angle that it was looking at would kind of change as it flies over. So it would look kind of just like that. But you also probably noticed that in that last video, you know, you saw things like cars and boats and whatnot that were moving, but the image below kind of looked stable. It didn't really look like the International Space Station or the Earth cast video camera was kind of changing its angle at all. So how that works, essentially what Earth cast does is it takes a bunch of kind of reference satellite imagery that's static and then kind of superimposes it below the video data that it's collecting so that, you know, things that are moving, like cars like boats are really just superimposed over a different satellite image that's static, that's still so that you can get kind of a more realistic looking image of or video of a certain area over about, I think it said over about 60 seconds or so. Any questions about that? About those. So you'll definitely see a midterm question that will ask you something along the lines of, you know, name one of the Canadian contributions to Earth observation from space that we talked about in class. So there's really five that we talked about. We talked about Roberta, we talked about Chris, we talked about not me. I'm astronaut Chris. We talked about Earth cast. We talked about radar sat and we talked about the Canada arm. So one of those you'll want to discuss to answer that question. Now, like I always do, a bit of a shorter lecture today. I got some practice questions. So go over these by yourself with something sitting next to you. If you'd like to stay and go over them, you're welcome to. I'll give you about five minutes to go over them. If you'd like to, then you can head out and I'll see you tomorrow in Woodward. Hi. I was just wondering that all the lands that are the all part of space junk now, or do they return back after like, space junk? Space junk, are they still like orbiting? Yeah, for the most part. Did you know that we could see satellites from our naked eye? Yeah. I witnessed some satellites. You know, when you go to Marine Drive and there's like no light at all the supplies. Yeah. I could see like, really tiny satellites just moving in. Yeah. You know, like, you can open your phone and like, there's an app where you can like monitor and what is going. Yeah. So there's different kinds of satellites. I thought it was pretty cool. Yeah. Yeah, totally. You're definitely able to view them from just your naked eye if you're just standing at a right and they're passing over totally. Yeah. Thank you. Cool. Yeah. No worries. Hey. Hey. How's it going? So, I was just like, I just want to get the chart done so more. Yeah, sure. And then, yeah. Yeah. And then, this is the ice side. You know, you know, you don't even need to put an ice set on that list, honestly. I think ice set is free, but we don't really talk about it. It's kind of a, it's own category because it's not a form of spectral or optical or passive remote sensing. It's really a kind of a different kind of data than these three. So just in terms of knowing what's free and what's not, just know that modus and land sense free and world view is not a good concept. Yeah. No problem. Hey. Where does love mean start? Footprint. Like, um, the set. Footprint. Yeah. So we didn't really, we didn't talk about it and I don't expect you to know what it means. But essentially, what it is, is, um, ice set is a lidar instrument, which just means that it sends laser pulses to the ground. Yeah. The size of the laser pulse, like the area that the laser pulse covers on the surface of the ground is about 70 meters. Oh, okay. So that's kind of what that is. Yeah. Yeah, no problem. Hey. I was wondering, um, for the world view satellite, this is just interesting. Yeah. Um, would a place like a concentration can be formed, like the makers in China or something like that, I know there was a lot of controversy about getting photos of that. Yeah. Would world view have gotten to the government of China and we're not going to run these photos of that or whatever? Um, probably, I mean, so the thing that's kind of unique about the world view satellites is they are able to point to kind of target what they're going to take an image of. So they kind of, as they're, yeah, as they're passing over areas on the surface of the earth, they kind of choose what they're, what they're looking at. Like, whereas Landsat and modus it is, Landsat and modus is everything. Exactly. But for those finer resolution satellites, they are typically kind of pointing at, um, you know, what area on the earth that they're looking at. So they probably, you know, there's international agreements, international laws that would probably limit them from collecting data on certain things in certain areas. But, um, yeah, does that answer your question? That's correct. Thanks so much. Yeah. Hey. Longest operating with observation satellite machine. Would that one be like Landsat? Yeah. Oh, I thought it was referencing like the longest time to take the photos. No, no, that's how, that's just how long it's been operating. Not the, not the, sorry, pardon me, not the revisit time. That sounds like a really good thing. Yeah, probably. Yeah. Yeah. Yeah. Yeah. It was static. How like your data passed to a static and you realized that different reference points are known as the greatest static image. Yeah. So does it actually launch multiple satellites into space? No, so it's, so it's using satellite imagery from a different satellite. Oh, so it's, or, or imagery that's just collected from a different camera or different sensor that's on the International Space Station. So how do they have authorization to actually use other satellites? Because Landsat's modus data is free, an open source. So they can use whatever they want in terms of those two data sets. If they want a higher resolution imagery, like worldview, for example, then they would just pay for it. So they're all attached to an international fashion space station? No. No, only Earthcast. Worldview Landsat modus, they're all completely separate satellites. Yeah. They're just, they just take satellite imagery and then based off the location that they take that satellite imagery, the Earthcast can say, okay, we're taking video over the exact same area, so we'll superimpose it on the image that, say, Landsat is taken because we know it's the same area. Oh, so as long as it doesn't really pivot to an extreme degree or something, it can capture something else. Yeah, it has like a, they have kind of advanced algorithms that correct for the different angles that, that they use to look at the surface of the Earth as they're passing over for the case of Earthcast. For Landsat and modus, they're pretty much always looking straight down. So you never really get like a situation where they're looking at, yeah, exactly. You never get a situation where they're kind of looking obliquely or anything like that. Thank you. Yeah, thank you. Yep, no problem. Okay, guys, let's go over this and I'll get you guys out of here. So, what is the significance of the V2 rocket? Can anyone think way back to about 50 minutes ago? Yeah. Guys, pardon me. Quietly. Thank you. Yeah. Yeah, I don't think it was 63, but it is the first image. 40. Exactly, yeah, but first image of the Earth from space. Yeah, exactly. You got it, 1946. What was the first environmental application of Earth observing satellites? Weather monitoring, yeah. How often does modus image the Earth? Everyone to two days. What is the longest operating Earth observation satellite mission? Yeah. Landsat, yeah, and then lastly, what's Earthcast and why is it significant? Yep. And it's a national privilege, probably that launch the high-end video camera that operates on the international space station which often backwards into pictures of the Earth and the Earth. Yeah, yeah, that's exactly right. It's a Vancouver based company. They launched a high-res HD video camera that's mounted on the international space station so they're able to take high-res HD videos from space. Cool. That's it for me, guys. See you tomorrow. Have a good week. Have a good day. Good evening. All that stuff. Good evening. All right. Hi, everybody. We can get started here. I just had a question for you guys first. Has anyone started or submitted their assignment to? Anyone by chance? No? Okay. Shouldn't be a problem then. Okay. I'm not going to. There was a typo in the canvas quiz submission for the Assignment which we just fixed. So if you haven't opened it yet, then it doesn't matter. So I was checking if anyone had opened it. It doesn't seem like it. Okay. So today we are talking about the electromagnetic spectrum. We're going to be talking about what the electromagnetic spectrum is, the different wavelengths in the electromagnetic spectrum, the ones that we use to observe the earth, the ones that we don't use to observe the earth as much. And we're going to want to be able to describe essentially the Different types of radiations that are emitted from the sun and That we use to ultimately go away and observe the surface of The earth. Wow. There's a ton of movement going on. Settle down, guys. Easy peasy. I'm in squeezy. You guys okay? All right. Okay, cool. Sweet. So. I got to get you guys to quiet down for me. I apologize. I got to do that. But it's really hard for me to focus. Guys in the back right there, I can hear you all the way down Here. Still talking, buddy, please. Hey. Thank you. All right. Awesome. So. We are going to be talking about radiation pretty much all Day today or for the whole lecture. The fundamental unit of radiation is something called the photon. Photons are released from objects when matter is either excited Thermally so when things warm up or when they're engaged in Some sort of nuclear process, whether that's fusion or fission. Photons are emitted from the sun. So that's radiation. Photons are just the fundamental unit of radiation. Photons are emitted from the sun and then they travel towards The earth where they'll hit the atmosphere or hit the surface of The earth and be absorbed or reflected or transmitted by matter. The speed of photons in a vacuum is 3.0. Times 10 to the 8 meters per second. So all that means is that a photon is essentially a type of Light or the fundamental unit of light. Light is radiation. All of that radiation, all of those photons no matter what kind Of light it is, no matter what kind of radiation it is, all Travels at the exact same speed of 3.0 times 10 to the 8 meters per second. Photons all travel at the exact same speed no matter what. Any kind of radiation, any kind of electromagnetic radiation Travels at the exact same speed no matter what and that speed is 3.0 times 10 to the 8 meters per second. It's often denoted as C. It's the speed of light. It's really fast. Now depending on the type of wave or type of wavelength or Length of wavelength type of radiation that you're talking about That you're referring to. There might be a different level of energy associated with it. So that just means that again the speed of light, the speed of That radiation at which it's traveling at will always be the same. It's always 3.0 times 10 to the 8 meters per second. But they have different energies related to their wavelengths. So there might be more energy or less energy involved or Associated with that radiation depending on the wavelength, depending on The size of the wavelength at which those photons are traveling at. So photons, even though they're this fundamental unit of radiation, They're particles and they're waves. And that's kind of the basis of the dual nature of light and of radiation Is that they can be measured with as particles. They can be also measured as waves. Generally speaking, we're going to be considering them and measuring them In this course as waves because it's the wave properties of radiation That are important to us when we're trying to use them to observe the earth. So, pretty much all of the radiation, all of the light That we use to observe the earth comes from the sun. So the sun is about 109 times bigger than the earth makes up in mass Nearly 100% over 99% of the total mass of the solar system. It's really hot, about 5800 Kelvin. And the energy created from the sun or on the sun is from this Nuclear fusion of hydrogen into helium. All that matters about the sun for our purposes, for earth observation, For remote sensing, for this course, is that the sun emits Essentially every part or every type of electromagnetic radiation. So this EMS here just stands for electromagnetic spectrum. Now, generally speaking, we relate frequency and wavelength By this equation here, c equals lambda times v. C here is that speed of light, so that's that 3.0 times 10 to the 8 meters per second. Lambda here is our wavelength size and v here is our frequency. All that's important to remember about this equation is that there is a proportional Relationship between wavelength size and frequency. Because this value here, c, will never change no matter what. It's always 3.0 times 10 to the 8 meters per second. If you have a lambda value that's increasing, then your v value here, Your frequency has to decrease, and the same vice versa. So that means that wavelengths that are larger have a lower frequency, While wavelengths that are smaller have a higher frequency. Now we measure wavelength in order to understand just what a wavelength is. We measure wavelength by just measuring the distance from one peak, The peak of one wave to the peak of an adjacent wave peak. Sorry, I didn't say that well. You measure your wavelength from the peak here to the adjacent peak here of a given wave. So you can see here you got a relatively longer wavelength here And a relatively shorter wavelength here. And this is how radiation travels. This is how light travels. This is how all kinds of electromagnetic radiation travel. They all travel as waves. You can measure them as waves. And thus you can measure their wavelength. So you can measure the length or the distance from a peak of the wave to the adjacent peak. You can also do the same with the trough of the wave. So from the bottom of the wave here to the adjacent trough here. To do the same thing. Now frequency, the definition of frequency is just the number of waves Or the number of wave peaks that pass by a certain point in a given amount of time. Generally in a second. So if you go back to this equation, c equals lambda times v. C here never changes, right? That speed is always the same. So you can see here if these two waves here, this one that's got the longer wavelength, This one that's got the shorter wavelength. They're both traveling at the exact same speed. And you measure, say at this point right here and this point right here, How many individual peaks of waves travel by that one point? You can see here for the one with shorter wavelengths. There's going to be more waves traveling by this point than there's going to be for this wavelength. This longer wavelength here, right? Does that make sense to you guys to see what that is? They're both traveling at the same speed. There's going to be more of these peaks that pass by a given point than for this one here. Because it's traveling at the same speed, but there's less peaks. That's what frequency is. So inevitably, if we have a smaller wavelength here, We're going to have a higher frequency. More of these individual wave peaks are going to pass by a given point Because it's traveling at the exact same speed as this wave here with a slightly longer wavelength. Now these different sized wavelengths and the different frequencies associated with those wavelengths Are essentially how we define and describe what's called the electromagnetic spectrum. The electromagnetic spectrum is essentially a spectrum that describes all of the different kinds Of wavelengths and thus all the different kinds of radiation that we know that exist. All of those different kinds of wavelengths, all of those different kinds of radiation Are emitted from the sun and that's ultimately what we use to measure and observe the earth. They come from the sun, they pass through the atmosphere, they bounce off or are reflected by the surface of the earth And that's what we use in earth observation to measure. Now essentially what we're doing for the rest of class today, I'll first introduce the electromagnetic spectrum a little bit more And then we'll talk about each of the kind of broad categories of different types of radiation Which are categorized by wavelength size. And we'll talk about what they're used for, some properties of them. If they're important or not for earth observation, etc. So it's a pretty today's lecture, it's pretty physicsy. I'll try to make it really clear what you need to know for the midterm. I'm not going to expect you guys to do any calculations or anything like that using this equation. But I do want you to be able to describe the relationship between wavelength and frequency, Which is just that longer wavelength, longer wavelength here, lower frequency, smaller wavelength here, higher frequency. I want you to be able to describe that relationship. But beyond that, I'm not going to ask you to use this to calculate anything. I will want you to understand the different types of radiation across the electromagnetic spectrum. There are approximate wavelength sizes and some rough, you know, some kind of easy background info about their properties. Which we'll go over in detail. So first, I'm going to play a video just introducing the electromagnetic spectrum as a whole. I'll go over it then and I'll highlight kind of some important points from that video. And we'll talk about each of the different types of radiation across the spectrum, one by one by one. So, we'll start with the video here. A lot of videos today. So you don't listen to me too much. Something surrounds you. How about you? Some of which you can't see. You can touch or even feel every day. Everywhere you go. It is odorless and tasteless. Yet you use it and depend on it every hour of every day. Without it, the world you know could not exist. What is it? Electromagnetic radiation. These waves spread across the spectrum from very short gamma rays to x-rays, ultraviolet rays, visible light waves, even longer infrared waves, microwaves, to radio waves which can measure longer than a mountain range. This spectrum is the foundation of the information age and of our modern world. Your radio, remote control, text message, television, microwave oven, even a doctor's x-ray, all depend on waves within the electromagnetic spectrum. Electromagnetic waves, or EM waves, are similar to ocean waves and that both are energy waves. They transmit energy. EM waves are produced by the vibration of charged particles and have electrical and magnetic properties. But unlike ocean waves that require water, EM waves travel through the vacuum of space at the constant speed of light. EM waves have crests and troughs like ocean waves. The distance between crests is the wavelength. While some EM wavelengths are very long and are measured in meters, many are tiny and are measured in billions of a meter, nanometers. The number of these crests that pass at an endpoint within one second is described as the frequency of the wave. One wave, or cycle, per second is called a hertz. Log EM waves, such as radio waves, have the lowest frequency and carry less energy. Adding energy increases the frequency of the wave and makes the wavelength shorter. Gamma rays are the shortest, highest energy waves in the spectrum. So, as you sit watching TV, not only are there visible light waves from the TV striking your eyes, but also radio waves transmitting from a nearby station and microwaves carrying cell phone calls and text messages, and waves from your neighbor's Wi-Fi and GPS units in the cars driving by. There is a chaos of waves from all across the spectrum passing through your room right now. With all these waves around you, how can you possibly watch your TV show? Similar to tuning a radio to a specific radio station, our eyes are tuned to a specific region of the EM spectrum and can detect energy with wavelengths from 400 to 700 nanometers, the visible light region of the spectrum. Objects appear to have color because EM waves interact with their molecules. Some wavelengths in the visible spectrum are reflected and other wavelengths are absorbed. This leaf looks green because EM waves interact with the chlorophyll molecules, waves between 492 and 577 nanometers in length are reflected, and our eye interprets this as the leaf being green. Our eyes see the leaf as green, but cannot tell us anything about how the leaf reflects ultraviolet, microwave, or infrared waves. To learn more about the world around us, scientists and engineers have devised ways to enable us to see beyond that sliver of the EM spectrum called visible light. Data from multiple wavelengths helps scientists study all kinds of amazing phenomena on Earth from seasonal change to specific habitats. Everything around us emits, reflects, and absorbs EM radiation differently based on its composition. A graph showing these interactions across the region of the EM spectrum is called a spectral signature. Characteristic patterns like fingerprints within the spectrum allow astronomers to identify an object's chemical composition and to determine such physical properties as temperature and density. NASA's Spitzer Space Telescope observed the presence of water and organic molecules in a galaxy 3.2 billion light years away. Viewing our Sun in multiple wavelengths with the Soho satellite allows scientists to study and understand sunspots that are associated with the light. That are associated with solar flares and eruptions harmful to satellites, astronauts, and communications here on Earth. We are constantly learning more about our world and universe by taking advantage of the unique information contained in the different waves across the EM spectrum. OK, so quick summary, wavelengths range in size from radio waves which are the longest down at this end of the spectrum, and they can be several meters long to kilometers long. We have visible light in the middle part of the spectrum which we typically measure in microns or millions of a meter. We have all the way down here gamma rays which we typically measure with angstroms, a unit that is about 10 to the power of negative 10 meters. These wavelengths down here, the gamma rays, they have the smallest or shortest wavelengths. Radio wave over here have the largest wavelengths, and thus inherently gamma rays also have the greatest frequency. Radio waves have the lowest frequency. Side note, not side note I guess, but important note that frequency is also directly proportional to energy. So these gamma rays here, they have the most energy associated with them because they have the highest frequency. These radio waves down here, they have the lowest energy associated with them because they have the longest wavelengths. You can see that here, frequency is given in Hertz, lowest here, lowest frequency, slowly increases, slowly increases, slowly increases, and then our wavelength size here, smallest here, increases. Now I generally speaking will want you guys to remember for midterm purposes what or where these different classes of wavelengths or of radiation occur on the spectrum. So if I say, you know, what's the longest wavelength, I want you to know that it's radio waves. If I say which wavelengths have the greatest amount of energy or the highest frequency, I'd want you to be able to identify gamma rays. If I said where do UV rays, ultraviolet radiation, where does it lie on the electromagnetic spectrum, I'd want you to be able to identify that is between visible and x ray. Those are the kind of questions you can expect about where things are on the electromagnetic spectrum. Any questions about this? Okay, sweet. So now we are going to talk in a bit more detail about each of these different kinds of wavelengths or each of these different kinds of radiation. So we're going to start with radio waves. Each of these videos that I play are from NASA. We're going to watch about a three minute video or so and then I'll kind of overview slash highlight what I'll actually want you to know or remember for midterm purposes. Is it too loud or too quiet, by the way? Thumbs up. Good. Okay. Who yell on Marconi's first radio transmissions in 1894 have spread in the space for over 100 years at the speed of light. They passed Sirius in 1903. They got in 1919. And regular in 1971. That signal has already passed over 1000 stars. Anyone orbiting one of those stars with a really good receiver could detect Marconi's signal and know that we are here. Radio waves are the longest and contain the least energy of any electromagnetic wave. While visible light is measured in minute fractions of an inch. Radio waves vary from about 19 centimeters about the length of a water bottle. Two waves of the length of cars, ships, mountains, all the way up to monstrous waves longer than the diameter of our planet. Heinrich Hertz discovered radio waves in 1888. The first commercial radio station went on the air in Pittsburgh, Pennsylvania on November 2nd, 1920. Then in 1932, a major discovery by Carl Jansky at Bell Labs revealed the stars and other objects in space, radiated radio waves. Radio astronomy was born. However, scientists need giant antennas to detect weak, long wavelength radio waves from space. The enormous Arasibo radio dish antenna measures 305 meters in diameter over three football fields. Scientists can link the signals from an array of separate radio antennas to focus on tiny slices of distant space. Such arrays act as a single, immense collector. This giant New Mexico array uses 27 marimolic dish antennas shaped into a giant Y with each are capable of stretching for 13 miles. Scientists have even spread these linked antennas across the globe. One of the largest stretches from Hawaii to the Virgin Islands and acts like such a powerful telephone lens that a baseball sitting on the moon would fill its entire field of view. Many of the greatest astronomical discoveries have been made using radio waves, pulsars, the existence of giant clouds of superheated plasma, which are among the largest objects in the universe, and even quasars, such as this one over 10 billion light years away, were all discovered using radio waves. Radio waves also provide more local information, astronomical objects that have a magnetic field usually produce radio waves, such as our sun. Thus, NASA's stereo satellite is able to monitor bursts of radio waves from the sun's corona. Wave sensors on the wind spacecraft report the radio waves emitted by a planet ionosphere, such as the bursts from Jupiter, whose wavelength measures about 15 meters. Radio waves fill the space around us to bring entertainment, communications, and key scientific information. Instead of here these radio waves, link to your radio to your favorite station. The radio receives these electromagnetic radio waves and then vibrates a speaker to create the sound waves we hear. We may not be able to tap our toes to the cosmic radio transmissions, but we certainly discovered much of that our universe's brand cosmic dance by listening to that. Okay, radio waves summary, most importantly, kind of their most important or notable property, is that radio waves have the longest wavelengths in the spectrum. The size of their wavelengths can range from the size of a football field all the way to larger than our planet. What the video was talking about there is you can tune a radio to a specific amplitude or frequency, so like a radio in your car. If you tune it to an AM radio station, then the frequency of the radio waves, you are kind of intercepting or measuring as they come in that are being transmitted from a radio station. The different AM stations are for different amplitudes of those waves that are being transmitted and then measured by your car's radio. If it's an FM radio station, then the amplitude is staying the same and you are measuring different frequencies of radio waves that are coming in and being measured and then converted to sound by your speaker. So radio radios receive these electromagnetic radio waves and then we'll just convert them to mechanical vibrations in the speaker to create sound, but AM FM, AM changes the amplitude, FM changes the frequency. Now, these massive kind of telescopes that they were talking about that are used to measure radio waves that are coming from all parts of the universe. They are these telescopes, these radio telescopes, and they have to be massive because astronomical objects will produce radio waves, but the size of these radio waves can be absolutely huge. So the size of this dish essentially to measure these incoming radio waves needs to be as big as the waves as the size of the wavelengths. So we get these massive, massive telescopes that are built up. Eventually, people scientists realized that we were limited. We couldn't just build a dish that kind of extended forever. And that's where these arrays started being built up. So now you'll see that these arrays are kind of set up in Y shapes in, you know, flat areas across the world. And they work just as one kind of giant telescope. So each one of these kind of telescopes is connected to one another because they're at a set distance apart from one another. Scientists can just interpret what the wave would look like in between the readings or measurements that each individual telescope is getting. And by doing that, if you have an array that kind of stretches all across our planet, then we can measure radio waves that are the size of our planet. So that's kind of how these array radio telescopes work. Any questions? In terms of radio waves, all I want you to really remember is that they are the longest wavelengths that we use them for radios, AM, changes the amplitude, FM, changes the frequency, and then we often use them with these radio telescopes. Yeah. Amplitude is the distance from kind of the middle of the wave, halfway between the crest and the trough, out to where the crest or trough is. So amplitude is from this imaginary dotted line here, out to the peak, that distance right there. So a higher amplitude would be a larger distance from this dotted line out to the peak. So a higher amplitude wave would kind of look like. And if that ever associated with greater or lower frequency, it would totally set the same. Frequencies set the same if it's for different amplitudes. Okay. So now we are going to talk about microwaves. Microwaves can pop your popcorn. They can catch a speed. They carry thousands of phone channels to speak your calls. But can microwaves help us learn about our world and our universe? That's my doubt. With wavelengths ranging from 30 centimeters down to 1 million, microwaves fall between radio waves and infrared. Microwaves are used in Doppler radar, which is widely used for short-term localized weather forecasting and what you see on TV weather news. Satellites have revolutionized weather forecasting by providing a global view of weather patterns and surface temperature. This unique perspective has greatly increased the accuracy of tropical storm and climate forecast. Different wavelengths of microwaves, grouped into bay rays, provide different information to scientists. Medium length, sea bay microwave, penetrate through clouds, dust, smoke, snow and rain, to reveal the Earth's surface. Satellite microwave measurements reveal the full Arctic sea ice cover every day. Even where clouds exist. These measurements show great variability from year to year, but also an overall decrease in Arctic sea ice since the late 1970s, illustrated here with maps and a time series of Arctic sea ice in September at the end of the summer melt. The Japanese Earth Resources satellite uses longer wavelength L-band microwaves for forest mapping by measuring surface soil moisture, such as this image of the Amazon basin, to identify areas of recent deforestation. L-band microwaves are also used by global positioning systems, such as the one in your car. Scientists routinely combine microwave information with information from other parts of the EM spectrum to study the composition of cosmic dust, or other supernova, such as this supernova image that combines X-ray, radio and microwave data. This recently known supernova is a Milky Way exploded just over 140 years ago at the time of the American Civil War. One important phenomenon is unique to microwaves. In 1965, using long L-band microwaves, Arno Penseus and Robert Wilson made an incredible accidental discovery. They detected what they thought was noise from their instrument, but was actually a constant background signal coming from everywhere in space. This radiation is called cosmic microwave background, and if our eyes can see microwaves, the entire sky would glow with a nearly uniform brightness in every direction. The existence of this background radiation has served as important evidence, supporting the Big Bang Theory, for how our universe began. Microwaves have become both staples and wonders of modern life. They are also the backbone of communications and of Earth's sensing systems, and they are an excellent guide to the ancient history and origins of our universe. Okay, so microwaves are the portion of the spectrum just smaller than radio waves. Communication satellites often use microwaves. They denoted in that video, the different bands they use, CXKU bands, and then in remote sensing or Earth observation, we often use these XCL and P bands. If you think back to our lecture yesterday, when we were talking about radar sat, it uses the C-band. So each one of these just describes the specific portion of the microwave spectrum that is being used for that satellite or for that instrument. You don't need to worry too much about what these represent right now and what bands are. We'll talk about them in a lot more detail next week and later in the course. But the advantage of using microwaves for communication, for satellites, whatever it might be, is that microwaves can penetrate through haze, light rain, clouds, and smoke. So that means that if there's crappy weather or something like that and you're using microwaves for your communication satellites, you're not going to have any communication disrupted by crappy weather, by large amounts of fog or storm or whatever it might be. Any questions about microwaves? Sweet. When you use remote control, you change channels under TV. You're remote is using light waves. But this light is beyond the visible spectrum of light. You can see back in 1800, William Herschel conducted an experiment measuring the temperature changes between the colors of the spectrum, plus one measurement beyond visible red. When that thermometer registered a temperature warwork in all the other colors, Herschel had discovered another region of the electromagnetic spectrum, infrared light. This region consists of short wavelengths around 760 nanometers, two longer wavelengths, about 1 million nanometers, or about 1,000 micrometers in length. And then, we can sense some of this infrared energy as heat. Some objects are so hot they also emit visible light, such as a fire. Other objects, such as humans, are not as hot and only emit infrared waves. We cannot see these infrared waves without a rise alone. However, instruments that can sense infrared energy, such as night vision goggles or infrared cameras, allow us to see these infrared waves from warm objects like humans and animals. Infrared energy can also reveal objects in the universe that cannot be seen with optical telescopes. Infrared waves have longer wavelengths than visible light and can pass through dense regions of gas and dust with lower scattering and absorption. When you look up at the constellation Orion, you see only the visible light, but NASA's Spitzer telescope was able to detect nearly 2300 planet forming disks in the Orion Nebula by sensing the infrared glow of their warm dust. Each disk has the potential to form planets and its own solar system. Incoming ultraviolet, visible, and a limited portion of infrared energy together sometimes called short wave radiation from the sun drives our Earth system. Some of this radiation is reflected off of clouds and some is absorbed in the atmosphere. Larger aerosol particles in the atmosphere interact with and absorb some of the radiation causing the atmosphere to warm. The heat generated by this absorption is emitted as long wave infrared radiation, some of which radiates out to space. The solar radiation that does pass through Earth's atmosphere is either reflected off snow, ice, or other surfaces, or is absorbed by the Earth's surface. This absorption of radiation warms the Earth's surface, and this heat is emitted as long wave radiation into the atmosphere, which allows only a small amount to radiate out to space. Greenhouse gases in the atmosphere, such as water vapor and carbon dioxide, absorb most of this emitted long wave infrared radiation, and this absorption heats the lower atmosphere. In turn, the warm atmosphere emits long wave radiation, some of which radiates towards the Earth's surface, keeping our planet warm and generally comfortable. The energy entering, energy reflected, energy absorbed, and energy emitted by the Earth's system constitutes the components of the Earth radiation budget. A budget that's out of balance can cause the temperature of the atmosphere to increase and eventually affect our climate. For scientists to understand climate, they must also determine what drives the changes within the Earth's radiation budget. The series instrument of ordinances Aqua and Terra satellites can measure the reflected short wave and emitted long wave radiation into space accurately enough for scientists to determine the Earth's total radiation budget. Other NASA instruments monitor the changes in other aspects of the Earth's climate system, such as clouds, aerosol articles, or surface reflectivity, and scientists are examining their many interactions with the energy budget. A portion of solar radiation from the Sun that is just beyond the visible spectrum is referred to as near infrared. Scientists can study how this radiation reflects off the Earth's surface to understand changes in land cover, such as growth of cities, or changes in vegetation. Our eyes perceive a leaf as green because wavelengths in the green region of the visible light spectrum are reflected, while other visible wavelengths are absorbed. Yet the chlorophyll and the cell structure of the leaf are also reflecting near infrared light, like we cannot see. This reflected near infrared radiation can be sensed by satellites, allowing scientists to study vegetation from space. Using these data, scientists can identify some types of trees, can examine the health of forests, and can even monitor the health of vegetation, such as forests infested with pine needles, or crops affected by drought. Studying the emission and reflection of infrared waves helps us to understand the Earth's system and its energy budget. Near infrared data can also help scientists study land cover, such as changes in snow, ice, forests, urbanization, and agriculture. Scientists are beginning to unlock the mysteries of cooler objects across the universe, such as planets, cool stars, nebulae, and much more, using infrared waves. Okay, there's kind of a lot more information in that video in particular than you're going to need to know for this course at least, so I'll try to summarize it kind of clearly. So, the infrared region of the spectrum is next to, as the name suggests, the visible red part of the spectrum. So, the infrared region is just a little bit longer, has slightly longer wavelengths than the visible part of the spectrum. It can be generally split up into near, mid, and far infrared energy, or radiation. The mid-infrared can also be called shortwave infrared sometimes. This, generally, these two here, both near and mid-infrared, are generally classified as reflective infrared. So, that's the infrared radiation in the latter part of that video, where he was talking about radiation that comes down from the sun, interacts with leaves, with vegetation, with forests, and then is highly reflected back out to the atmosphere, and in our case, back out to our satellite, or whatever instrument we're using to observe the earth. On the other hand, we have this far infrared, also called thermal, or longwave infrared, and these wavelengths are generally best for studying the longwave thermal energy radiating from our planet. So, any object will emit heat. So, the thermal or far longwave infrared radiation that is emitted as an object warms is what we sense as heat. So, if you have your thermal infrared goggles, or whatever that military use, the reason that people appear so bright, as opposed to some forest or some plants, is because we're much, much warmer than those other objects like plants. Because of that, we are emitting more thermal, longwave, or far infrared energy, and that's what these kind of thermal goggles measure. They measure how much radiation of that far infrared energy is leaving a person's body. Now, some objects are so hot that they also emit visible light, such as a fire. So, as objects heat up, they'll emit all different kinds of radiation all across the spectrum. Sometimes that includes visible light, which is why a fire appears red. As it gets hotter, you know, the hottest part of the fire will often appear blue. That's because that's the hottest part of the fire. So, as things change in temperature, they release different kinds of energy, different kinds of radiation across the spectrum. Humans, us, we don't emit any visible radiation. The only real radiation that we emit because of the temperature that we are at, which you know is about whatever our body temperature is, 30 degrees Celsius or something like that. We only emit far infrared energy, which is what you can measure with these night vision goggles. Now, the portion of radiation that's just, just, just beyond the visible red part of the spectrum, so far or thermal infrared energy is kind of well beyond the visible part of the spectrum. It goes visible red, then near infrared, then far infrared. The near infrared part of the spectrum, the part that is the wavelengths that are really close to visible red in terms of the size of those wavelengths, these are the radiation or wavelength types that come down and interact with different kinds of surfaces on the earth. In particular, they're really, really useful for environmental applications, for conservation, for forestry, because forests, vegetation, and particularly healthy vegetation highly reflects near infrared energy. So, incident near infrared energy is highly reflective by healthy vegetation, which means that if I am a satellite and I am observing the reflectance of different kinds of surfaces across the spectrum, if I look at just the near infrared energy that's being reflected off the surface of the earth, areas where there's very healthy forests are going to have very, very high near infrared reflectance, which is very, very useful for monitoring things like forest health, vegetation health, vegetation cover, whatever it might be. So, just note that there's a big difference there between what we consider to be thermal infrared and this reflective infrared, this reflective near or mid infrared, we often use to measure vegetation health, whereas that thermal infrared is really what we use kind of to measure things like temperature. Make sense? Sort of? Any questions? Okay. Next, we are going to talk about visible light. So, this is the part of the spectrum that you can actually see with your eyes. This is the part of the spectrum that allows you to make up different colors and perceive things that are blue, green, red, whatever it might be in our world that you see through color. All electromagnetic radiation is light. Visible light is the only part of the spectrum you can see. For all your light, your eyes are light on this one narrow band of EM radiation to gather information about your world. There are some visible light appears white, it is really the combined light of the individual rainbow colors with wavelengths ranging from 5 out of 380 nanometers to red at 700 nanometers. Before Isaac Newton's band experiment at 1665, people thought that a prism somehow colored the sun's white light as it bent to the spread of sun beam. Newton disproved this idea by using cheap prisms to show that white light is made up of the bands of colored light. Newton used a second prism to show that the bands of colored light combined to make white light again. Visible light contains important scientific clues that reveal hidden properties of objects throughout the universe. My new gaps in energy at specific visible wavelengths can identify the physical condition and composition of stellar and interstellar matter. Human eyes aren't nearly sensitive enough to detect these faint peaks, but scientific instruments can. Scientists can learn the composition of an atmosphere by considering how atmospheric particles scatter visible light. Earth's atmosphere, for example, generally looks blue because it contains particles of nitrogen and oxygen, which are just the right size to scatter energy with the wavelength of the light. When the sun is low in the sky, however, light travels through more the atmosphere and more blue light is scattered out of the beam of sunlight before it reaches your eyes. Only the longer red and yellow wavelengths are able to pass through, often creating breathtaking sunsets. When scientists look at the sky, they don't just see blue, they see clues about the chemical composition of our atmosphere. However, visible light reveals more than just composition. As objects grow hotter, they radiate energy with a shorter wavelength, changing color before our eyes. Once a flame ships from yellow to blue, as it is adjusted to burn hotter, in the same way, the color of stellar objects tells scientists much about their temperature. Our sun produces more yellow light than any other color because of its surface temperature. If the sun's surface were cooler, say, 3,000 degrees Celsius, it would look reddish like the stars and taries and beal juice. If the sun were hotter, say, 12,000 degrees Celsius, it would look blue like the star right here. Like all parts of the electromagnetic spectrum, visible light data can also help scientists study changes on Earth, such as assessing damage from a volcanic eruption. This NASA E01 image combines both visible and infrared data to distinguish between snow and volcanic ash and to see vegetation more clearly. Since 1972, images from NASA's Landsat satellite have combined visible and infrared data to allow scientists to study changes in city's neighborhoods, forests and farms over time. Visible light images taken by NASA's Mars landers have shown us what it would look like to stand on another planet. They have expanded our minds, our imagination, and our understanding. NASA instruments can do more than passively sends radiation. They can also actively send out electromagnetic waves to map topography. The Mars orbiting laser altimeter sends a laser pulse to the surface of the planet and sensors measure the amount of time it takes for this laser signal to return. The elapsed time allows the calculation of the distance from the satellite to the surface. As the spacecraft flies above hills, valleys, craters, and other surface features, the return time varies and provides a topographic map of the planet's surface. Back in Earth orbit, NASA's ISAT mission uses the same technique to collect data about the elevation of the polar ice sheet to help monitor changes in the amount of water stored as ice on our planet. Laser altimeters can also make unique measurements of the heights of clons, the top of the vegetation canopy of forests, and can see the distribution of aerosols from sources such as dust storms and forest fires. Finally, visible light helps us to explore the far reaches of the universe that humans could not hope to reach physically. Using visible light, the Hubble Space Telescope has created countless images that spark our imagination, inflame our curiosity, and increase our understanding of the universe. On a day-to-day basis, you can imagine visible parts of the spectrum are probably the most useful to you. They are the part of the spectrum that your eyes are actually capable of sensing. All of these different types of waves are always existing, at least on our planet, if they're able to pass through our atmosphere, they're always kind of all over the place. It just so happens that our eyes, the only ones that we can sense, are in this narrow part of the spectrum from about 380 nanometers in wavelength size, all the way to about 700 nanometers in wavelength size. On the smallest wavelengths here, we have indigo and blue, and then we have red in the longest wavelengths of the visible part of the spectrum. Now, they did in that video there, they actually showed, if you guys remember from yesterday, I briefly mentioned that ISAT uses a LIDAR instrument. So they actually showed their nice visualization of what LIDAR is and kind of how it works, just kind of travels over an area, sends laser pulses down to a target, and then measures how long it takes for that pulse of laser light to return to the sensor. By measuring how long it takes the light to travel and hit a target and go back to the sensor, it can get a measurement for how far away the target is, and thus the elevation of that target on the surface of the earth. Now, they use that, or they show that with an example of visible light, and ISAT does use visible light in their LIDAR sensor and their laser light. Generally speaking, LIDAR uses near infrared energy, not visible light, so just kind of a side note, we'll talk about that in a lot more detail after the mid-term when we talk about active remote sensing, and we talk about LIDAR and ISAT in a lot more detail. For now, what's important to understand is that when we have a satellite, a camera, whatever kind of sensor it might be that's measuring electromagnetic radiation being reflected off of a target, if it's measuring the visible part of the spectrum, then it's measuring the parts that we can, or the colors that we can see, or the wavelength sizes that we can see with our eyes. As soon as you go beyond 380 nanometers, or 700 nanometers on either end, you have gone into parts of the spectrum that we can no longer sense with our eyes. So our eyes are just able to sense wavelength sizes in this range, and beyond that, our eyes can't see them, but we do have sensors on board satellites, on cameras, that we mount on drones or airplanes that can sense and measure the reflectance of those different sized wavelengths, and that is really kind of the basis of Earth observation and of remote sensing, is that we have all these different kinds of radiation, all these different wavelength sizes, and we can pick our sensor to measure certain wavelength sizes and the reflectance of those wavelengths, and then from that be able to deduce information about the characteristics of that surface. Again, we'll get into that in a lot more detail and coming lectures for now, just kind of understand what the sizes of the visible spectrum are so that it kind of starts from 400 or 380 nanometers. You'll see on one of my other sides, it starts at 400 nanometers, not 380. Either one, I would accept in a midterm or exam setting, it's different depending on what source you're looking at. If you want a rough estimation, 400 to 700 nanometers is fine. About 500 or so 600 nanometers, and that ranges where green and yellow are. Okay, any questions about visible light? No, see. So your camera on your, say, iPhone or whatever, your Android, it's a remote sensing instrument, it's just only able to measure reflectance of wavelengths in the visible part of the spectrum. All we do with Earth observation is just expand that to create sensors that are able to measure wavelengths outside of just the visible part of the spectrum. But we also measure the visible part of the spectrum as well. Yeah. So, you know, how like when you look at like the solder, you know where sunglasses, they like the UV light damages your eyes, how come visible light doesn't damage your eyes? Yeah. Another example is like X-ray, if you do that X-ray too much, the radiation will harm your body. Usually because of the frequency associated with a given wavelength type. As soon as you go on this end of the spectrum, as soon as you move this way and you get into UV light and then X-rays and then gamma rays, all of those have a very high frequency, just a very large amount of energy associated with them. So when they come into contact with your body, they can have kind of potentially negative effects from the chemical reactions associated with the energy from those wavelengths interacting with your body. Okay. So that's essentially. Like microwave radio waves, don't you do it again? Exactly. Because they don't have nearly as much energy associated with them. Yeah. Okay. Sweet. Next is ultraviolet, we'll watch the video for ultraviolet and then that'll probably be the last video that we watch. We'll also go over X-rays and gamma rays, but I won't play them because, or I won't play the videos that are from NASA about them because they're not very applicable to Earth observation. The videos are posted on canvas. So if you're curious to watch the video about gamma rays or about X-rays, you're welcome to, but I'll play the video on UV ultraviolet. I'll talk about it. And then I'll just go over briefly X-rays briefly gamma rays and then kind of summarize all of the different wavelengths that we've talked about and highlight what I want you to know for the midterm. Moving spiral arms of galaxy M33 can be seen in visible light, but the true extent of these spiral arms are revealed in ultraviolet light. Just as a dog can hear a wisdom just outside the range of human hearing, bugs can see light just outside the range. Arise can see a bug's dapper amidst this ultraviolet light to attract insects. Johann Ritter conducted an experiment in 1801 to find out what, if any, electromagnetic waves are beyond violet, Ritter knew that photographic paper would turn black more rapidly in blue light than in red light. So we tried exposing the paper beyond the violet end of the visible spectrum. Sure enough, the paper turned black, proving the existence of light beyond violet, ultraviolet rays. These ultraviolet rays or UV radiation vary in wavelength from 400 nanometers to 10 nanometers and can be subdivided into three regions, UVA, UBE and UBC. Visible light from the sun passes through the atmosphere and reaches the Earth's surface. UVA, long-wave ultraviolet, is the closest to visible light. Most UVA also reaches the surface, but shorter wavelengths, called UBB, are the harmful rays that cause sunburn. Fortunately, about 95% of these harmful UBB rays are absorbed by ozone in the Earth's atmosphere. UBC rays are the shortest and most harmful and are almost completely absorbed by our atmosphere. The ozone monitoring instrument, or NASA's aura satellite, detects ultraviolet radiation to help scientists study and monitor the chemistry of our atmosphere, including UBE absorbing ozone. While atmospheric protection from harmful UBE radiation is good for humans, it complicates the study of naturally-produced UV rays in the universe by scientists here on the Earth's surface. Young hot stars shine most of their light beyond the visible light spectrum at ultraviolet wavelengths. Scientists need telescopes in orbit above the Earth's UV-absorbing atmosphere to find and study these UV- bright regions of star formations in distant galaxies. New young stars in the spiral arms of galaxy M81 can be seen in this galaxy evolution explorer, galaxy image from NASA. Chemical substances, both atoms and molecules, interact with UV light making this region particularly interesting to scientists. An ultraviolet instrument aboard Cassini has detected hydrogen, oxygen, water ice, and methane in the Saturn system. UV data have also revealed details of Saturn's aurora. Scientists also use UV waves shining from distant stars to view permanently shadow regions of lunar craters. The Lyman Alpha Mapping Project, or LAND instrument of organisms lunar reconnaissance orbiter, can use this main star shine to look for possible water ice on the moon. Ultraviolet rays may be harmful to humans, but they are essential to studying the health of our planet's protected atmosphere and give us valuable clues to the formation and composition of distance of celestial objects. Ultraviolet light is slightly shorter wavelengths than visible light. Most UV waves are visible to humans. There are some insects, bumblebees, other species that are able to see UV light. The sun is a source of all UV light as well as radiation across the entire spectrum. All of the radiation that we know that exists is emitted by the sun. The UV-C rays, which they mentioned in that video, those are the most harmful ones. They are almost completely absorbed by our atmosphere. So our atmosphere blocks those rays from coming down and hitting us or the surface of the Earth. UV-B rays are the ones that cause sunburns, so some of those are transmitted through the atmosphere. And then only a small amount of these UVA waves hit the Earth. And those are sometimes used in Earth observation, but generally speaking, not really used very commonly. So we're not really going to talk about the use of UV for Earth observation at all throughout this course, but they are generally used for monitoring the ozone and the absorption characteristics of the Earth. Okay. Next couple kinds of waves. Again, this is the video on X-rays. I'm not going to play it because we don't really use X-rays in Earth observation. X-rays have a much higher energy and much shorter wavelength than ultraviolet light. As a result, we often refer to X-rays in energy rather than their wavelength size, just because they're so small. Now, how X-rays are able to image things like our teeth, like our bones, is because different objects absorb different levels of X-ray radiation. So if we image our arms or whatever, our bones absorb X-rays much more than our skin. So that's how we're able to penetrate through our skin and see our bones. And we have sensors that are able to detect the level of absorption of those X-rays. Now, like I said, we don't really use X-rays to observe changes on the Earth or for Earth observation in general or remote sensing. So we're not really going to talk about them at all throughout this course, but just note they exist. The last is gamma rays. Gamma rays have the smallest wavelengths and thus the most energy associated with them. They're produced by the hottest and most energetic objects in the universe, like neutron stars, pulsars, and regions around black holes. On Earth, gamma rays are generated by nuclear explosions, lightning, radioactive decay. Those are really the only phenomena that we know that generate these gamma rays. And unlike optical light or visible light and X-rays, gamma rays can't be captured or reflected by mirrors. So that means that in order to detect gamma rays, we have to have these very special kinds of sensors. Wave lengths of gamma rays are so small that they can literally just pass through the atoms that make up a detector or make up a mirror. So gamma ray detectors are usually these densely packed crystal blocks. As a gamma ray passes through those crystal blocks, they collide with electrons, and then sensors are able to measure those collisions based off of how those electrons are reacting after they get hit by gamma rays. The point is gamma rays are really hard to measure and to monitor. We don't really use them very much in Earth observation. Okay. Summary kind of up here. Gamma rays and X-rays are normally measured in angstroms. That's a unit that is about 10 to the power of 10 meters. And most of these kinds of radiation are completely blocked by the Earth's atmosphere. We then have ultraviolet radiation in the range of about 1 to 400 nanometers in wavelength size. These are also pretty much completely blocked by Earth's atmosphere, except for a small portion close to the visible spectrum, near about 300 to 400 nanometers. We then have the visible part of the spectrum, which we classify as about 400 to 700 nanometers, starting out about, you know, violet or blue all the way up to red. This is the peak solar wavelength, which just means that the sun emits visible wavelengths more than any other kind of wavelength that we know that exists. An Earth's atmosphere is mostly completely transparent, which just means that visible light that's being emitted from the sun comes down to the atmosphere of the Earth and pretty much gets transmitted right through the atmosphere, which comes down, hits the surface of the Earth, and allows us, when we're outside, to be able to see things in color. We then have the reflective infrared, so this is both near and mid-infrared, out of about 700 to 3000 nanometers. There's high absorption of near-infrared by water vapor in the atmosphere, but this is commonly used by Earth observation satellites to monitor vegetation, cover, and health. So reflective, near-infrared, very, very commonly used in Earth observation, very, very useful for us. We're going to be talking about it a lot and its applications throughout this course. We then have thermal far-infrared, that's how we feel or sense heat, that's from about 3000 to 10,000 microns or micrometers. It's terrestrially derived, which just means that thermal or far-infrared energy is heat that's being emitted from objects, it's not something that we measure that's being reflected off objects. It generally speaking also has a lot of absorption in the atmosphere, and then the last two, we have the microwave and the radio part of the spectrum. Microwaves are about 0.1 to 30 centimeters in size. They're the wavelengths used in radar, so in radar sat, we use microwaves, despite radar standing for radio detection and ranging. We actually use microwaves in radar. The atmosphere is mostly transparent to microwaves, and then we have radio waves. Radio waves are anything bigger than 30 centimeters, but can be hundreds and hundreds and hundreds of meters long, and the atmosphere is also pretty much completely transparent to radio waves. Any questions about these? Yeah? How do the atmosphere block the smallest ones that are the biggest ones? Essentially, the reason that the atmosphere blocks certain wavelengths and transmits certain wavelengths is because of the chemical makeup of our atmosphere. Certain molecules that exist in the atmosphere, when they interact with these incoming kinds of radiation, just have a specific kind of chemical reaction where they get absorbed and then re-emitted as heat, whereas others are just able to pass right through. That's really all it is. Just the different, generally speaking, we talk about molecules or particles that are highly absorbent in the atmosphere of radiation, being things like water vapor, carbon dioxide, ozone. There's a bunch of different, but depending, there's all these kind of different compositions and make-ups of the atmosphere, and each one of them may interact with different parts of the spectrum differently. That's kind of the long answer. Is that answer your question, though? Any other questions? Yeah? Yeah? I'm sure I can hear you. Sorry, can you say that louder for me? I can hear you. I'm sure that only the other ones are absorbed from objects, or the different ones that are directly from the different ones. Near infrared absorbed by objects? Yeah. So objects can absorb any part of the spectrum, technically. It just so happens that most radiation that comes down and hits a surface, if it's absorbed, then the chemical reaction that goes on results in thermal far infrared than being emitted as heat. So generally speaking, we don't really have any far thermal infrared radiation that's emitted from the sun and passes through the atmosphere, and hits the surface of the earth. All the thermal infrared energy that we talk about is being emitted from different objects. I'm not sure if that answers your question or not. Yes? No? I'm not sure if you said I have an object, but it's not an answer that is absorbed into the atmosphere. Is it absorbed in with what? Human beings? With human beings. Yeah, so it would be. So we as an object, as a bunch of matter, essentially, that's composed together, we have our own spectral signature. So we absorb and reflect different kinds of radiation depending on the chemical makeup of our skin or whatever it might be. So sometimes, in the case of near-infrared that you're talking about, yeah, we would likely be absorbing that radiation and then re-emitting it as thermal infrared or as heat. Does that kind of make sense? Okay. Okay, that kind of brings me to surface interactions. Electromagnetic radiation interacts with the features on the earth's surface in three ways. It can be absorbed where it kind of comes and hits that surface. Some chemical reactions go on and essentially that energy is lost and partially re-emitted as heat or as long wave thermal radiation. It can be reflected where it comes down, bounces off the surface, and is reflected away in an opposite direction that it came down and hit that surface. Or it can be transmitted. It can pass right through that surface. And if you think about it logically, different kinds of surfaces that you know that exist in the world, they have different levels of absorption, of reflection, of transmittance. You can think of something like very clear water when you're able to kind of see through it. There's a lot of transmittance there going on. There's a lot of light that's penetrating through the surfaces of that water. Reflection is often very high with things like ice and snow. You know, when you're walking out on a mountain and there's a ton of snow, you've got to be really careful that you don't get burnt by all the energy that's coming down, being reflected off that snow, and going back and hitting you. And then there can also be lots of absorption. Things like black concrete have really high absorption properties where energy comes down and is absorbed by that concrete. And that's why when you touch that concrete, it's really, really hot because it's absorbing a ton of energy and re-emitting it as heat. Now, when visible light strikes a leaf, certain light is reflected, creating the image of the leaf we see. And this is true for any object. It just so happens with leaves, the visible parts of the spectrum come down from the sun, get transmitted through the atmosphere, hit a leaf, and the leaf absorbs the red energy a lot and the blue energy a lot and reflects the green energy more than the red or blue energy. So then when we are looking at that leaf, because there's more green light being reflected off that leaf, then red or blue light, our eyes perceive it as green. So that's essentially how that works. That's why leaves look the color they do. Now, some of that energy that that leaf absorbs is going to be re-emitted as heat, and the leaf's reflectance and absorption characteristics are ultimately what gives it the color we perceive it as. So upon striking some sort of surface, whatever it might be, incoming radiation is partitioned into one of these three responses. It has to be one of these three. It's either transmitted, so when radiation penetrates into certain surfaces, like water, other materials that are very transparent and thin, it'll just kind of pass right through. It can be absorbed. Some radiation is absorbed through electron and molecular reactions within that medium. A portion of that energy is then re-emitted as heat as far infrared, so think of concrete, something like that. And then reflectance. Some radiation reflects, so it bounces off the target, scatters away at potentially one angle, potentially various different angles, depending on the surface roughness of that target and the angle of incidence of the rays. We'll talk about transmittance, absorption, reflectance, and a bit more detail next lecture, so don't worry too much about their definitions right now. But what I do want you to know for now is that all the energy that comes down and hits the surface has to either be transmitted, absorbed, or reflected. So these three parameters can be dimensionless numbers represented generally between zero and one, often as just a percentage. So if I say, you know, I have some light coming down, some energy, 60% of that light is being transmitted, 30% of that light is being reflected, how much absorption would be going on? Anyone guess? 10%, has to equal 100%, so the total has to equal 100%, energy has to either be transmitted, reflected, or absorbed. Okay, so practice questions for you guys. I'll leave you for a couple minutes. I'll go over these. If you want to head out now, you are welcome to, and I will see you next week. If you want to stay in and go over the answers to these, I'll go over them in three minutes or so. Hey, yeah, what's up? Is it like it's open, for example, like in a time period and you do it in that time period, but you can do it anywhere? Yes, that's correct. So it'll be open during class time, so it'll be open from 5 to 6.30pm on, I forget exactly what day of the week it's on, but it'll be open from 5 to 6.30pm. You'll write it during that time, but from anywhere. We're not going to be here, so you'll write it until completely online. It's not like it's open and you do it anytime that day. It's correct. It's like open just during schedule class time from 5 to 6.30pm. Yeah, yeah, no problem. I'm confused with these two concepts. Do you briefly explain different? Yeah, yeah, so the main difference here is that there is emitted radiation and reflected radiation. Emitted radiation is that thermal infrared energy. It's what we sense as heat. So that's like what's emitted from the, let's say, like, laptop, anything. Yeah, yeah, yeah. That's the heat that we sense. There you go. Thermal. That's thermal. That's thermal. And then these two are... For near and mid-infrared, these are both reflective infrared types of energy, which means it's energy that comes from the sun, bounces off a target, and is measured the reflectance of it. How much is bouncing off of a given surface is what we measure. So this is near, this is mid, and then this is far? No, sorry, no. So infrared includes all of the mid, near, and far infrared energy. The near and mid-infrared energy is the, are the energies that are reflected. The far infrared energy is the energy that's emitted. Which is the thermal. Yeah, exactly. And then what does this mean exactly? Infrared? Yeah. It essentially just means that it is close to red in the part of the visible part of the spectrum. Alright, thank you. Can you walk me through transmission? Yeah, let's, can I, for all of you? Yeah, we'll talk right after. I'm just going to go over these answers because it's almost in a class, and then I'll talk to all of you. Okay. Let's go over the answers to these guys because I know it's almost technically in a class. I want to get you guys out of here if you wanted to stay for the answers. So, what happens to photons, which we know photons are just the fundamental unit of radiation. So when I say photons, talking about light, what happens to photons that are not reflected? Yeah. Or? I don't have to wait. Pardon me, guys. Pardon me. Guys, pardon me. Thank you. Yeah. What did you say? Absorbed? Or? Yeah. So if they're not reflected, they're either absorbed or, yeah. Yeah. Absorbed are transmitted exactly. So how do you calculate the frequency of a wave? What's that equation look like that we were looking at? Yeah. So if you look at the equation, it's C equals the potential upside down Y. Yeah. Lambda? Yeah. Times V. How often a point passes or how often a top of a wave passes a point? Yeah. Exactly. Yeah. Exactly. And you can calculate frequency by just saying the speed of light divided by the wavelength. That also be the kind of mathematical calculation. Okay. What is the longest wavelength type that we talked about? Yeah. Radio waves? Exactly. What is, what did I mention? That near infrared light is frequently used to measure or monitor from earth observing satellites. Yeah. How about vegetation? Exactly. Vegetation. Vegetation health and vegetation covers the key. One that I want you to remember for that question. And then what are the shortest and longest wavelength sizes of visible light in nanometers? And what colors do they represent? So what's the shortest wavelength of visible light? Yeah. Like 300, 80, 400. Exactly. And what color does that represent? Like a purple. Yeah. Like a violet or blue. Violet for 380, blue for about 400. What's the longest wavelength size of visible light? Yeah. 7, 100. And what color is that? Red. Red. Exactly. Awesome. Good job, guys. Thank you. See you next week. All right, guys, we're going to start here. I am just going to start. We're going to start by talking about the midterm. So I'm going to give it to Evan. He's going to give you his usual reminders of what to be working on this week and then talk about midterm format. If you have any questions about the midterm, blog posts, anything, course, logistics related. It's a good time to ask Evan. Yeah. All right. Hello, everyone. What should we work on this week? Both do on Thursday assignment two and blog posts three. And then next week on Tuesday, February 14th, we have our midterm. I'll talk a little bit more about that later, but I'm just going to go over office hours and a few other things first. So Tristan Douglas, he's going to be holding office hours this Wednesday and Thursday. Times are up there. This will be posted, so no need to take a picture or anything. And then one thing about assignment grading. So when we give you your assignments and we get them back, sometimes we give you these fill in the blank answers and they'll show up like you see here. You can see that red dot where it says you answered 21, correct answer is 19. And that's showing you that you've put it in incorrectly. But before you email us saying, I think the answer is actually 19, just make sure that you check that you haven't had it manually adjusted to 2.5 or 2.5 points. So basically what's happened here is when the assignment was designed, we put the correct answer in improperly and we can't go back and change it, so we have to manually adjust it. So all I want to get across here is before you email us saying that you put the right answer and it's showing up wrong is just double check that your points are correct. That's all I want to get across here. We get a bunch of emails about this every year. We don't have a workaround yet, but just make sure you've got the right mark before you email us. And yeah, under the midterm. So midterms are coming, hope you guys are all having a good time in your courses. I like this little graphic of how I feel about midterms. I'm personally number five where I just want to eat them. Great. So midterm date and time. On Tuesday, February 14th, that's next Tuesday. There's no class. The midterms online open book. You're welcome to come to the class to write your midterm. No teaching staff is going to be here, so feel free to come and work here if you want. It's a semi-decent idea if you don't have reliable internet because UBC has decent internet. It's going to be an hour and a half long. It's during class time. Make sure you start on time because you will get cut off at the end. It's going to be administered through Canvas. You just log into Canvas, open the quiz. I'm going to be on my email the entire time. So if you have any technical issues, email me immediately. Don't wait. Don't mess around with it. If it's just internet reboot it, whatever. But if something's gone wrong, email immediately, I can extend your midterm. I can do a bunch of stuff to work through it. The format, there's going to be 30 multiple choice questions, four film the blank questions and six short answer questions. We only want a brief paragraph for the short answer questions. Don't write a huge novel for us. We're just looking for that you get what's going on pretty much. The exam is open book, online. The questions are going to appear one by one and you must answer a question to go on in the next question. One unique thing about this midterm because it's online and open book, you can't go back and change your answers to a question. So make sure you are very sure on your answer before you move on. In addition to that, the questions appear in a random order. This is to reduce group work. We want you all to work individually. It must be done individually. Don't work together. Don't share answers. Don't discuss answers. If you are an access and diversity student, that will be manually added to your profile. So you don't need to email me asking for it. If you've given me the sheet, I will update it. As a reminder, if you haven't given me your A&D forms, send them to me before the midterm. Plagiarism. Don't be a copycat. This is an open book midterm. That being said, it's an open book midterm. We encourage you to utilize the slides and other materials, but what we don't want you to do is to copy sentences directly. Looking for something like spelling, totally chill. Don't worry about it. But just make sure for the short answer questions you answer in your own words. Don't go and just copy paste directions and slides or just Google the question and copy paste the first thing that comes up. We can find that. Chris is going to host a review session next Monday, the day before the midterm. If you post questions in the midterm discussion board, that's the best way to make sure that he goes over it. Yeah, final notes. It's worth 20% of your overall grade. All the practice questions are posted on Canvas. We don't directly provide answers, but if you post a discussion board saying, hey, I'm not really sure what's going on with this question, I'll get back to you and tell you what's up. We're not going to give you a whole list of the answers, but we will try and guide you along the right way. Mid-term content covers everything from lecture one up until spectral signatures. You're doing resolutions today? No. Okay, so today is the last thing before the midterm. Resolution content, that's tomorrow, not on the midterm. Anything that's on the assignments, not on the midterm. You just want to focus on the lectures. If you have questions, post in the discussion board, that's where Chris is going to know what to talk about in the review session. I'm going to be monitoring it and answering questions. Make sure you have reliable internet. Don't do this via your hotspot on your phone in the middle of the woods. Bad idea. You won't be able to email me saying you're having technical difficulties. We're not reliable for your internet connection. That being said, if you do have technical difficulties, send me an email. I'll do what I can to get you extra time or another attempt if something's gone wrong. One other thing with that is make sure you start your exam on time. Don't start it at 6 p.m. and assume you'll get the hour and a half. You need to start by 5 to get the whole time. Another thing with that, there's a 10-minute buffer on either side of the exam. You can start at 450 and you can write until 6.10, 640. The absolute last time of day you can start at to get the full hour and a half would be 5.10 p.m. Good luck. Do the practice questions. Look over your notes. Make sure you pay attention to what Chris highlights. He really highlights the important stuff all the time and you're going to do great. If you have any questions about the midterm, that was a great time. Hi. I saw a question about the assignment. Yeah. Assignment 1? Yeah. We're aiming for Friday. Yeah. For me, some questions are there something you can fill in the middle of the back and stand up. You can't go back and check them. So make sure you're confident your answer before moving on. It just appeared one at a time. Yeah. And then you have to submit that question to go on to the next question. You can't go back looking at it. Once you go on to the next question, you're on to the next question. Okay. Are you good? Any other questions for everyone? Yeah. Yeah. Just a little bit of your question. Yeah. No assignments or anything. And feel free to email me or post in the discussion board if you have questions and don't want to ask in front of the class. That's totally chill. Perfect. Thank you. All right. One. Yeah. You can head out. One thing I want to highlight that Evan noted was that next Monday, so a week today, I'll do a review session. I'm not going to really prepare any new content for the review session. If there's lecture content from before the midterm that you want me to go over, then post that in the discussion board, say, hey, can we go over this in the midterm review session and I'll go over it. So I'll update kind of a PowerPoint presentation up to about an hour or so before lecture next Monday. So as long as you post before then, then I'll have some content to go over. But if you don't, if no one posts anything and, you know, no one asks any questions, then it'll be, it'll be quick. I won't have anything kind of prepared, but you guys are also welcome to just come and ask questions. I'll have all the slides for all the different lectures and everything available. Okay. Tristan's going to come at the end of class today to talk about assignment two. Give you some tips and tricks. You can ask him any questions if you have any questions about assignment two. Today, we are talking about spectral signatures. I think that in this course, this is the kind of part of the course that starts to get a bit more fun, a bit more interesting, a little bit more sciencey as well. So bear with me. So today we are going to talk about something called spectral signatures. And there's really two key things that I'm hoping that you'll take away from class today. One is just understanding what a spectral signature is and how it might look for different surfaces of the earth. And then the second is understanding the spectral response of specifically vegetation. So we'll talk about leaves and the spectral response of leaves and the properties of leaves that influence the spectral response across different parts of the electromagnetic spectrum. Okay. So reviewing from where we ended off last week, kind of one of the final slides we looked at was about surface interactions. So we know that there's all these different kinds of radiation all across the electromagnetic spectrum. They're all emitted from the sun. They travel at the speed of light through space. Basically they come in contact with the earth's atmosphere. If they make it through the earth's atmosphere, they come down to the earth's surface where they interact with surfaces on the earth in one of three ways. They're either absorbed, reflected or transmitted. So they're either transmitted where they kind of pass through a material. They're reflected where they bounce off a material or they're absorbed by that material. So common materials that have relatively high transmission on the surface of the earth might be things like water. When you're underwater and you look up, you can kind of see little bits of light rays that are penetrating below the surface of the water. That's because that light is being transmitted through the water. Reflected common surfaces that have high levels of reflection might be something like ice. Ice and snow when sun comes down or when light rays from sun come down and hit snow or ice, they bounce off that snow or ice in the opposite direction. Before they might be absorbed, black concrete is really, really hot because all the incident light that's coming down and hitting that concrete is absorbed. Not much light is reflected and really none is transmitted. Now for visible light striking a leaf, in particular, if you imagine a leaf on a particular tree in a forest, when light strikes that leaf, some of the light is absorbed, some of it is reflected. The light that is reflected is what allows you to perceive that object, in this case that leaf, as a particular color. So leaves are green because visible light is radiated from the sun, travels through space, through the earth's atmosphere, down to the surface of the earth, hits a leaf and when that visible light hits the leaf, green light is reflected more than the other visible wavelengths of light that hit the leaf. So all parts of the visible spectrum will come down through space from the sun and hit the leaf, but red parts of the spectrum, blue parts of the spectrum are absorbed more than green. Visible green light is reflected relatively more than red and blue light, which allows us and our eyes to perceive that leaf as green. And that's true for anything when you're outside. When you're outside walking around and you see something that is brown, say a tree trunk or something else or brick on the side of a house, that's because that brick, that tree trunk, is reflecting more red light than, say, blue and green light, giving it kind of a brownish reddish haze. So your eyes perceive colors because of the light that is reflected off of those surfaces, off of those surfaces or off of those targets. Now a spectral signature is just the pattern of spectral response of a material across a certain part of the electromagnetic spectrum. It's typically visualized with a graph and it just shows the percentage of radiation of different wavelengths reflected from a given object. So this is an example of a couple different spectral signatures here. You'll have the wavelength size along the x-axes. You'll have the reflectance and percentage along the y-axes here. You can see here for green vegetation, that's this green line here, we have a little bit of a peak there in the visible green part of the spectrum. And then it increases and there's lots of near infrared reflectance and then it's kind of variable in the mid-infrared part of the spectrum. You can see for soil, soil has strong absorbance, so low reflectance in the blue and the green part of the spectrum. But when we get to the visible red part, right around 700 microns or micrometers, it's got a relatively higher level of reflectance than say in the green or blue part of the spectrum. When we look at water spectral signature, it's going to have the most reflectance in the blue part of the spectrum. And then lower but kind of close in the green part of the spectrum and then much lower in the visible red part of the spectrum. And all of that allows you to perceive certain surfaces in certain colors. Now by plotting spectral signatures of different materials together, the portions of the spectrum where their signatures differ can be readily identified. So all that means is that if I plot several spectral signatures, which I've done here, then I say, okay, let's look at the near infrared part of the spectrum and see how different the reflectance is for wavelengths of that size. You can see that the green vegetation has a much higher reflectance in the near infrared. Soil kind of has a moderate amount and water has a very, very low amount. The value in that for remote sensing scientists for Earth observation is that by knowing what level of reflectance there is for certain materials at different wavelength sizes, we can easily differentiate which materials or surfaces we're looking at on the surface of the ground with, say, a satellite or some other sensor for Earth observation. So that's really the value of that. Okay, this is a video that kind of just introduces in a bit more detail what spectral signatures are and how they might look for different common surface materials on the Earth. Hi, everyone. In this video, we're going to be covering spectral signatures. In remote sensing, we use the spectral information of materials or features to be able to distinguish between them and analyze them. Each material or feature reflects energy differently due to the mechanical and structural interactions. Energy emitted by the sun is reflected by objects on Earth, which we are then able to measure. In remote sensing, we call this reflectance of energy spectral information. A simple way to analyze a spectral information is through a spectral signatures graph, which is what you see here. A spectral signature shows how much an object or material reflects energy across a spectrum of wavelengths. As we've learned, a photon can be either reflected, absorbed, or transmitted, and what we receive with our eyes and our camera systems is the reflected wave. In this graph, we see reflectance of photons on the y-axis and the wavelength on the x-axis. For the purposes of this tutorial, we're going to distinguish between three components of the electromagnetic spectrum, visible, which we're able to see with our eyes and near infrared and short-waiting for an which are wavelength we cannot see. The visible part of the spectrum is the easiest for us to interpret because it is the only part of the electromagnetic spectrum we were able to see with our naked eyes. We see the wavelengths in the visible spectrum's colors, while all other wavelengths outside of the visible spectrum are invisible to us. For example, the color blue is actually about 400 nanometers, green about 550 nanometers, and red about 700 nanometers. So now that we understand that colors are actually wavelengths within the visible spectrum, that we can see, let's take a look at some common features that we would recognize in our own sense of imagery. Let's start with motion water, which looks blue to us, but remember that blue is a wavelength. The water looks blue because of the wavelength, which is around 400 nanometers, is being reflected the most in the visible spectrum. That could look somewhat green, but rarely red. That's why these wavelengths are not reflected as much as blue. What about water in an hearing thread? Remember that just because we see water as blue doesn't mean that it is reflected, absorbing, or transmitting other wavelengths. Water is particularly simple in the hearing thread and short-waiting thread, as almost all these wavelengths are absorbed. That's why we see zero reflectance for most of the hearing thread and short-waiting in the red wavelength. Okay, let's do another feature first now. How about a soil? In the visible spectrum, soil is often brown, which means that it is reflecting the red wavelengths, which are around 700 nanometers the most. Blue and green wavelengths are still being reflected, but not as much as red or brown. In the near-and-red part of the spectrum, we see that the reflectance of soil continues to rise and then starts to let it off in the short-waiting thread. You can see that soil reflects much more energy in the short-waiting thread wavelengths when it does in the visible or near-and-for-red parts of the electromagnetic spectrum. Let's try cloud's next. Clouds are very white, which means that they reflect all wavelengths in the visible spectrum a lot. Remember that when you combine all colors together that we can see, you get white. In near-and-for-ed and in a short-waiting thread, we see that clouds then start to drop off in reflectance. This happens because the clouds are most available to be a while, and as we saw earlier with the dark blue signature on our grass, water absorbs wavelengths in the near-and-for-ed and short-waiting thread, which is why we see the spike of signature of cloud continue to steadily drop. Okay, let's take a look at one more feature class. How about vegetation like a forest? A healthy forest looks green to our eyes, which means that it is reflecting more green wavelengths than blue or red. This is why we see a dramatic bump in the reflectance of vegetation in the green wavelengths around 550 nanometers. What about vegetation in the near-and-for-ed? Probably the most dramatic change in reflectance we've seen so far, forests are shown to be very high when reflected in the near-and-for-ed section. This is largely due to the structural and chemical compositions of the leaves, which highly reflect near-and-for-ed wavelengths. In the short-waiting thread, forests are shown to be variable depending on the wavelength. We see that forest reflectance declines and falls with an overall downward trend. Okay, so let's grab out now. This is the final product of our spike of signature scrap, which shows us how water, soil, cloud, and forest reflect photons of different wavelengths in a visible, near-and-for-ed and short-waiting thread sections of the electromagnetic spectrum. The chemical and structural compositions of each feature cause reflectances across wavelengths to be different from one another, telling us how much energy a particular feature cause reflects through each wavelength. A real-world application of this information would be to determine which wavelengths would be best for differentiating among the features in our monocensing image. For example, for water we see relatively high reflectance in the blue wavelength around 400 nanometers and very low reflectance from around 900 nanometers onwards. Using this information could help us differentiate water from other features of an image. Soil reflects the red around 700 nanometers quite a bit and is also highly reflected in a short-waiting thread range around 25 hundred nanometers using these two wavelengths to help us differentiate soil from other features of an image. Cloud is unique in that it reflects highly in the whole visible spectrum and has low reflectance at around 25 hundred nanometers. These wavelengths can be used to help us differentiate clouds of an image. And finally, vegetation reflects green around 500 nanometers a lot and has a big spike in reflectance in the near around 1,050 nanometers. These wavelengths can be used to differentiate forests or vegetation of an image. One can quickly see that understanding the cycle signatures of features can help us to distinguish our movement around sensing information and lead to high quality analyses. Okay, so the key here is that when we use more two or more wavelengths or more than two wavelengths, when we use multiple wavelength sizes and plot the reflectance of different materials for those particular wavelength sizes, then we have this improved ability to be able to distinguish different materials on the surface of the earth. And that is really the basis for pretty much all of earth observation remote sensing. All that means is if we have this spectral signature graph here and we say, okay, what is the spectral response or the reflectance of each of these different materials for a wavelength size of about 500 nanometers or 0.5 microns, then we can say, okay, well, green vegetation is a little bit lower, water is a bit higher, soil is a bit higher, etc. And then we can say, okay, well, what about the reflectance of those materials in this near infrared band, this part of the spectrum here? And then what about those reflectance of those materials also in the mid infrared or short wave infrared, this part of the spectrum here? If we combine the information from each of these bands, each of these particular wavelength sizes, which are just kind of arbitrary, I've just picked it random, but if we combine multiple of them, we can very, very, very accurately and efficiently distinguish different materials that are on the surface of the earth and potentially monitor how they're changing through time. So if we look at this example, we have a spectral signatures graph of four different materials, grasslands, pine woods, red sand and silty water. And I wanted to kind of ask you guys some questions about how you can pick apart this graph, how you can derive some information from this to use in an earth observation setting. Which region of the spectrum shows the greatest reflectance for each one of these different surface cover types? So maybe just, you can just, someone can just yell it out to me. So for grasslands, what region of the spectrum, what approximate wavelength size has the highest level of reflectance? Yeah, kind of right around there, exactly, what about pine woods? Also kind of around there, so we got grasslands, yellow right there, pine woods and green, kind of a similar area of peak reflectance, what about red sand? Point six or so, yeah, right around here, and then what about silty water? Where's its highest reflectance? Yeah, point five or so, point six, right in around there. But point seven five, we can say for grasslands, about point eight two for pine woods, about point five nine for red sand, maybe about point five four for silty water. Okay, now at zero point six microns, so right here, do you guys think that you could distinguish these four classes, these four different materials on the surface of the earth? Yes or no? Could you, well, discuss with someone sitting next to you, brainstorm for a bit, I'll give you a couple minutes, and we'll come back and talk about it, but which of these materials could you distinguish and why, which of them could you not? Restawn for a couple minutes, then we'll discuss. Okay, what do you guys think? Which ones could I distinguish, maybe, which ones could I maybe not distinguish? Anyone have any ideas? I feel like, briefly, yeah? We thought that grasslands and pine woods would be really hard to distinguish your plants. Grasslands and pine woods, yeah, the green yellow signatures here would be really hard to distinguish, how come? They're just so close and if they're way up in space. They take three readings, you would have a better. For sure, yeah, their reflectance is so close at 0.6 microns, it'd be really hard to tell them apart. What could you maybe easily distinguish, though? Red sand. Yeah, yeah. Red sand, from probably all the rest, it has a really high reflectance, very different from all of the other ones. What about silty water down here, this blue one? It's like decent, you could probably distinguish it at least very well from maybe red sand. You might have a little bit of difficulty distinguishing it from, say, grasslands or pine woods, but in general, it's still pretty different, so you could maybe distinguish it. So if we saw grasslands had about an 18% reflectance and pine woods had about a 21% reflectance of wavelengths with a size of 0.6 microns, yeah, for sure, those would be hard to distinguish. What red sand has a much higher reflectance, silty water has a much lower reflectance, those maybe we could much more easily distinguish. Okay, and then just lastly, which material is brightest at about 0.6 microns? We'd say red sand, that's right around here, and at 1.2 microns, right around there, we would say grasslands has the highest reflectance, or are the brightest materials. Okay, so that's kind of just a brief example of how you could look at spectral signatures for various different materials on the surface of the earth and analyze them and consider how they're different, how they have different levels of brightness or reflectance for different wavelengths. Now for the rest of the lecture, we're going to focus specifically on vegetation and essentially specifically on leaves, and how the spectral signature of healthy versus unhealthy vegetation looks and why it looks that way, what influence is it? So we know that as we go from summer into fall into winter back into spring and then summer again, we can get a wide variety of colors when we look at leaves, right? So leaves in the middle of summer might be nice and bright green, leaves in kind of towards the fall, we'll start getting a bit red, a bit orange, maybe eventually kind of brownish, eventually they'll fall off. But there's all these different potential colors that you can see leaves, and we're going to talk about essentially why they appear the colors that they do from kind of more of a scientific perspective. So this is the spectral signature for vegetation, for healthy vegetation in particular. You can see here that it's characterized by a little bit of a peak in the visible green part of the spectrum with much lower blue and visible red reflectance. And then as we get into the near infrared part of the spectrum here, it's got much higher reflectance up around 50%. And then as we get into the mid infrared or shortwave infrared part of the spectrum through here, you can see that it kind of fluctuates, but generally has an overall decreasing trend of reflectance. So just a reminder review from our electromagnetic spectrum lecture, this is the visible part of the spectrum here from about 0.4 to 0.7 microns in wavelength size. We have the near infrared here from about 0.7 to 1.5 microns in wavelength size. And then we have our mid infrared, which will sometimes be interchangeably used with the term shortwave infrared from about 1.5 to 2.5 microns. Now for each part of the electromagnetic spectrum, there's a different function or different factor of the leaf that influences the spectral response that we see there. In the visible part of the spectrum, it's a cell type in leaves called the paliside parencoma. In the near infrared part of the spectrum, it's a different type of cell in leaves called the spongy mesophil cells, and then in the mid infrared part of the spectrum, what dominates the shape of the spectral signature is the water content in a leaf. Now one other thing that you'll notice on this graph and that I have listed on the right here is something called water absorption bands. Water absorption bands are where water vapor in the atmosphere or just water in general is has highly absorptive properties or essentially just absorbs wavelengths very, very, very strongly, which result in very low reflectance. And that's had about these different wavelength sizes at about 0.97 microns, 1.19 microns, 1.45 microns, 1.94 and 2.7. And that's what each of these dips are here. So when you see this dip here, that large dip there, this dip here, this dip here, this dip here and this dip here, these are the atmospheric absorption bands. These two are also atmospheric absorption bands. This diagram just only points to the three of them here. But these kind of gray dips are representing this y-axis here, which is atmospheric transmission. And what you can see here is that atmospheric transmission is 100% at the top here, 0% or close to 0% at the bottom here. And so each of these dips of this kind of gray that's coming down from the top represent wavelength sizes that have very, very, very little transmission through the atmosphere. When they hit the atmosphere, water vapor in the atmosphere strongly absorbs wavelength of those sides. So very little of them actually make it down to the surface of the ground. And we're going to talk in a bit more detail about each of the different factors that influence the spectral response in these three different parts of the electromagnetic spectrum. So if we look at the cross section of a leaf, it'll look a little bit something like this. So by a cross section, I mean if you kind of turned a leaf on its side, so you know, this was the top of my leaf, this was the bottom of my leaf, and you were looking right at the side of the leaf, and you zoomed in, you zoomed in, you zoomed in, got kind of a microscope style of zoom in on the leaf, you would see something that essentially would look like this. Now this top layer here is something called the epidermis. We're not going to worry about that too much. But these right here, these kind of perpendicular or parallel cells that are all lined up next to each other nicely, these are called the paliside parenchyma cells. So these cells here that are lined up, you can imagine, you know, there's been many depictions of them, one common one is if you're barbecuing, and you are barbecuing some hot dogs, some sausages, some weiners if you will, getting some nice juicy weiners going on your barbecue, you might line them up like this, all next to each other right on the grill, boom, boom, boom, boom, boom, boom, right? And so that's what you can imagine the paliside parenchyma look like. A bunch of sausages or hot dogs lined up nicely on your grill. The spongy parenchyma mesophil cells are just called often the spongy mesophil cells, are these circular cells, these ones here, and these ones you can see have a lot of actually empty space between them. If you notice there's kind of a lot of these gaps here like right there, right there, right there, right there, and that's just air. There's a lot of air in between each of the individual spongy mesophil cells. Again, if we look at, yeah. So that's how well that particular wavelength size of light is being transmitted through the atmosphere. Oh, yeah, so when so for these wavelength sizes here, this one, this one, this one, as well as kind of these two here, there's very, very little transmission of those wavelength sizes through the atmosphere because they're getting absorbed by water vapor very, very strongly. So if it's 100 degrees, all of it is transmitted through the atmosphere. Correct. It's all of the region's surroundings. Correct. Yeah, yeah. Okay, so this is just another example of diagrams of the cross-section of a leaf. This is kind of a, you know, graphical depiction of it and this is what it would actually look like if you're looking at it with a microscope. But again, you can see here the paliside parenchyma cells all lined up like this. And this is what they would actually look like in reality. These are all the paliside parenchyma cells all lined up. And then you have your spongy mesophil cells just below that. And again, this diagram nicely points out that there's a lot of space sometimes in between these spongy mesophil cells. There's a lot of air, whereas these paliside parenchyma cells are all lined up very close to one another boom, boom, boom, boom, boom. Now, there's chlorophyll pigments, chlorophyll pigments that reside in the paliside parenchyma cells. Chlorophyll in plant cells is what drives photosynthesis. So chlorophyll is a pigment in cells that drives photosynthesis, which is essentially how the leaves create energy and ultimately how they create matter to be able to grow. So chlorophyll pigments in the paliside parenchyma have a very significant impact on the absorption and reflectance of visible light. Paliside parenchyma cells are what control the spectral response of leaves in the visible part of the spectrum. On the other hand, spongy mesophil cells have a significant impact on the absorption and reflectance of near infrared light or near infrared energy. So spongy mesophil cells are what dominate and highly influence the spectral response of leaves in the near infrared part of the electromagnetic spectrum. So the paliside parenchyma here, that hosts the majority of the chlorophyll. That's what dominates the spectral response in the visible part of the spectrum. The spongy mesophil, these guys here, they have a lot of air space between them, kind of look like little peas or little pods. These are what dominate the spectral response of leaves in the near infrared part of the spectrum. So for the paliside parenchyma cells, again, these ones that are all nicely lined up, these have chlorophyll that reside in those cells. The chlorophyll drives photosynthesis in that cell which drives the health and the energy creation in that cell. Chlorophyll has very high absorption properties of visible blue and visible red light. High absorption properties of visible blue and visible red light. That means that chlorophyll de facto has to have low reflectance of blue visible light and red visible light. The chlorophyll strongly absorbs blue visible and red visible light and thus has very, very low reflectance of blue visible light and red visible light. That's why you see this pointed out here, these chlorophyll absorption bands. So chlorophyll in the paliside parenchyma cells strongly absorbs light in this area of the spectrum and in this area of the spectrum. And that's why you see this little kind of hump of green is because it doesn't absorb green light as much as it absorbs blue and red light. Now in reality, there's lots of other pigments that also reside in the paliside parenchyma cells and potentially can influence the color in the visible part of the spectrum or the spectral response in the visible part of the spectrum for leaves. There's things like, there's obviously chlorophyll that's what we've talked about, but there's also carotenoids, flavonoids, carotenoids, anthocyanins, xanthophils, all of these different pigments that could potentially influence the spectral response of the leaf in the visible part of the spectrum. Now chlorophyll, the pigment that drives photosynthesis in the leaf has, as I mentioned, very high absorption in both the blue and red part of the spectrum. Very very high absorption in the blue and red part of the spectrum. Now technically, there's two specific types of chlorophyll, chlorophyll A and chlorophyll B. I don't expect you to be able to differentiate between the two of them, but it's very important to know that chlorophyll's influence on the spectral response of a leaf is that it has very high absorption of visible blue light and very high absorption of visible red light, which is what you can see here. So this on this graph here, you're seeing absorption on the y-axis, wavelength size on the x-axis. You can see here, very high absorption in the visible blue part of the spectrum, and here quite high absorption in the visible red part of the spectrum. Now because of that, subsequently, you're going to see very low reflectance in the blue part of the spectrum and very low reflectance in the visible red part of the spectrum due to chlorophyll. I also mentioned that there's these other pigments that are potentially present in the palocyte parentcomacels of leaves. We have carotenes, xanthophils, phyosinins, all of these other kind of wavelengths, and these wavelengths might have absorption that's high in, say, the green part of the spectrum and the blue part of the spectrum. And that's what you see here. You can see here there's much more absorption going on in the kind of green part of the spectrum, right here, very little absorption going on in the green part of the spectrum. Here much higher levels of absorption going on in the green part of the spectrum by these other pigments that are always present in leaves. So each of these pigments, all of these ones, they're always present in the leaf. They're always there. Chlorophyll absorbs blue and red light strongly. Most of these other pigments absorb blue and green light strongly. So why is it then that we see a healthy leaf as green? Well, the reason is when vegetation is healthy, chlorophyll pigments are just the dominant pigment in a leaf. There's lots of photosynthesis going on. It's a very healthy leaf. So there's lots and lots and lots of chlorophyll. This essentially masks the effect of these other pigments that are also always present in the leaf. So essentially this strong influence of large amounts of chlorophyll results in very, very high visible blue absorption and very, very high visible red absorption. That essentially precedes this absorption going on in the green part of the spectrum. Now what that means is when vegetation starts to sinesse, when fall comes around, vegetation starts to get yellowish or reddish. That's not because there's new, you know, there's new pigments that are appearing in the leaf. That's only because chlorophyll pigments are dying out in the paliside parenchyma cells. When the chlorophyll pigments die out, there's no longer that very strong absorption in the visible blue and visible red part of the spectrum, but the absorption properties of those other pigments, the caratines, phyosinins, xanthophils are still present, which means that there's still relatively strong absorption in the blue and green part of the spectrum resulting in a higher overall reflectance in the red part of the spectrum, which is why we then see leaves as red, orange, yellow, things like that. So healthy leaves are going to look like this. It's going to have, because of the chlorophyll, very low reflectance in the visible blue and visible red part of the spectrum. Again, because chlorophyll absorbs visible blue and visible red light very strongly, it's going to have relatively low or very low reflectance of the visible blue and visible red light. And then overall, it's going to have a slightly higher reflectance in the visible green. When leaves become unhealthy, when they lose their chlorophyll, you don't see a massive dip in the amount of green reflectance, because all that's really changing is now you don't have chlorophyll strongly absorbing visible blue and visible red light. So chlorophyll essentially, because it absorbs visible blue and red light so strongly, it makes the leaf appear green. When the leaf becomes unhealthy, it loses that chlorophyll, that visible blue and red absorption that the chlorophyll was doing is no longer there. Thus, we're going to have higher reflectance in the visible blue and particularly high reflectance in the visible red, meaning that now our leaf is going to appear kind of red ish or kind of yellowish or kind of orange-ish. This is a super, well, it's a pretty important concept and it's one that I'll definitely test you guys on, on the midterm, on the final exam, potentially. Leaves appear green not because they have an overall very high reflectance of green light. They appear green because blue light and red light is absorbed much more strongly by chlorophyll than by green light. So it doesn't necessarily have to do with green reflectance being much higher in an absolute sense. Green reflectance is higher than blue and red for a healthy leaf relatively, but that's only because the abundance of chlorophyll and healthy leaves strongly absorbs visible blue and visible red light. Make sense? Any questions? Sweet. Okay. Definitely expect a midterm or final exam question on that. Maybe both. Okay. So that's the visible part of the spectrum. That's what influences the response that we see in the visible part of the spectrum. So a healthy leaf is characterized by strong visible blue and red absorption and a relatively higher amount of green reflectance. And then an unhealthy leaf is characterized by much lower visible blue and red absorption and thus much higher visible blue and red reflectance, kind of a relatively similar level of green reflectance, but because we now have these other pigments that are dominating, we'll see that the leaf will appear yellow or orange because this wavelength size here and the yellow or orange part of the visible spectrum will be a much, much higher level of reflectance than if there were an abundance of chlorophyll. Lot of words, lot of things being said. Are you guys with me? Decent amount of knots. I'll take it. Okay. Next part of the spectrum that we're going to look at, the near infrared part of the spectrum. I mentioned that spongy mesophil cells are what controls the spectral signature or the spectral response in that part of the spectrum in the near infrared part of the spectrum. When you look at the spectral signature of a green leaf, of a healthy green leaf, the near infrared reflectance dramatically increases between 700 to about 1200 nanometers, which you can kind of see here, right? Once we get to the edge of the visible part of the spectrum, we get to red here, we get into the near infrared, boom, our reflectance gets much, much, much higher. In the near infrared, healthy vegetation is generally characterized by a high level of reflectance and a relatively high transmittance as well to potentially underline leaves below that leaf in the canopy. There's very little absorption going on, but there's a moderate amount of reflectance and a moderate amount of transmission. Now the key of why you see such a large amount of reflectance here, despite when light hits the leaf, it doesn't have a ton of reflectance right away, maybe just for you to 60%, but it will have an overall high reflectance when you look at a whole forest or a whole tree, and that's because there's high diffuse reflectance of the near infrared energy from plant leaves due to internal scattering at the cell wall interface or cell wall air interface. I'll talk about what that means now. Essentially, these spongy mesofil cells, right? These are the ones below the paliside parenkima. Light comes down from the sun, gets through the atmosphere, comes down and hits the leaf. When it hits the leaf, the visible light interacts with the chlorophyll and other pigments in the paliside parenkima cells. The near infrared light gets transmitted through the paliside parenkima cells, so it doesn't really interact with the paliside parenkima very much, but does interact with these spongy mesofil cells here. So the near infrared light goes through the paliside parenkima, but then starts to interact with these spongy mesofil cells. The way that near infrared light interacts with spongy mesofil cells is quite unique. It essentially will hit a spongy mesofil cell and potentially reflect or reflect off that cell out to another cell and then kind of reflect off that cell potentially over to another cell. So essentially, when near infrared light comes down, gets to the spongy mesofil cells, it's bouncing all around, kind of bouncing all around within the leaf structure. So it looks something like this. There's an audio to this that I do not require. So if we have near infrared photons, which we know is just light, is radiation, coming down from the sun, we got these spongy mesofil cells in green here. Once those photons hit the cells, they're going to bounce all around within those cells. That's because there's so much air space between those cells. There's this ability for this near infrared light to bounce around between the cells. Eventually those photons, that light is going to bounce out of the cell. Now it might bounce out of the cell and thus out of the leaf back to our sensor, back to our satellite, back to whatever it is that we're using to measure reflectance on that target or at that point of the surface of the earth. But that light may also just bounce out down to another leaf below it or across over to another leaf on the side of it. And then when that light hits that other leaf that's next to it, it'll start bouncing around between the spongy mesofil cells in that leaf again. When it does that, it might then be kind of bounced out back towards the sensor that we're using to measure reflectance or maybe it'll bounce down to another leaf and hit that leaf and bounce around again. The point is that there is a ton of essentially near infrared light coming down hitting a leaf, bouncing around in the spongy mesofil cells and then bouncing out in kind of a diffuse direction, which just means in all potential directions and then maybe we'll hit another leaf. This repeated coming down of near infrared light, interacting with spongy mesofil cells, bouncing around and then bouncing out to hit another leaf and then bouncing around and then bouncing out to hit another leaf and then bouncing around and then bouncing out to hit another leaf. And again and again and again, that process is what results in this very high level of near infrared reflectance. This repeated reflectance and transmission of near infrared light through a leaf to another leaf, then repeated reflectance and transmission through that leaf to another leaf and so on and so on and so on. So the leaf already off the bat may reflect, say, 40% to 60% of the near-infrared energy that comes down, but the remaining 45% to 50% of that near-infrared energy may penetrate, may transmit through that leaf, or may be scattered to a different direction from that leaf and then be reflected again by a potential other leaf to the side of it, above it, below it, whatever direction it might be. So this part of the spectrum, why we see such a high level of near-infrared reflectance here, is really due to the structure, which it says here, the structure of the spongy mesophil cells. Because the spongy mesophil cells are this kind of pod shape and they have lots of intercellular airspace between them, there's this ability for near-infrared light to bounce around all between the spongy mesophil cells and then bounce out to another leaf and then bounce all around the spongy mesophil cells in that leaf and again and again and again, which results in ultimately this very high level of overall near-infrared light reflectance. Does that make sense? Nod still, sweet. This is this part of the, this lecture is usually the lecture that I get students saying that they have the toughest time with, so if you're not following around and you want this kind of reviewed, in the midterm review session, final review session, whatever it is, feel free to ask me, but just a warning. You'll definitely see midterm and final exam questions about this stuff and I know it's a little bit tougher to wrap your head around, so feel free to ask me questions as we go through. Okay, everyone's good for now though. Sweet. Okay. Last part of the spectrum, we're going to talk about the mid-infrared or short-wave infrared part of the spectrum, that's kind of this part of the spectrum here. Now as I kind of mentioned, when we first looked at this graph, water vapor in the atmosphere creates five major absorption bands across the near-infrared to mid-infrared part of the spectrum and that's these, each of these dips that you see here in terms of transmission. So when these dips come all the way down here on this part of the graph, that's showing a zero percent transmission for that wavelength of light. That's called an absorption band. That is where water vapor in the atmosphere is highly absorbent of wavelengths of that size. You can see there's five of them, one, two, three, four, five and they're denoted by these wavelength sizes here. Now likewise, water content in leaves create water absorption bands. So when you look at a detailed spectral signature of a leaf, you'll see a slight dip in reflectance at those same bands. So there's a slight dip there, a slight dip there, a slight dip there, a slight dip there and a dip here and that's because water that's present in the leaf is highly absorbed in absorbent, absorbent, absorb, what's the word, absorption, absorbent, absorbent, absorbent, I have no idea. That's okay. There's a lot of absorption going on, that's the point. So right around this wavelength size, this wavelength size and then these three wave like sizes here, water that's present in the leaf is highly absorbing those wavelength sizes. The same way that those that water vapor in the atmosphere is highly absorbing incident energy coming from the sun of those wavelength sizes. So that's why you see these kind of little dips here, here, here and here. This is just a less detailed version of this, which is why you see these two little dips here but you don't see them here but they do exist. Now that just explains why you see this general kind of pattern of a dip, a dip, a dip, a dip in this spectral signature. The more important thing to remember is that there's a strong relationship between the reflectance in the mid-infrared region and the overall amount of water present in the leaf. Water in leaves absorbs incident energy between the absorption bands in the mid-infrared or short wave part of the spectrum at increasing strength at longer wavelengths. All that means is when you look at this spectrum, you'll see here that from right about there where the mid or short wave infrared part of the spectrum starts. As we travel to the right and get longer and longer wavelength sizes, you'll see overall there's this trend of lower reflectance. Longer wavelengths, we see lower reflectance overall. There's this overall downward trend. That's all we're describing here. And that's just because at longer wavelengths in the short wave infrared or mid infrared part of the spectrum, water is more strongly able to absorb. The longer the wavelength in the mid-infrared or short wave infrared part of the spectrum, the more strong water in the leaf is able to absorb that energy. This is what I just said. Water is going to absorb the mid-infrared. So the greater the water kind of leaves, the lower the mid-infrared reflectance. Yeah, exactly. So if we look at a spectral signature of leaves of say wet leaves versus dry leaves, we'll see that if we look at the spectral signature of a drier leaf, say only about 30% moisture content, there's this overall higher level of reflectance all throughout the mid-infrared or short wave infrared part of the spectrum. If we have a wetter leaf, say 90% moisture content, then you'll see that overall, there's a much lower level of reflectance and the short wave infrared or mid-infrared part of the spectrum. So that just means that this whole line either shifts up or down all throughout that part of the spectrum. Okay, just to review, there's three kind of key concepts we went over talking about the spectral signature of vegetation in the mid-infrared or short wave infrared part of the spectrum. One is why we see these dips. Why we see each of these little dips here? These ones are especially prominent in the short wave mid-infrared part of the spectrum. Why do we see those dips? It's because those are absorption bands. Those are called water absorption bands. That's where water is strongly absorbent of those particular wavelength sizes. And we also talked about why we see this overall decreasing trend throughout the mid-infrared or short wave infrared part of the spectrum. That's because as we get longer wavelengths, as we get larger and larger wavelengths, water absorbs mid-infrared or short wave infrared light more strongly and more strongly and more strongly. Okay? So we've explained why we see these dips. We've now explained also why we see this overall downward decreasing trend in that part of the spectrum. And then lastly, the overall amount of water content in the leaf will dictate overall how high or low this whole part of the spectral signature is in our spectral signature's graph. A drier leaf for the whole part of the spectral signature will be a much, much higher reflectance throughout while a wetter leaf for the whole part of the spectral signature and the mid-infrared or short wave infrared part of the spectrum will have a lower reflectance because that increased level of water is absorbing that energy more. Are you still with me? Couple of nods. I'll still take it. Okay, cool. So quick review then, kind of summary, the dominant factors controlling leaf reflectance in the visible part of the spectrum are the various leaf pigments in the paliside parentoma. When leaves are healthy, chlorophyll dominates and chlorophyll has very high absorption properties in the visible blue and visible red part of the spectrum. When leaves become unhealthier, that chlorophyll goes away, that strong absorption in the blue and red part of the spectrum is lost and thus those other pigments start to become dominant. We see higher levels of reflectance in the yellow red parts of the spectrum. We see leaves that appear more yellow or red. In the near infrared part of the spectrum, the scattering, the repeated reflectance and transmission of near infrared energy that's bouncing all around the spongy mesofil is why we see a very high level of overall near infrared reflectance in that part of the spectrum for vegetation. For short wave infrared or mid infrared light, we know that that's controlled by water. We see those dips in the spectrum in the spectral signature because we know that there's water absorption bands. We see that there's an overall increased amount of absorption as we get longer and longer wavelengths. So water absorbs more strongly in the mid infrared as we get longer and longer wavelengths of light. Then we also see for the entire portion of the short wave infrared or mid infrared part of the spectrum, there is going to be a higher overall reflectance for that whole portion of the spectrum if we have a drier leaf and there's going to be a lower level of reflectance for that whole portion of the spectrum if we have a wetter leaf. Make sense? Sweet. A video here just kind of summarizing that just to drill at home one more time. Let's just remind ourselves how the spectral signature should be drawn. On the x axis we have wavelength which could be in centimeters from 400 to 2,500 and on the y axis in the reflectance in percent because between 0 and 100. Now let's think about the three main components of the leaf and how that affects the vegetation response. The leaf is made up of the parasite, the verncoma and the parasite, the verncoma, thousands of people. The dominant pigment would be in chlorophyll. We know the pigment response drives out spots in the visible bundle spectrum from 400 to 0,100 nanometers. In the near infrared part of the spectrum it's a structural belief in the sponge you need to know what's driving the effect and then in the minute shot I have the red as the water in the leaf that causes the patterns that we see. Now let's draw our healthy layer. We now have healthy data so far above. If this whole car flows out of the absorbing in the car for white waves which is blue and red, we have always the dips in the spectrum in the blue of the red because the absorption is more. If the absorption is higher, the reflectance is lower. In the green model spectrum there is no absorption because of particles so we see a counter at the same time. We see a very high level of reflectance up to 80% in the near infrared part of the spectrum associated with the sponge you need to know. In the mid infrared part of the spectrum water becomes a major impact. Leaves are very healthy and lots of water as a result of the photons being absorbed. A wetter leaf will have more place on absorption. If there's more absorption there must be less reflectance. We see a dip around 1500 as associated with the water by the atmosphere and then reduction in reflectance down to about 20% in the mid infrared and short wind infrared part of the spectrum. Now let's contrast this healthy leaf spectrum with an unhealthy leaf. And let's practice this by saying that an unhealthy leaf is going to have less power of health breaking down the structure and will be dry. With that in mind let's think about the visible part of the spectrum. An unhealthy leaf has less power of health. If it has less power of health it has less absorption. If it has less absorption it must have more reflectance. So then we would expect the line in the red and the blue part of the spectrum to be above that of the healthy life. The green may stay the same so we still may be having a green leaf that's starting to lose some of its particles. So we expect the green reflectance to be similar but the blue and the red to be high. In the near-to-red part of the spectrum there's much less bouncing around because the structure of the leaf is breaking down. The front ones are not able to bounce around as much and they start to get absorbed by the leaf itself. This absorption increases causing a reduction in the red lightness. So we expect to see the line increasing the near infrared but no in the air as steeply as it does for our healthy leaf. And also what the other lead less near-to-red reflectance than for the healthy leaf. In the water, the drier in the water in the shot by the red part of the spectrum the leaf is becoming drier. That makes there's less water. If there's less water there's less absorption. If there's less absorption there's more reflectance. So we would actually expect higher loss of reflectance in drier leaves in this medium-to-red part of the spectrum. So connecting those components together we see that the vegetation spectrum of the dye leaf is in fact about the healthy leaf and the visible below the healthy leaf and the air in the infrared and back and past the leaf, the healthy leaf and the near-to-red and the mid-to-red and the short-weighting for red as we move to that longer part of the spectrum. Okay, now any questions about that before we kind of move on to our final topic? No? Okay, cool. So the last thing I want to talk about is essentially just an example of applying the knowledge we have of the spectral signatures of healthy versus unhealthy vegetation to create something called a vegetation index or vegetation indices. So essentially we know that in this red edge part of the spectrum where we go from visible red to near-infrared there's this very large discrepancy in the level of reflectance for healthy leaf. We get this characteristic, very high near-infrared reflectance and very low visible red reflectance. And we could take advantage of that to be able to assess things like vegetation health and vegetation cover. So the first vegetation index that was created was called the simple ratio. It just used the near-infrared divided by the visible red to get a ratio, to get a value that essentially just described the health of the vegetation. So if it was healthy vegetation, there'd be a higher near-infrared value, so an overall higher simple ratio. If there was less near-infrared reflectance, an unhealthy leaf, you get a lower simple ratio value, indicating lower plant health. Conversely, in the red part here, if you had a healthy leaf, you'd have very low red reflectance, so you get an overall very high simple ratio value reflecting a healthy leaf. And on the other hand, if you had an unhealthy leaf, you'd get very or relatively much higher red reflectance, giving you a lower overall simple ratio value reflecting a unhealthier leaf or portion of vegetation. So it just takes advantage of this inverse relationship between chlorophyll absorption of red radiant energy, visible red, and increased reflectance of near-infrared energy when plants are healthy. So it just takes advantage of this, right? It takes advantage of healthy leaf, healthy plant, high near-infrared reflectance, low visible red reflectance. Unhealthy leaf, unhealthy plant, much lower near-infrared reflectance, much higher visible red reflectance. Now, the problem with the simple ratio was it was just kind of an arbitrary value. There was no range to it. There wasn't anything to kind of say, well, you know, this value is reflective or associated with this amount of plant health. So we came up with something called the Normalized Difference Vegetation Index, or NDVI. And all NDVI does is takes the concepts behind the simple ratio and just allows for standardized values, which just means that NDVI only ranges from a value of positive one to a value of negative one. Generally speaking, where any positive value from zero to one represents some sort of vegetation cover and values that are closer to one represent much healthier vegetation. Values closer to zero represent much less healthy vegetation. And again, it takes advantage of this same concept where we just have a higher level of near-infrared reflectance for our healthy vegetation. So this near-infrared value is going to be high and a lower value of visible red reflectance when we have healthy vegetation. So this visible red value is going to be low. That's going to give us an overall high value of our denominator, which is going to give us an overall high NDVI value. And then vice versa if we have unhealthy vegetation. So NDVI is a variety of applications. I've used vegetation indices in my research to quantify the greenness or presence of green vegetation around areas where we're sampling for grizzly bear and grizzly bear currents, which I think is a pretty fun application of it. But there's also a wide variety in agriculture and just being able to monitor levels of growing seasons, how long growing seasons occur, how the green up of vegetation occurs seasonally, large variety of applications. But if I ask you on an exam and you say to quantify bear habitat is a nice application of NDVI, I'll be really stoked. Just saying. You don't have to. But again, it just takes advantage of the phenomenon that we know exists where if we have a high level of near infrared reflectance and a low level of visible red reflectance for healthy vegetation, we're going to get a larger denominator, which gives us a positive, very high positive value closer to one, representing healthier vegetation. And then if we have lower near infrared reflectance, higher visible red reflectance that we're going to get a value for NDVI that's closer to the value zero representing much unhealthier vegetation. Again, still taking advantage of this phenomenon right here. Boom, boom. Ultimately, we can also go away and build up entire kind of grids or rasters or data sets that look at the entire surface of the earth and can help us understand things like where different eco regions are, where different biomes occur, all different kinds of things based on understanding what the NDVI or in this case the EVI, which is just an updated version of NDVI, is telling us about vegetation health, about vegetation cover, about greenness, about how plants are cycling through the year, when they're senescing, when they're greening back up and starting to grow again, lots of different applications and a really valuable data set. Okay. I'm sure you are all tired of hearing me talk, which is totally understandable. I want to give you a couple minutes to quickly brainstorm these practice questions, give you about two minutes or so, and then we'll go over the answers to them, and then I'm going to give it to Tristan. He's going to go over a couple tips and tricks for assignment two, and then you can come down and ask him any questions if you have any about assignment two. So if you want to head out now and knock over these questions, please do so. If you want to stick around and go over these, then I'll do that in about a minute or two. If you are heading out, please do so swiftly. We don't have a lot of time, so I'm going to have to go over these quite soon. Thanks, guys. Hey. Yeah. That's a little graph, right? So like, you're going to divide it to three thirds, right? Yeah. I understand the, what was the middle one called? The near infrared? The near infrared. Yeah. So that's how it works, right? What affects it is destruction, right? Yeah. The spongy mesophiles were down to the ground, right? Yeah. Yeah. Yeah. Yeah. So when it breaks down, like, how does it differ? So essentially when it breaks down, the spongy mesophiles just aren't as healthy. They're not spaced out as nicely, and so the chemical essentially reactions that go on with the incident near infrared light and with the spongy mesophiles are no longer such that it's bouncing around and reflecting everywhere, the spongy mesophiles are now absorbing more of that near infrared light. Okay. Also, hypothetically, it feels like it works, right? It's like a bunch of other space between those spongy mesophiles. Correct. It's like if you're most spaced together, it wouldn't work. If like the spongy mesophiles were, like, really tight? It's right. If you're more tight. Yeah, yeah. Then there wouldn't be nearly as much kind of bouncing around between the cells as there is. Oh, left. Yes. Yes. Yeah, no problem. Yeah. Hey. Yeah. So the spongy mesophiles controls how much you reflect the light in the near infrared light and just in the near infrared portion of the spectrum. Okay. Which is only. And I have a question on, okay, so the healthy one in the NIR, it reflects a lot because like half of the one or half of the light, what did you say? Wait, wait. So how's the weight length are absorbed? And then the half are like reflected. So not half are absorbed. Half are reflected. Half are transmitted. Very, very low proportion is absorbed when it's healthy vegetation. Half are reflected. Half are transmitted and then bounces off. Keep bouncing off and then reflects, right? Yeah. So theoretically, even like, I got it. We're going to run out of time. So I got to go over these questions really quick. But I'll have plenty of time to talk to you like right after I go over these. Is that okay? Sorry about that. I got to go over these and then we can talk after. I just want to do a second. I don't have any? No? No. If you need to ask questions, best time is right after lecture. Okay. Yeah. But you can also email me and I'm happy to type out a response. If you really need to set up a meeting with me, you can also do that too. Okay, guys. We don't have a lot of time left. Let's quickly go over the answers to these questions. So the first one, which surface interaction is used by Earth observing satellites to take images of the surface of the Earth. So there's three interactions that can occur between light and surfaces on the Earth. There's transmission, absorption, reflectance. Which one of those is the surface interaction that we use to take images of the Earth? Reflectance, yeah, exactly. And then what are remote sensing scientists able to accomplish by comparing spectral signatures of different materials? What's so valuable about doing that? Yeah? They can distinguish and analyze different materials. Exactly. They can easily differentiate different materials on the surface of the Earth and then analyze them. What does NDVI stand for? Anyone tell me? Anyone? Normalized different vegetation index and what parts of the electromagnetic spectrum does take advantage of? What two parts of the spectrum? Visible red light and near infrared light. Yeah, exactly. And then one final time, let's go over it together. What are the dominant factors controlling the spectral response of leaves? It would be visible part of the spectrum. Yeah? Chlorophyll in which cells? The palisite parencoma. Chlorophyll in the palisite parencoma. In the near infrared part of the spectrum, it's what? Spongy mesophil. Yeah, exactly. So it's the leaf structure, exactly. It's the structure of the spongy mesophil having lots of air between them so that the near infrared can bounce around. And then what about the mid infrared part of the spectrum? Water. Water. Water content in the leaf. Exactly. Okay. That is it for me. I'm going to hand it to Tristan here. He's going to go over some tips quickly and then you're welcome to come chat with him if you have any questions further about that. There's only technically about three minutes left in class. So it would be swift and then it would always come down. This got right there. You can hold this right here, Mike. Oh, right, right. Yeah. Hey, everyone. Yeah, so I'm just going to go over a few tips and tricks. Just as a reminder, I have two hour long office sessions remaining. One on Wednesday at 11 a.m. and one on Thursday at five. So there hasn't been a whole lot of questions about this assignment and very few people shut up to office hours. So this could either mean that people haven't got around to starting it or nobody's having any issues. So that's both perfectly good reasons. But I, so instead of addressing common problems, I thought I would just further explain some of the questions and make sure everyone knows how to interpret some of the websites you have to use. So the first is regarding question six and seven where you use theistic indices to compare map projections. So basically the way this works is that you have a series of red spheres and in an idealized map where there's no distortion taking place, you would see that all of the circles would be the exact same size and shape. So that would, that's the ideal. And that would correspond to a map that depicts all of the surface features of Earth as they are on the actual Earth. But as you know, all map projections have to make some sort of compromise. So basically if you look at the Mercator, we know that it is a conformal map projection that prioritizes preserving angles. So this means that shapes, oh, I shouldn't just give the answer. But yeah, so you can see that one of the features is distorted. So the size of features is distorted. And you can see moving from the equator out towards the poles that the shape of these spheres remains circular, but they're inflating in size. And so you can use the same line of thinking to investigate the other map projections and just see whether or not it's preserving the shape and size. So in some of them, it'll preserve the shape in certain areas. But then if in other areas it distorts it. Yeah, and for question seven, it's basically asking in these two map projections where you know that shape is being distorted, it's just asking you to look at certain parts on the map and to visually inspect whether or not in these areas if there is a region where the shape is being preserved. And you can basically figure this out by looking at the Tisic indicators. And then you can also look at some other supplemental information that can help bolster your argument for it. So I mean, Wikipedia is an okay resource for this level of information. But there's also, and I'm going to post this in the discussion board, there's the Esri ArcGIS page that gives you definitions of all the map projections. And some of them, it'll just tell you where the shape is preserved in the different projections. So I'll be posting that to the discussion board tonight. I haven't had any questions about this, but for question 13, yeah, so one of this, if you plug in the coordinates properly, you'll end up with two maps like this. And it's just asking you which is the most direct route comparing these two types of distances. So by direct we just mean the shortest distance. So it's not taking into account any means of travel or anything. So it's just comparing distances. So for question 14, this has to do with UTM zones. So I'll just quickly explain how to read a UTM zone. So unlike, for example, the Mercator, which is measured in the coordinates, the UTM zone, the unit of measurements are always meters. And so in each of them, the center line, the meridian always corresponds to 500 kilometers. So it's basically asking you at the widest part in the UTM zone, which is at the equator, what is the maximum meters on this side and on that side? So it's a pretty simple calculation you have to make knowing, oh, my pointer's gone, knowing what the width is at the equator and knowing what the value is at the center point. And for question 16, you're comparing a geographic north pole, so basically true north. So that is a fixed point. So it's based on the rotational axis of the earth. So that's always in one fixed point. And basically the idea is just to think about how that differs from magnetic north. So magnetic north is based on the earth's magnetic properties. And it is not based on like a reference coordinate system. It's based on other physical properties of the earth. And so this is an interesting figure that shows how much the magnetic north has drifted since 1900. So yeah, I think the question is pretty straightforward if you look at a few figures like this. Yeah, and that's it. Thanks guys. Do you have any questions about how did you come down that first thing? Thank you. Byron! Okay, hi everyone. Welcome to our midterm review session. I am going to start by giving the mic to Evan. He's going to just go over the midterm format again. I know we already talked about it, but he's just going to drill at home one more time. If you have any questions, feel free to ask him. He'll also just give you his usual weekly update. I'll do what lecture about what there were some questions about on the discussion board. I have some slides prepped for that. I'll leave the rest of the time for an open Q&A. I'll give it to Evan first. Hello everyone. As Chris said, I have the midterm tomorrow. That's really the major thing you guys have coming up for the next few weeks. After the midterm, next week's reading week. There's nothing new next week at all. That being said, we've posted assignment three blog post four. Then assignment four will also be posted by Friday. If you do want to get ahead in the class, you'll have plenty of time to do assignments if you are interested in doing that over reading week. As for office hours, we're going to announce them after the midterm. Don't stress out about it. That's pretty well it. After the midterm, it's coming tomorrow. I've already done this meme. I'm not doing it again, but it's on the slide. So it's tomorrow. It's during class time. There's no class. You can come here and do your midterm if you want. The Wi-Fi should be decent. You'll have an hour and a half to write it. Make sure you start on time. That being said, there isn't a ten minute buffer surrounding it. You can start at 450 pm and you can write until what is an hour and a half, 640 pm. It's going to be administered through canvas. I'll show you where it is in the modules after the slide show. I'll be monitoring my email the whole time in case you have technical difficulties. Your internet goes out. Let me know. Anything like that? Let me know. I'll deal with it. Format, there's 30 multiple choice questions worth one point each. Four fill in the blank questions worth four points each. They vary in how many blanks there are. And then there's six short answer questions. Each of them is about one brief paragraph. Don't write us a novel. They're with four points each. The exams open book. The questions are going to appear one at a time. You have to answer questions. You can't go back and change your answers. So once you've answered a question, you can't look at it again. And also the questions are going to be one of the questions. You can't look at it again. And also the questions are going to appear in a random order. Yeah. Yeah. Can you say that if you're in a random order, would you get like a multiple choice question and then a short answer question? No. Within groups. So all the multiple choice, all the fill in the blank, all the short answer. Great question. Yeah. So make sure you're confident in your answer before you move on because you can't go back. Please work individually. Don't work together. Don't share answers. Don't discuss answers. If you're an access and diversity student, I've gotten all the forms I believe and that's already been updated so you'll already have your extra time. If you haven't sent me your form, please do so before the mid-term so that I can update your profile. Please don't plagiarize. It's an open book mid-term. Utilize the slides and other materials. That's totally chill. Just don't copy entire slides or entire sentences from the Internet or the slides for the short answer questions. Write your answers in your own words. That's like copycat. Don't be a copycat. Review session. Chris is hosting review session right now. You've posted questions on the discussion board. He's got slides related to them. It's worth 20% of your final grade. The practice questions are available on Canvas. I'll be monitoring that discussion board to answer last minute questions pretty much until the mid-terms open. So at 4.50 pm I'll stop looking at it. You'll still be able to look at that during the mid-term. So if you ask questions there, you can see what we said. It covers everything up to and including spectral signatures. Resolution content is not in the mid-term. I know Chris started that last week. And then assignment material not in the mid-terms. You don't need to worry about anything from there unless Chris is covered in class. You need reliable Internet. Please make sure you have reliable Internet. Contact me if anything goes wrong. And ensure you start your exam by 5.10 pm in order to get the full hour and a half. You guys are going to do great. Do the practice questions. Go over your class notes. Pay attention to what Chris highlighted. That's the important stuff. You're going to do great. And now I'll show you where on Canvas it is. So you're in Canvas. You're home. You're modules. Scroll all the way to the bottom. There's mid-term exam in the exams section. That's where you want to find it. You can also probably find it in assignments. See. Yep. Mid-term exam right there. If anyone has any questions now is a great time. If not, Chris will go into the review session. Hey. Is lecture eight the end? Or is it the lecture nine? Is lecture eight spectral signatures? No, that's the lecture nine. So lecture nine spectral signatures. That's the last one that's included. Yeah. We removed references to numbers in this slide show. Yeah. So anything after resolutions? Don't worry about anything before resolutions, but not including resolutions. That's on the mid-term. Make sense? Any other questions? Go format. Sweet. Sweet. All right. Cool. Good luck, everyone. We're going to do great. I'll see you tomorrow. Yeah. Okay. I'll just highlight one thing that Evan went over again. He had a slide about plagiarism. So just a note on that. There are a couple of short answer questions where you might be able to answer in a brief phrase. You know, you might not need an entire sentence. That's fine. You know, in that case, you don't need to worry about answering in your own words per se because, you know, if it's just two or three words and it's just a two or three word answer, then that's fine. You can use the exact wording that we've talked about in our lecture slides, whatever it might be. There are a couple of questions that require you to synthesize things, which just means you have to write a couple of sentences, a couple of complete sentences in order to link some things together. And for those questions, you need to answer in your own words. Specifically, just don't copy and paste things from the slides from the Internet, whatever it might be. Does that make sense? Any questions about that? Okay. So I'm going to go over map distortions. How to use a marine chronometer. My animations are messed up. I'm going to go in the order. I don't know why this is just awful that that's appearing like that. So I'm going to go over map distortions. And then the marine chronometer, how it works, we'll talk about global navigation's satellite system errors. There are some questions on that. We'll talk about the first images acquired from space. And then we'll talk about, I had a question specifically about the spongy mesophil and how it controls spectral reflectance in the near infrared part of the spectrum. I'll just talk about as a whole leaf properties and how they influence the spectral response of vegetation. So hopefully that answers that question. But if you have any questions as we're going through, I'm hoping this will be relatively interactive. If you have extra questions, if you're not understanding things, please let me know. I'm happy to dive deeper to explain further. Okay. So first thing that I'll talk about is map distortions. To essentially summarize the different projections we talked about and how they are distorted, you can more or less just look at this slide. So there's four types of projections that we discussed. Conformal projections, equivalent equal area projections, equidistant projections, and compromised projections. Generally speaking, we talked about distortions of shape, direction, distance, and area. Shape and direction are more or less analogous or the same. You can consider if something has a shape that is distorted, then its direction is distorted. And if something has a direction that is not distorted, then its shape is also not distorted, you can more or less consider those two the same for the purpose of this midterm and for this course. So when we talk about conformal projections, those are projections where shape or direction is preserved. A direction on a conformal projection is true. If you look at a conformal projection and try and get directions with it, and it shows you to take a turn 90 degrees to the right, then in real life it would want you to take that turn 90 degrees to the right. So direction is preserved because of that shape is also preserved. So this is a conformal projection up here. One that we often talk about is the Mercator projection. The Mercator projection preserves shape, so it makes these very relatively aesthetically pleasing maps, but it distorts area and distance quite heavily. In particular, we talked about how it distorts area and how because the Mercator projection is often used or has often been used historically in institutions in education that it has perpetuated Eurocentrism. We've talked about that in the context of increased distortion in the Mercator projection as we travel from the equator to the poles in either direction. Specifically with regards to area, things appear bigger and bigger and bigger, the further we get to the north and south pole in the Mercator projection, then they are in reality. Because of that, it perpetuates this sense of European dominance or of Northern hemispherical dominance on the rest of the world, or greater importance, greater dominance, greater dominance, whatever you want to consider. You'll definitely see a question about Eurocentrism related to the Mercator projection on the midterm. And there are two real key things that we talked about. One was what I just mentioned, the increased distortion of areas as you get closer to the north and south pole. The other that we talked about is how in the Mercator projection the prime meridian or central meridian is often right here, the very center of the map, often passing through England, which just makes Europe kind of always the center part or centerpiece of the map. So two key things that we talked about there. I'll leave it there. Any questions about that? Okay. Then we talked about the equivalent equal area projection. This projection preserves area. It distorts distance and shape or direction. Then we talked about the equidistant projection. The equidistant projection preserves distance, generally just from a single point on the map. So in this example that we looked at in class, the distance that is preserved is the distance from this center point to any other point on the map. If you try to measure distance from say over here to over here, that would be incorrect. It wouldn't be proportionally correct, correct. But if you tried to measure distance from the center point anywhere out to any other point on the map, it would be proportionally correct. Equidistant projections do, however, distort shape, direction, and area. The last kind of projection we talked about are the compromise projections. Compromise projections often look something like this. They can create pretty aesthetically pleasing maps. However, none of the map elements shape, direction, distance, or area are preserved in a compromise map. Every map element we discussed is distorted. Which generally makes it not very commonly used when you're talking about using compromise maps in mapping because it's not very practical since all of its elements are distorted. But just for the purpose of creating something that looks aesthetically pleasing, we often might use a compromise map. Any questions about that? About map projections? Nope. Okay. Next thing I wanted to talk about was the marine chronometer. We talked about the marine chronometer quite a bit. First off, wanted to remind you of its significance, why we talked about it. We talked about it because when we talked about historically the first methods used in order to define someone's absolute position, you know, mostly in the context of C navigation, we had for finding latitude, celestial navigation, so using, say, the north star, for example. And for longitude, we talked about the use of the marine chronometer. So its significance is that it was the first instrument used to measure longitude accurately at sea. And how it essentially works is the marine chronometer is really just a very, very accurate clock. One of the, you know, for its time when it was created, the most accurate clock that there was available for C navigation. The marine chronometer would essentially keep track of mean Greenwich time, or potentially, you know, a different time wherever you were leaving from, but generally it was mean Greenwich time. And so this chronometer, this clock essentially, would just always keep the time of what it was in Greenwich, England. It would always just measure what time it is in Greenwich, England, no matter where you were across the globe. Navigators could then use the angle of the sun in the sky to determine their local time so they could look up in the sky and wait for the sun to reach its highest point in the sky and say, okay, I now know that it is noon, that it is noon because the sun has reached its highest point in the sky. I know it's noon exactly where I am. If I then look at my marine chronometer and I say, okay, I know it's noon where I am, but my marine chronometer with its very accurate clock says that back in Greenwich, it's, say, you know, two o'clock, then I know, okay, there's a two-hour time difference there, and every hour difference is equal to a 15 degrees difference in longitude. So if I have a two-hour difference, that's a 30-degree difference in longitude. Does that make sense? Any questions about that? Clarification, yeah? Yeah, their own clock, they would kind of estimate what time it is where they are themselves and say, okay, we know that based off the angle of the sun, it's this time where we are, and then our chronometer has the time of what it is back in Greenwich, and then we would just compare the times between those two clocks and then work out our difference in longitude from that. Makes sense? Well, so, yes, I mean, there often wasn't, you know, literally a second clock. They were just determining what time it was based off the angle of the sun, and then they say, okay, we know what time it is here. Our chronometer is essentially just a clock measuring what time it is in Greenwich, England. Cool. Any questions about that? Yeah? So, the three-camera-women tracking like a half-digit time, it was like a business-dintingian moment. I know that, I'm here, like, a half-digit position, but I'm not measuring like a whole-time limit, two hours, so I travel there. Correct. It's not traveling how far you, or how long you've been traveling for. It's just telling you what the time is in Greenwich, England. Then you determine what time it is where you are based off the angle of the sun. The difference between those two times is your longitude. Yeah. Yeah. Any other questions? Okay. So, that's the chronometer. We then talked about global navigation satellite systems and location findings. We talked about how you can determine your position using GNSS in a couple of steps. You download the almanac from a satellite onto your receiver. You download the ephemeris and synchronize the receiver clock. You measure the change in time, delta T, to at least four satellites for a verified and accurate position. Then you determine the range using that difference in time to those four satellites. And then based off that, you're able to calculate a position of X, Y, and Z. Now, quickly, I know there's always confusion about what the difference is between the ephemeris and the almanac. I'll just briefly mention here the ephemeris is always required to calculate position. You have to have the ephemeris in order to locate where you are using your receiver. The almanac's not required. You don't need the almanac to determine your position, but it is very useful because it generally allows the receiver to find other nearby satellites for positioning much quicker. The ephemeris gives you detailed information about the satellite accuracy and health, clock correction coefficients, the orbital parameters of those satellites, so where they're supposed to be in their orbit. And it's valid for only two hours. The almanac is generally much less accurate information about the satellites and their health. It really is just used to speed up connection to other satellites. And it's valid for 90 days, so quite a while. You always need the ephemeris. It's only valid for two hours. You don't always need the almanac, but it is useful, and it's valid for 90 days. Any questions about the difference between these two? Okay. I also had a question about how accurate our phones are for positioning services. So generally speaking, depending on, you know, obstructions or potential causes of errors, our phones are about three to five meters in accuracy or error for our positioning. The accuracy of our phone and its ability to determine our location is influenced by the number and position of satellites that it's able to get in contact with, by atmospheric effects, by obstruction, such as trees and buildings, by the receiver quality, so if you have a newer phone, you might just get a better, more accurate position, and by the potential or ability to have corrections or post-processing. Now, in general, we talk about or classify several different kinds of GNS errors as the following here. Receiver errors, so just errors associated with maybe having an older or poor receiver. Clock errors, so clock errors either coming from the satellite or on your receiver, but issues with timing with the clocks. Ephemeris errors, so some ephemeris data that's just outdated or incorrect. Tropospheric delays, ionospheric delays, both delays related to atmospheric effects. We discussed how the ionospheric delays are much more prominent, much more of a larger issue. Often the largest source of error in GNSS positioning comes from ionospheric delays. We also talked about multipath errors, where you might have trees or buildings obstructing the ability for those waves to come down and reach your receiver. Multipath errors, specifically being when that radio wave is bouncing off of obstructions, buildings, then ultimately bouncing all the way down to your receiver. Okay, so those are the different types of errors, and how do we actually reduce those errors? There's a number of things that we can do, and we can generally classify them into before taking our positioning measurements, while taking our positioning measurements, and after taking our positioning measurements. Before taking our measurements, we can perform good mission planning. We can go on to some of the websites that I think you guys have already played around with, or are going to be playing around with, and determine, okay, for this specific area or location where I'm going to go out and take some position measurements, when is there going to be a lot of satellites overhead, when am I going to get a good level of accuracy? While taking my measurements, I want to make sure I'm remaining in the open. I don't want obstructions. I don't want to be under a very dense forest canopy, or under a bunch of buildings. That's going to make it very hard. I want to avoid these buildings in tall trees. I want to take several measurements, so I want to just stand there and take lots and lots and lots of measurements, and then average them if they're all kind of a little bit different by 0.00001 meters or whatever, and I average them, then I'll get the most accurate possible measurement. And then I also want to be patient. Sometimes, because satellites are constantly orbiting, satellites that are maybe not in view of your receiver will come into view of your receiver once they've traveled over to a different portion on the surface of the earth. After taking your positioning measurements, you can engage in post-processing. So we talked about differential GPS, where you have a base station nearby that already has a well-established position, and you can use that well-established position to correct for potential errors in the position measurement that you've done yourself. Make sense? Any questions? Yeah? Yeah. Multi-path error? So multi-path error specifically is when we have radio waves that are bouncing off of buildings, trees, other obstructions, ultimately bouncing, bouncing, bouncing, but then reaching down to our receiver. So they're not traveling straight from the satellite to our receiver. They're bouncing around, hitting a bunch of obstructions along the way. Okay. Now, we also talked about how we measure errors or measure accuracy of our positioning measurements. We talked about dilution of precision. We have a good dilution of precision when our satellites are spread equally across in the sky, horizontally, side to side, and vertically, up and down. So you can see here there's lots of kind of low satellites, lower satellite, medium satellite, medium satellite, nice high satellite. So that's a good vertical dilution of precision. And then we have also ones that are far out to the right, far out to the left, and then kind of middle to the left, middle to the right. That gives us a good dilution of precision because those satellites are nicely spread out in the sky. A bad dilution of precision might come in a situation like this where all of those satellites are kind of bunched together. These satellites are horizontally, side to side, very close to each other, and vertically, up and down, pretty close to each other. That's going to result in a bad dilution of precision, which is going to give us poor accuracy in our position measurements. There's a couple of different parts to dilution of precision. We generally classify position dilution of precision as the most commonly used measurement. It's a combination of vertical dilution of precision and horizontal dilution of precision. Then we also have time dilution of precision, which is just a measure of the accuracy of our clocks. Any questions about satellite positioning accuracy errors, dilution of precision? Yeah, yeah. Is that dilution of precision? Because if it takes a part or it's a primary figure of precision? It's pretty much into the details of exactly why mathematically, but that is right. That's the main one. Cheaper? That's pretty much it. The other difference that is possible is sometimes with differential GPS, you have a permanent base station. It's like a building, something that is always there. Whereas with RTK, it's generally a rover that you bring around to your site where you're doing the measurements. And so, just because of that, that's maybe a little bit of extra work. Okay. And is the system GPS a form of differential GPS with cell power? No. No, it's not. Yeah. Assisted GPS mainly just uses cell towers to connect your receiver to satellites quicker. So it doesn't necessarily improve the accuracy. It just allows you to connect quicker. Yeah. Yeah. Okay. So one of the things we talked about when we started talking about the history of Earth observation from space, where the first image is acquired from space. So the first images acquired from space were taken from rockets. The American V2 rocket in 1946 was the very first one. And generally, there were a lot of disadvantages of using rockets to take imagery of the Earth. But the big issue that we discussed in class was that rockets would have film on board, which the imagery would be stored on. So it would be just literally a film camera on the rocket that would take the image. And then this film would have to be physically retrieved. So it would essentially get ejected from the rocket, have a little parachute that it would kind of float down to Earth. And then someone, in this case the Americans would have to go out and find it and physically retrieve it. So nowadays, imagery is just sent remotely to ground stations. But back in the day, you know, you had to go out and search and retrieve these capsules of film. That was a lot of work. So that was a major disadvantage that we talked about in terms of rockets collecting satellite imagery, or not collecting satellite imagery, but collecting imagery of the Earth from space. Questions about that? At all? Okay, sleep. Okay. I think this is the last topic I had to go over today. It's always the one that students I find struggle with the most. So I will go over in detail, feel free to stop me if you have questions. A spectral signature is the pattern of spectral response of a material. It's typically visualized with a graph like this where we have wavelength size along the x-axis, and then reflectance as a percentage along the y-axis. And it just shows the percentage of radiation of different wavelengths reflected from a certain object. And it's important because it essentially creates and forms the foundation of multispectral Earth observation remote sensing. And one of the key things that we do in Earth observation is take our knowledge of spectral signatures and differentiate different materials and surfaces that are on the surface of the Earth based off of what the reflectance is that we measure from satellites of those surfaces. Now, we talked in detail specifically about the spectral response of vegetation and of leaves. And I'll go over briefly here kind of a summary of those properties and how they influence the spectral response of vegetation. So, if we break our spectral signature's graph of vegetation into three sections, the visible, the near infrared and the middle infrared part of the electromagnetic spectrum, we know that the visible part of the spectrum is influenced by paliside parenchyma cells in the leaf. Paliside parenchyma cells are where the pigments in the leaf are stored, including chlorophyll pigments. Chlorophyll pigments have very strong red and blue visible light absorption properties, which is why we get these kind of chlorophyll absorption bands, where there's very low reflectance in the blue and red part of the spectrum. Because of that, there's relatively more green light reflected than blue and red in the visible part of the spectrum, which is why leaves appear green to our eyes. Okay, then we have the near infrared part of the spectrum. The near infrared part of the spectrum is influenced by the spongy mesophil cells. Near infrared light penetrates through the paliside parenchyma cells down to the spongy mesophil cells, where, because there is so much intercellular air space in between the different spongy mesophil cells, near infrared light repeatedly bounces around all the different cells within the structure of the leaf. Ultimately, that near infrared light bounces out of the leaf, maybe back towards where the light came from, but maybe in a completely different direction as well. Because it can then hit another leaf and interact with that leaf again by bouncing around its spongy mesophil cells, and then bouncing out again, there's this repeated effect of near infrared light reflecting and transmitting off of a leaf, potentially to another leaf and then to another leaf and then to another leaf and then to another leaf. And because of that repeated reflectance and transmission and reflectance and transmission, we get this very high overall level of near infrared reflectance when we look at leaves or at vegetation. The last part of the spectrum we talked about was the mid-infrared or short-wave infrared part of the spectrum. There were a couple of different things that we talked about in detail about this part of the spectrum. The most important to remember is simply that when we have a leaf that has a increased water content, so a wetter leaf, there is going to be lower mid-infrared reflectance overall. When there is a drier leaf, there is going to be a higher amount of mid-infrared reflectance overall. That's really the key there. Now we also talked about the absorption bands, and we talked about how at longer wavelengths, water absorbs more strongly than at shorter wavelengths in the mid-infrared. We talked about those as well, but what's most important for you to remember is just that when leaves are wetter, they will reflect less mid-infrared light overall. When leaves are drier, they will reflect more mid-infrared light overall. I have these two summary slides to just summarize all of that. In the visible part of the spectrum, the various leaf pigments in the paliside parencoma, such as the chlorophyll and carotenes and other pigments, dominate the spectral response of leaves in the visible part of the spectrum. In the near-infrared part of the spectrum, the scattering or the repeated reflectance and transmission of near-infrared energy in the spongy mesophil is what dominates the reflectance pattern we see in the near-infrared portion of the spectrum. In the mid-infrared portion of the spectrum, the reflectance pattern we see is dominated by the amount of water in the plant. To dive a little bit deeper, in the visible part of the spectrum, chlorophyll pigments dominate and absorb very strongly visible blue and red light when the leaf is healthy. Other pigments, such as carotene, xanthophils, are generally always present in the leaf, but when the leaf becomes unhealthy and those chlorophyll pigments are no longer dominant, there's no longer that strong absorption in the blue and red part of the spectrum. So there's a higher level of reflectance in the red and yellowish parts of the spectrum, which is why in fall for deciduous trees, we get their leaves appearing yellowish and reddish and eventually brownish when they die off. In the near-infrared portion of the spectrum, there's high near-infrared reflectance when leaves are healthy because there's this nice, healthy leaf structure of spongy mesophil cells that are nicely spaced around with lots of intercellular air space between them. When leaves are unhealthy, they have lower near-infrared reflectance because that structure of those cells starts to break down and you no longer get that rid of the leaf. You no longer get that repeated effect of reflectance and transmission and bouncing around of the spongy mesophil cells or of the near-infrared light in the spongy mesophil cells. Or in between them, I should say. So, lastly, mid-infrared portion of the spectrum, higher water content. I just want to make sure I'm recording here. Thank goodness. Higher water content results in lower mid-infrared reflectance overall, while lower water content results in higher mid-infrared reflectance overall. So, that is ultimately why we get a spectral signature pattern that looks like this. We get that strong blue and red absorption from the chlorophyll when the leaf is healthy. Thus, relatively speaking, more visible green light. Then we get that very characteristic peak in the near-infrared part of the spectrum because of the spongy mesophil cells, the ability of the light to bounce around between the spongy mesophil cells, and then the repeated reflectance and transmission between leaves in the forest or in the canopy or in the trees. And then in the mid-infrared part of the spectrum, we get an overall much higher level of mid-infrared reflectance when the leaf is drier and an overall much lower level of mid-infrared reflectance when the leaf is wetter. Any questions about that? Okay. Last thing I want to go over is just a reminder because we just had the resolutions lecture, what you need to know for the midterm about each of the satellite programs we discussed. It's not too much. So, for MODIS, you just want to remember that MODIS gives us fine temporal resolution data and coarse spatial resolution data. So, it has very large, relatively large pixel sizes and can revisit any point on the earth every one to two days. Landsat, you need to know it's the oldest program dating all the way back to 1972. It gives us a moderate spatial resolution at about 30 meters and a moderate to find temporal resolution with a 16-day revisit time. World view we briefly talked about, you just need to know that it is the finest spatial resolution satellite data that we have available to us, and that is private, which just means it's expensive. It costs money. Lastly, we briefly mentioned ISAT. All you need to know is that it uses LIDAR. Don't need to know what LIDAR is. Just know it uses LIDAR. We'll talk about what LIDAR is after the midterm and just know that it's used to image ice, clouds and elevation. It's what ISAT stands for. Any questions? Anyone wants me to go over with the whole class? Otherwise, I am happy. Yeah? Yeah? Generally, that would be either the Landsat or MODIS program, potentially world view, but really just any spectral or optical sensor, any satellite sensor that's using parts of the near infrared or visible red spectrum. You can use to get those indices. So it doesn't really matter too much specific to a satellite, but of the ones we've talked about, LIDAR is really the only one you couldn't do that with, so therefore ISAT, you wouldn't be able to get that information from. Okay. Yeah. Okay. Yeah. Yeah. Yeah. So NDVI, the normalized, normalized difference vegetation index is used to measure the health of vegetation, the greenness of vegetation, as well as just vegetation cover across the landscape. Those are the key things it's used for. It can be applied in a variety of different ways on top of that, but that's ultimately what it's used to measure, but it can be used for crop forecasting, for understanding the health of forest, for understanding habitat of wildlife, so it can be applied in many ways, but that's what it's ultimately measuring. Okay. If you want to head out, you're welcome to. I'm happy to stick around. If any of you want to come up and ask me any questions, otherwise, good luck on the midterm tomorrow. Have a good reading break, and I will see you after the break.